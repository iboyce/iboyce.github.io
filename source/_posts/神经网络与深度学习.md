---
title: 神经网络与深度学习
date: 2017-08-30 17:20:41
tags:
      - 深度学习
      - Andrew NG
      - 公开课
categories: AI梦
mathjax: true
---


### 第一、二周：概论与基础
> sigmoid在逻辑回归里为什么没用被ReLU替代

  * sigmoid在感知器结果输出里可以将任意输入连接的映射到 $[0,1]$ 之间，且结果保持连续
  * ReLu作为深度学习引入的激活函数，特性在于稀疏的激活性、单侧抑制与相对宽的兴奋边界

> LR中损失函数不用平方误差，为什么它是非凸的。

* 因为加入了Sigmoid函数，$a=\sigma (z)$ 它的特点：
$$ da/dz = z(1-z), dL/dz= a - y \quad  if \quad L=-( y log a + (1-y)log(1-a) ) $$

> 矩阵运算Numpy的高效

* 避免矩阵运算使用For循环，100m时差300倍效率，因为 numpy使用到了CPU\GPU的SIMD指令，运行速度更快。同时，对于numpy有内置的函数也一样，尽量使用，如log,max  etc..
* 在python中，z= np.dot(w.T, x) + b , b会自动扩展成一个n维向量，这在python称为broadcast
* 在使用的过程中，reshape是o(1)操作，多多益善，尽量不使用rank为1的数组。

### 第三周 浅层神经网络
* 二级神经网络，即只有一个隐藏层的神经网络。
*  输出层通常使用sigmoid函数，而隐藏层不用，一般至少使用tanh来代替，因为它的输出介于-1与1之间，这样均值为0，便于下一层的计算。考虑到梯度下降，sigmoid与tanh在最大小值处导数变化小，reLU及leaky reLU更常用。当然这一切根据实际情况调整。
*  隐藏层不应使用线性激活函数，若用则相当于没有隐藏层，输出层若要出回归值，则可使用。
*  随机w,并乘以较小的系数，一般让随机参数比较小。

### 第四周 深层神经网络
*  hyper parameter：比如层数、迭代次数、下降速率、激活函数的选择、batch size等等
*  parameter: w,b  被hyper control
