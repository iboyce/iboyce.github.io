---
title: 神经网络与深度学习
date: 2017-08-30 17:20:41
tags:
      - 深度学习
      - Andrew NG
      - 公开课
categories: AI梦
mathjax: true
---


### 第一、二周：概论与基础
> sigmoid在逻辑回归里为什么没用被ReLU替代

  * sigmoid在感知器结果输出里可以将任意输入连接的映射到 $[0,1]$ 之间，且结果保持连续
  * ReLu作为深度学习引入的激活函数，特性在于稀疏的激活性、单侧抑制与相对宽的兴奋边界

> LR中损失函数不用平方误差，为什么它是非凸的。

* 因为加入了非线性的Sigmoid函数(注意，若是纯线性的，使用最小二乘可直接估计 $w$)，它使得平方误差损失函数是非凸的，而以下损失函数定义（极大似然的思想）没有这个问题，而且它易于求导的特点决定了使用梯度下降来求极值更方便[**在无约束优化问题中，对目标函数直接微分的方法一般难以求解，故常用迭代的方法**]：
$$ da/dz = z(1-z), dL/dz= a - y \\  \mbox{if } L=-( y log a + (1-y)log(1-a) ) $$
![ng_gradientdescent](http://p15i7i801.bkt.clouddn.com/e70536dc3cdac781213e6e86a904d5ac.png)

> 矩阵运算Numpy的高效

* 避免矩阵运算使用For循环，100m时差300倍效率，因为 numpy使用到了CPU\GPU的SIMD指令，运行速度更快。同时，对于numpy有内置的函数也一样，尽量使用，如log,max  etc..
* 在python中，z= np.dot(w.T, x) + b , b会自动扩展成一个n维向量，这在python称为broadcast
![ng_broadcast](http://p15i7i801.bkt.clouddn.com/849e2ed8c823392209bd02a6e7397727.png)
* 在使用的过程中，reshape是o(1)操作，多多益善，尽量不使用rank为1的数组。

### 第三周 浅层神经网络
* 二级神经网络，即只有一个隐藏层的神经网络。
*  输出层通常使用sigmoid函数，而隐藏层不用，一般至少使用tanh来代替，因为它的输出介于-1与1之间，这样均值为0，便于下一层的计算。考虑到梯度下降，sigmoid与tanh在最大小值处导数变化小，reLU及leaky reLU更常用。当然这一切根据实际情况调整。
*  隐藏层不应使用线性激活函数，若用则相当于没有隐藏层，输出层若要出回归值，则可使用。
*  随机w,并乘以较小的系数，一般让随机参数比较小。

### 第四周 深层神经网络
####  hyper parameter——控制参数的参数
  * 层数
  * 迭代次数
  * 下降速率
  * 激活函数的选择
  * batch size
####  parameter
  * 权重 $w$
  * 偏移量 $b$
