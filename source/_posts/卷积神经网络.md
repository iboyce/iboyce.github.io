---
title: 卷积神经网络
date: 2018-01-15 21:09:47
tags:
      - 深度学习
      - Andrew NG
      - 公开课
categories: AI梦
mathjax: true
---

# 卷积神经网络

## 第一周 卷积神经网络
### 计算机视觉
* 图像分类
* 目标检测
* 衍生的新的艺术形式：风格迁移
* 以上处理参数巨大，难以有足够的数据防止过拟合，所以需要引入卷积

### 边缘检测
* 使用卷积进行边缘检测的示例
![ng_edgedetect](http://p15i7i801.bkt.clouddn.com/d62af4f28a74993ca8203e2bc49724f7.png)
![ng_edetect](http://p15i7i801.bkt.clouddn.com/a5e08bd2c256d7f8869b1529f39f067b.png)
* 卷积核或者说过滤器的选择很多文献讨论过，除了常见的过滤器，如下图的边缘检测，过滤器也可以做为参数学习出来
![ng_filter](http://p15i7i801.bkt.clouddn.com/f1ed787d1a20b5960ed419dd5d60c69e.png)

### 卷积神经网络
以下$n$为图像大小，$f$为filter大小，$p$为padding大小
* valid: $n-f+1$为过滤后的大小, 这样输出**图像会越来越小,而且会丢掉边缘位置**，因此引入padding使用输出图像大小
* same: $n+2p-f+1=n$, $p=(f-1)/2$，因此$f$为奇数时，便可以选择相应大小的padding填充尺寸，所以在计算机视觉中很少看到偶数尺寸的过滤器，除非使用不自然的不对称的填充，而且用奇数过滤器有个中心便于指出它的位置.因此，建议使用*奇数过滤器*。
* 若加入步伐stride,则输出尺寸为
![ng_stride](http://p15i7i801.bkt.clouddn.com/6ae57951570e44ca6b74003fe54cde53.png)
* 多个过滤器，一个过滤器通常一个二维输出（即使有多个通道，但过滤规则就一个，比如通常用红的横向边缘检测），但可以用多个过滤器去卷之，这样同样能得到一个输出立方体。注意过滤器的长宽可以不一致。
![ng_multifilter](http://p15i7i801.bkt.clouddn.com/9ad4f66cd2080d4086a0acec0852eda9.png)
* 单层卷积，由超参过滤器大小f，步伐s,Padding p决定下层的大小和过滤数的个数filters决定下一层的深度，即通道数，最后一层所有参数作为输入经激活函数得出结果。
![ng_conv](http://p15i7i801.bkt.clouddn.com/0cb72ff62d4c89ad9b8136f2e0db0331.png)
* 池化，一般有最大池化和平均池化，前者后普遍使用，经常使用超参f=2,s=2，输出大小 正好减半，也有f=3,s=2或f=3,s=1。至少为什么用池化，总之在实验室效果很好，直观上在某个区域只要获取特征是否存在，存在取最大值。同时池化基本不用padding，后面会讲。
* 参考LeNet5的一个完整的CNN示例，关注以下几点
  * 在这里把CONV-POOL作为一层，POOL层无参数，但有的文献算为两层
  * 超参的选择最好是参考论文，比如S=F=2，大小正好减半
  * 这里的全连接层每一层400，第二层120，百度原因全连接在于对细化特征的重新分类，参考知乎上的说法，
  * 这里的全连接层每一层400，第二层120，百度原因全连接在于对细化特征的重新分类，[参考知乎](https://www.zhihu.com/question/41037974)上的说法，近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP，即卷积核为1*1）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。需要指出的是，用GAP替代FC的网络通常有较好的预测性能，它减少了参数和过拟合。
  * 参数表下图，有二特点，一输出维度越来越小；二参数，输入没有参数，第一层卷积，卷积核参数5*5*8+8（BIAS）=208个参数，池化层无参数，第
![ng_cnn](http://p15i7i801.bkt.clouddn.com/f6c006b85ade925f134b42d60bb13cab.png)
![ng_cnn_1](http://p15i7i801.bkt.clouddn.com/db0d8e0123bc420d34c9a6705ac60b13.png)
* 为什么用卷积：参数共享+稀疏连接，前者是建立在对输入不同区域其特征检测方法仍然适用，正如很多人知道的，CNN擅于抓取具有平移不变性的特征
* 在建立完成CNN后，定义好损失函数，我们就可以用梯度下降或RMSPRO等等方法优化神经网络的参数了

## 第二周 深度卷积神经网络
### 实例研究
#### LeNet-5
* LeCun et al., 1988. Gradient-based learning applied to document recognition
* 由于两个全连接，参数在6万左右
* 数据大小逐渐减少，通道数逐渐增多
* 过程为：输入——卷积——池化——卷积——池化——全连接——全连接——softmax
* 激活函数用的是sigmoid\tanh
* 如下图：
![ng_LeNet-5](http://p15i7i801.bkt.clouddn.com/f1c276be3dd216d841203567c98c4b17.png)
#### AlexNet
* Krizhevsky et al., 2012. ImageNet classification with deep convolutional neural network
* 结构和LeNet很像，只不过参数变成了6千万
* 使用了Relu
* 使用了多个GPU，并且特别研究了如何将计算量分布到多个GPU上面
![ng_AlexNet](http://p15i7i801.bkt.clouddn.com/7b6475049b5a00feb38029552230eb9c.png)
#### VGG
#### ResNet,152层
#### Inception
