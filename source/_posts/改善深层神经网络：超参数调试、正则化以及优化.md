---
title: 改善深层神经网络：超参数调试、正则化以及优化
date: 2017-09-15 17:45:29
tags:
      - 深度学习
      - Andrew NG
      - 公开课
categories: AI梦
mathjax: true
---


#  第一周 深度学习的实用层面
## 训练、验证、测试集
* 在小数时代，比例一般为6：2：2，大数据时代可适当调整，验证及测试集为10000左右即可。
* train set 最好和后两者有相当的分布，比如猫的问题，在网上爬的猫有许多精良的照片，而用户实际上传的可能比较糙。
* test set是为了拿到测试的无偏估计
* ML是一个高度迭代的过程，即使最牛的专家也是如此。

## 偏差与方差
* 偏差代表训练集的错误率有多高，方差代表dev set 和train set的错误率的差别

## 正则化
regularizer，规则化，通过先验的知识来最大化贝叶斯最大后验估计，本质上是贝叶斯最大后验估计的**似然函数**
* L1
* L2：Frobenius Norm，直观上Lambda越大，权重矩阵被设为零
![nn_reg_12181821](http://p15i7i801.bkt.clouddn.com/2d81fcb8cfe1cb048929564aa91f9c4f.png)

* dropout
* 其它减少过拟合的方法
    * Data augmentation
    * early stop: 此种方法相对L2来说减少lambda的尝试计算量

## 正则化输入（normalize)
使用零均值，主要是用来提升速度，范围尺度类似即可，也不是一定要0-1。如下图，若不同特征相差尺度很大，则会形成如左不规则超球体，这样w的迭代次数会大大增加，对应的迭代步长也应设置偏小。

## 梯度消失及爆炸
由于层数太多，梯度指数级变化，这容易导致步长会非常小，训练时间会变得很长。传统激活函数都存在这一缺点，正向时爆炸，反向时消失，为防止上述问题，权重的初始值的选择有：
* 若用RELU作为激活函数（[种类及特点](http://blog.csdn.net/mzpmzk/article/details/77418030)), $w$ 初始值可以使用 $np.sqrt(\frac{2}{n^{[l-1]}+n^{[l]}})$
![ng_vanishing_exploding_grad](http://p15i7i801.bkt.clouddn.com/1f3a7fe0374fc7a514f6c69809520867.png)
# 第二周 优化算法
* Min-batch梯度下降：避免需要处理所有数据才能进行下一步
* 指数加权平均
* 动量（Momentum）梯度下降:类似移动平均线减缓摆动
$$ v_{dw} = \beta v_{dw} + (1-\beta)dw $$
$$ v_{db}=\beta v_{db} + (1-\beta)db $$
$$ w=w-\alpha v_{dw} $$
$$b=b-\alpha v_{db}$$
* RMSprop：让幅度大的参数变缓，让幅度小的参数变大

$$ S_{dw} = \beta S_{dw} + (1-\beta){dw}^2 $$
$$ S_{db}=\beta S_{db} + (1-\beta){db}^2$$
$$ w=w-\alpha \frac{dw}{\sqrt{S_{dw}}}$$
$$ b=b-\alpha \frac{db}{\sqrt{S_{db}}}$$
* Adam(Adaptive Moment Estimation): 各算法适应于不同的神经网络结构，该算法结合Momentum and RMSprop，非常常用。
  * 公式
  $$ v_{dw} = \beta v_{dw} + (1-\beta_1)dw $$
  $$ v_{db}=\beta v_{db} + (1-\beta_1)db$$
  $$ S_{dw} = \beta S_{dw} + (1-\beta_2){dw}^2 $$
  $$ S_{db}=\beta S_{db} + (1-\beta_2){db}^2$$
  $$ w=w-\alpha \frac{v_{dw}}{\sqrt{S_{dw}}}$$
  $$ b=b-\alpha \frac{v_{db}}{\sqrt{S_{db}}}$$
  * 超参数一般值：
      * $\alpha$ : needs to be tune
      * $\beta_1$ : (first momentum):0.9
      * $\beta_2$ : (second momnetum:RMSprop):0.99
* 学习率衰减：多个方法，如倒数、指数等等
* 关于局部最优：在大量参数情况下可能会出现saddle问题，这样问题容易到达plateau状态，较难走出。这种可能性较小，通常使用Adam之类的优化方法解决

# 第三周 超参数调度、Batch正则与程序框架

## 超参数调整
![ng_hp](http://p15i7i801.bkt.clouddn.com/149d38d31a5fa4e0576ff617c0408f08.png)
* 首先是红色的 $\alpha$，再者是黄色的学习率，第一、二动量及min-batch，最后再考虑调整层数及学习率延时 $\varepsilon$。
* 使用随机数、从粗到细
* 使用合适的分布尺寸，而不是使用均匀分布，比如对于 $\alpha$可以使用a log scala，举例而言：
  ```python
  r = -4*np.random.rand()
  alpha = 10^r
  ```

## 实践经验：两个流派
* 使用各领域的常见参数（**这意味需要对各个智能领域有的了解**）
* 直觉很有效，**定期更新参数，比如几个月**
* babysitting（pandan）: 长期照看，评估调整——（**不断的持久化模型吗？，一直迭代下去，还是每次都重新训练，模型可以以直训练下去吗？**）
* train in paraller: Caviar(鱼子酱)

## 网络内激活函数的normalizaion
![ng_batchnorm](http://p15i7i801.bkt.clouddn.com/eaa10f3214ad4cbd03b074755a13a8e0.png)
请注意以下三点：
* 通过 $\alpha、\beta$两个超参数，可以控制 $Z^i$ 的范围，如右上图所示
* 单层网络中我们通过normalizaion来加快梯度下降的速度，那么也可以通过控制隐藏层的输入值来控制
* andrew的推荐: Normalize $z$ rather than $a$

## 将Batch Norm拟合进神经网络
![ng_addBatchNorm](http://p15i7i801.bkt.clouddn.com/b398a029678304574bdf4e67d9ecc406.png)
* 偏执项b从此被标准化抵消，即式子可简化为 $z^l=w^l a^{l-1}$

## Batch Norm奏效的原因
* 加速梯度下降
* It makes weights, later or deeper than your network, more robost to changes to weights in earlier layers of neural netowork. 举例而言，对于不带颜色的猫可能训练得很好，当带上颜色后，对于同样判断是否为猫的模型而言，输入数据的分布变化了，导致模型需要重新训练。这也称为**covariate shift**，为解决这个问题，可以使用Batch Norm。
* 顺便的作用，如dropout，引入了噪声，相当于加入了regularization.

## 测试模型时BatchNorm的计算
* 需要估算平均值及标准差
* 简单靠谱的处理方式是用指数加权平均去**估算**
* 跑全量数据batchnorm也可以

## softmax分类:LR的一般形式
![ng_softmax](http://p15i7i801.bkt.clouddn.com/6d6b110e436fb245d51cbf1179d231d2.png)
