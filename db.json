{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"5a4ab475cc1f2119b2043c94519bd8a184f46992","modified":1512468766876},{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1512468451211},{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1512468451209},{"_id":"themes/next/.git","hash":"042ff34da0707513a5681580b37513c890c671ef","modified":1512711688940},{"_id":"themes/next/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1512468451212},{"_id":"themes/next/.hound.yml","hash":"289dcf5bfe92dbd680d54d6e0668f41c9c9c0c78","modified":1512468451219},{"_id":"themes/next/.gitignore","hash":"ee0b13c268cc8695d3883a5da84930af02d4ed08","modified":1512468451218},{"_id":"themes/next/.javascript_ignore","hash":"cd250ad74ca22bd2c054476456a73d9687f05f87","modified":1512468451219},{"_id":"themes/next/.jshintrc","hash":"b7d23f2ce8d99fa073f22f9960605f318acd7710","modified":1512468451220},{"_id":"themes/next/.travis.yml","hash":"6674fbdfe0d0c03b8a04527ffb8ab66a94253acd","modified":1512468451222},{"_id":"themes/next/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1512468451221},{"_id":"themes/next/README.cn.md","hash":"419b60d064a4ac66565ddeec1be55802acf68c8b","modified":1512468451224},{"_id":"themes/next/README.md","hash":"631d68e9cbced2f11cd976bf883b7d8b08b9b365","modified":1512468451225},{"_id":"themes/next/LICENSE","hash":"ec44503d7e617144909e54533754f0147845f0c5","modified":1512468451223},{"_id":"themes/next/_config.yml","hash":"55c41dd2fc38ff03b18187f34e5ebbaff9bd7dc4","modified":1513573813329},{"_id":"themes/next/bower.json","hash":"47471a8f13528dc4052b746db5b4be2375682173","modified":1512468451227},{"_id":"themes/next/package.json","hash":"39370e2aadf1f9a7c105edff064c6e47682b3932","modified":1512468451327},{"_id":"themes/next/gulpfile.coffee","hash":"412defab3d93d404b7c26aaa0279e2e586e97454","modified":1512468451228},{"_id":"source/_posts/action-in-machine-learning.md","hash":"54f0629ac32f37554b54c64ef798198e0394fa7d","modified":1513574555120},{"_id":"source/_posts/Neural-Networks-for-Machine-Learning.md","hash":"aa14e7b846cbf886c262d3ba8c1fe3967e99e8d2","modified":1513675361763},{"_id":"source/_posts/神经网络与深度学习.md","hash":"3e9caffe67a815a553d664e025fae509a1dcdaa2","modified":1513574555120},{"_id":"source/_posts/Practical-introduction-to-data-structures-and-algorithm-analysis.md","hash":"c30a5effe453d7c7a6499c4af8754a598c9c494c","modified":1513574555120},{"_id":"source/about/index.md","hash":"9433482f87818b221e552b822e9fcd5876191868","modified":1512552163070},{"_id":"source/tags/index.md","hash":"047bc48ea113247ae48eece5212d62ccae459654","modified":1512551944816},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"b1ec000babd42bb7ffd26f5ad8aac9b5bec79ae5","modified":1512468451215},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5adfad3ef1b870063e621bc0838268eb2c7c697a","modified":1512468451214},{"_id":"source/categories/index.md","hash":"050f7dc3ea25c3fa162d9720fa885e69fdc2962e","modified":1512552165295},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"1228506a940114288d61812bfe60c045a0abeac1","modified":1512468451216},{"_id":"themes/next/languages/default.yml","hash":"b3bcd8934327448a43d9bfada5dd11b1b8c1402e","modified":1512468451231},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1512468451217},{"_id":"themes/next/languages/de.yml","hash":"fd02d9c2035798d5dc7c1a96b4c3e24b05b31a47","modified":1512468451230},{"_id":"themes/next/languages/en.yml","hash":"2f4b4776ca1a08cc266a19afb0d1350a3926f42c","modified":1512468451231},{"_id":"themes/next/languages/id.yml","hash":"dccae33e2a5b3c9f11c0e05ec4a7201af1b25745","modified":1512468451233},{"_id":"themes/next/languages/it.yml","hash":"a215d016146b1bd92cef046042081cbe0c7f976f","modified":1512468451234},{"_id":"themes/next/languages/ja.yml","hash":"37f954e47a3bc669620ca559e3edb3b0072a4be5","modified":1512468451235},{"_id":"themes/next/languages/ko.yml","hash":"dc8f3e8c64eb7c4bb2385025b3006b8efec8b31d","modified":1512468451236},{"_id":"source/_posts/改善深层神经网络：超参数调试、正则化以及优化.md","hash":"55e0b29b7298c3dea235288a98fb85aa306b8f9c","modified":1513592875793},{"_id":"themes/next/languages/pt.yml","hash":"2efcd240c66ab1a122f061505ca0fb1e8819877b","modified":1512468451239},{"_id":"themes/next/languages/ru.yml","hash":"e33ee44e80f82e329900fc41eb0bb6823397a4d6","modified":1512468451239},{"_id":"themes/next/languages/pt-BR.yml","hash":"568d494a1f37726a5375b11452a45c71c3e2852d","modified":1512468451238},{"_id":"themes/next/languages/nl-NL.yml","hash":"213e7a002b82fb265f69dabafbbc382cfd460030","modified":1512468451237},{"_id":"themes/next/languages/vi.yml","hash":"a9b89ebd3e5933033d1386c7c56b66c44aca299a","modified":1512468451240},{"_id":"themes/next/languages/zh-hk.yml","hash":"fe0d45807d015082049f05b54714988c244888da","modified":1512468451242},{"_id":"themes/next/languages/zh-tw.yml","hash":"432463b481e105073accda16c3e590e54c8e7b74","modified":1512468451243},{"_id":"themes/next/layout/_layout.swig","hash":"2164570bb05db11ee4bcfbbb5d183a759afe9d07","modified":1512468451247},{"_id":"themes/next/languages/zh-Hans.yml","hash":"39e4d2087a8f529ad3dbe3a1d7f8e2e6d31d915a","modified":1513573813329},{"_id":"themes/next/languages/fr-FR.yml","hash":"efeeb55d5c4add54ad59a612fc0630ee1300388c","modified":1512468451232},{"_id":"themes/next/layout/archive.swig","hash":"9a2c14874a75c7085d2bada5e39201d3fc4fd2b4","modified":1512468451321},{"_id":"themes/next/layout/index.swig","hash":"555a357ecf17128db4e29346c92bb6298e66547a","modified":1512468451323},{"_id":"themes/next/layout/category.swig","hash":"3cbb3f72429647411f9e85f2544bdf0e3ad2e6b2","modified":1512468451322},{"_id":"themes/next/layout/schedule.swig","hash":"87ad6055df01fa2e63e51887d34a2d8f0fbd2f5a","modified":1512468451325},{"_id":"themes/next/layout/page.swig","hash":"e8fcaa641d46930237675d2ad4b56964d9e262e9","modified":1512468451324},{"_id":"themes/next/layout/tag.swig","hash":"34e1c016cbdf94a31f9c5d494854ff46b2a182e9","modified":1512468451326},{"_id":"themes/next/layout/post.swig","hash":"7a6ce102ca82c3a80f776e555dddae1a9981e1ed","modified":1512468451324},{"_id":"themes/next/scripts/merge-configs.js","hash":"5758f8f3f12d17bc80da65bb808a20b3a8aae186","modified":1512468451337},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1512468451347},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1512468451621},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1512468451623},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1512468451622},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512468451461},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1512468451245},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1512468451246},{"_id":"themes/next/layout/_macro/post.swig","hash":"4ba938822d56c597490f0731893eaa2443942e0f","modified":1512468451251},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"8c56dd26157cbc580ae41d97ac34b90ab48ced3f","modified":1512468451249},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"931808ad9b8d8390c0dcf9bdeb0954eeb9185d68","modified":1512468451275},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"b9f9959225876fb56fb3fba96306d19396e704d4","modified":1512468451253},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"c0f5a0955f69ca4ed9ee64a2d5f8aa75064935ad","modified":1512468451274},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9be624634703be496a5d2535228bc568a8373af9","modified":1512468451282},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"e2e4eae391476da994045ed4c7faf5e05aca2cd7","modified":1512468451254},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"f83befdc740beb8dc88805efd7fbb0fef9ed19be","modified":1512468451250},{"_id":"themes/next/layout/_macro/reward.swig","hash":"357d86ec9586705bfbb2c40a8c7d247a407db21a","modified":1512468451252},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"8301c9600bb3e47f7fb98b0e0332ef3c51bb1688","modified":1512468451308},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"fa882641da3bd83d9a58a8a97f9d4c62a9ee7b5c","modified":1512468451310},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"ba75672183d94f1de7c8bd0eeee497a58c70e889","modified":1512468451307},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"554ec568e9d2c71e4a624a8de3cb5929050811d6","modified":1512468451310},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"9a188938d46931d5f3882a140aa1c48b3a893f0c","modified":1512468451312},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"a0bd3388587fd943baae0d84ca779a707fbcad89","modified":1512468451309},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4adc65a602d1276615da3b887dcbf2ac68e7382b","modified":1512468451255},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"db15d7e1552aa2d2386a6b8a33b3b3a40bf9e43d","modified":1512468451311},{"_id":"themes/next/layout/_partials/head.swig","hash":"f14a39dad1ddd98e6d3ceb25dda092ba80d391b5","modified":1512468451257},{"_id":"themes/next/layout/_partials/header.swig","hash":"c54b32263bc8d75918688fb21f795103b3f57f03","modified":1512468451261},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"77c61e0baea3544df361b7338c3cd13dc84dde22","modified":1512468451262},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"1634fb887842698e01ff6e632597fe03c75d2d01","modified":1512468451263},{"_id":"themes/next/layout/_partials/search.swig","hash":"b4ebe4a52a3b51efe549dd1cdee846103664f5eb","modified":1512468451264},{"_id":"themes/next/layout/_partials/footer.swig","hash":"ba96bab0c127aea98c9fddf8ebe283fddb6d0a61","modified":1513573813329},{"_id":"themes/next/source/css/main.styl","hash":"a91dbb7ef799f0a171b5e726c801139efe545176","modified":1512468451461},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1512468451462},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1512468451464},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1512468451463},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1512468451465},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1512468451467},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1512468451467},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1512468451468},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1512468451469},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1512468451470},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1512468451472},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1512468451471},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1512468451473},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1512468451474},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1512468451475},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1512468451477},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1512468451476},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1512468451479},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1512468451478},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"99b66949f18398689b904907af23c013be1b978f","modified":1512468451347},{"_id":"themes/next/scripts/tags/button.js","hash":"eddbb612c15ac27faf11c59c019ce188f33dec2c","modified":1512468451347},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"ac681b0d0d8d39ba3817336c0270c6787c2b6b70","modified":1512468451347},{"_id":"themes/next/scripts/tags/label.js","hash":"6f00952d70aadece844ce7fd27adc52816cc7374","modified":1512468451347},{"_id":"themes/next/scripts/tags/full-image.js","hash":"c9f833158c66bd72f627a0559cf96550e867aa72","modified":1512468451347},{"_id":"themes/next/scripts/tags/exturl.js","hash":"5022c0ba9f1d13192677cf1fd66005c57c3d0f53","modified":1512468451347},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"bcba2ff25cd7850ce6da322d8bd85a8dd00b5ceb","modified":1512468451347},{"_id":"themes/next/scripts/tags/note.js","hash":"f7eae135f35cdab23728e9d0d88b76e00715faa0","modified":1512468451347},{"_id":"themes/next/scripts/tags/tabs.js","hash":"aa7fc94a5ec27737458d9fe1a75c0db7593352fd","modified":1512468451347},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512468451279},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512468451279},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512468451430},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512468451440},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512468451461},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512468451430},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512468451461},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"9b84ab576982b2c3bb0291da49143bc77fba3cc6","modified":1512468451276},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1512468451278},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1512468451281},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"4c501ea0b9c494181eb3c607c5526a5754e7fbd8","modified":1512468451299},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"4dcc3213c033994d342d02b800b6229295433d30","modified":1512468451301},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"1600f340e0225361580c44890568dc07dbcf2c89","modified":1512468451300},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"af7f3e43cbdc4f88c13f101f0f341af96ace3383","modified":1512468451302},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"493bd5999a1061b981922be92d8277a0f9152447","modified":1512468451303},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"9246162d4bc7e949ce1d12d135cbbaf5dc3024ec","modified":1512468451304},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"4050553d44ba1396174161c9a6bb0f89fa779eca","modified":1512468451305},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"ff947f3561b229bc528cb1837d4ca19612219411","modified":1512468451285},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"7e65ff8fe586cd655b0e9d1ad2912663ff9bd36c","modified":1512468451306},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"71397a5823e8ec8aad3b68aace13150623b3e19d","modified":1512468451286},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"753d262911c27baf663fcaf199267133528656af","modified":1512468451287},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"7b11eac3a0685fa1ab2ab6ecff60afc4f15f0d16","modified":1512468451288},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"7d94845f96197d9d84a405fa5d4ede75fb81b225","modified":1512468451290},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"ccc443b22bd4f8c7ac4145664686c756395b90e0","modified":1512468451291},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"a10b7f19d7b5725527514622899df413a34a89db","modified":1512468451289},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"b1e13df83fb2b1d5d513b30b7aa6158b0837daab","modified":1512468451292},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"45f3f629c2aacc381095750e1c8649041a71a84b","modified":1512468451293},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"8a399df90dadba5ad4e781445b58f4765aeb701e","modified":1512468451295},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"5a8027328f060f965b3014060bebec1d7cf149c1","modified":1512468451296},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"e6d10ee4fb70b3ae1cd37e9e36e000306734aa2e","modified":1512468451294},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"f9a1647a8f1866deeb94052d1f87a5df99cb1e70","modified":1512468451297},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"34599633658f3b0ffb487728b7766e1c7b551f5a","modified":1512468451317},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"93479642fd076a1257fecc25fcf5d20ccdefe509","modified":1512468451317},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1512468451318},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b83a51bbe0f1e2ded9819070840b0ea145f003a6","modified":1512468451299},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1512468451265},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a8c7f9ca7c605d039a1f3bf4e4d3183700a3dd62","modified":1512468451266},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1512468451267},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"d8c98938719284fa06492c114d99a1904652a555","modified":1512468451320},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"f5e487b0d213ca0bd94aa30bc23b240d65081627","modified":1512468451260},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"9e3d133ac5bcc6cb51702c83b2611a49811abad1","modified":1512468451269},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"d4fbffd7fa8f2090eb32a871872665d90a885fac","modified":1512468451271},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"d9e2d9282f9be6e04eae105964abb81e512bffed","modified":1512468451270},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"0a9cdd6958395fcdffc80ab60f0c6301b63664a5","modified":1512468451272},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1512468451259},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1512468451430},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1512468451430},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"0e55cbd93852dc3f8ccb44df74d35d9918f847e0","modified":1512468451440},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"7896c3ee107e1a8b9108b6019f1c070600a1e8cc","modified":1512468451440},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"58e7dd5947817d9fc30770712fc39b2f52230d1e","modified":1512468451461},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"a25408534f8fe6e321db4bbf9dd03335d648fe17","modified":1512468451461},{"_id":"themes/next/source/css/_variables/base.styl","hash":"b1f6ea881a4938a54603d68282b0f8efb4d7915d","modified":1512468451461},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"4069f918ccc312da86db6c51205fc6c6eaabb116","modified":1512468451461},{"_id":"themes/next/source/js/src/affix.js","hash":"1b509c3b5b290a6f4607f0f06461a0c33acb69b1","modified":1512468451482},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"b35a7dc47b634197b93487cea8671a40a9fdffce","modified":1512468451486},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"1512c751d219577d338ac0780fb2bbd9075d5298","modified":1512468451486},{"_id":"themes/next/source/js/src/motion.js","hash":"885176ed51d468f662fbf0fc09611f45c7e5a3b1","modified":1512468451487},{"_id":"themes/next/source/js/src/post-details.js","hash":"93a18271b4123dd8f94f09d1439b47c3c19a8712","modified":1512468451488},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"02cf91514e41200bc9df5d8bdbeb58575ec06074","modified":1512468451491},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"b7657be25fc52ec67c75ab5481bdcb483573338b","modified":1512468451492},{"_id":"themes/next/source/js/src/utils.js","hash":"b7ddc240208d57596a67c78a04a11b0f0bdecc97","modified":1512468451494},{"_id":"themes/next/source/js/src/exturl.js","hash":"a2a0f0de07e46211f74942a468f42ee270aa555c","modified":1512468451484},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"0289031200c3d4c2bdd801ee10fff13bb2c353e4","modified":1512468451484},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1512468451523},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1512468451523},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"b02737510e9b89aeed6b54f89f602a9c24b06ff2","modified":1512468451533},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"9be892a4e14e0da18ff9cb962c9ef71f163b1b22","modified":1512468451533},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1512468451553},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"672d3b5767e0eacd83bb41b188c913f2cf754793","modified":1512468451533},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1512468451563},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1512468451553},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"cb431b54ba9c692165a1f5a12e4c564a560f8058","modified":1512468451483},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1512468451563},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1512468451563},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"865d6c1328ab209a4376b9d2b7a7824369565f28","modified":1512468451574},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"bf3eef9d647cd7c9b62feda3bc708c6cdd7c0877","modified":1512468451553},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1512468451553},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"90fa628f156d8045357ff11eaf32e61abacf10e8","modified":1512468451574},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"68a9b9d53126405b0fa5f3324f1fb96dbcc547aa","modified":1512468451553},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"a9b3ee1e4db71a0e4ea6d5bed292d176dd68b261","modified":1512468451553},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4ded6fee668544778e97e38c2b211fc56c848e77","modified":1512468451574},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"f4a570908f6c89c6edfb1c74959e733eaadea4f2","modified":1512468451575},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"b930297cb98b8e1dbd5abe9bc1ed9d5935d18ce8","modified":1512468451574},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1512468451584},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"e0acf1db27b0cc16128a59c46db1db406b5c4c58","modified":1512468451574},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1512468451585},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1512468451587},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"bf773ad48a0b9aa77681a89d7569eefc0f7b7b18","modified":1512468451576},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1512468451591},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1512468451588},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1512468451590},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1512468451589},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1512468451595},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1512468451592},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1512468451593},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1512468451594},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"8aaa675f577d5501f5f22d5ccb07c2b76310b690","modified":1512468451598},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1512468451596},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1512468451597},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"2d9a9f38c493fdf7c0b833bb9184b6a1645c11b2","modified":1512468451600},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"8148492dd49aa876d32bb7d5b728d3f5bf6f5074","modified":1512468451602},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"46a50b91c98b639c9a2b9265c5a1e66a5c656881","modified":1512468451601},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"63da5e80ebb61bb66a2794d5936315ca44231f0c","modified":1512468451612},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"92d92860418c4216aa59eb4cb4a556290a7ad9c3","modified":1512468451612},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1512468451616},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1512468451617},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1512468451619},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"14264a210bf94232d58d7599ea2ba93bfa4fb458","modified":1512468451579},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"e33aa8fa48b6639d8d8b937d13261597dd473b3a","modified":1512468451580},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"2ce5f3bf15c523b9bfc97720d8884bb22602a454","modified":1512468451582},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1512468451574},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1512468451316},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"24ee4b356ff55fc6e58f26a929fa07750002cf29","modified":1512468451430},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"218cc936ba3518a3591b2c9eda46bc701edf7710","modified":1512468451315},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"12662536c7a07fff548abe94171f34b768dd610f","modified":1512468451420},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"91ca75492cd51f2553f4d294ed2f48239fcd55eb","modified":1512468451430},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"1da5c800d025345f212a3bf1be035060f4e5e6ed","modified":1512468451430},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"3f40e8a9fe8e7bd5cfc4cf4cbbbcb9539462e973","modified":1512468451430},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"8f86f694c0749a18ab3ad6f6df75466ca137a4bc","modified":1512468451357},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"237d185ac62ec9877e300947fa0109c44fb8db19","modified":1512468451357},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1512468451430},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"8b32928686c327151e13d3ab100157f9a03cd59f","modified":1512468451357},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"ea9069645696f86c5df64208490876fe150c8cae","modified":1512468451430},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1512468451357},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"7ad4081466b397e2a6204141bb7768b7c01bd93c","modified":1512468451357},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"4f2801fc4cf3f31bf2069f41db8c6ce0e3da9e39","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"6eb4bcc3056bd279d000607e8b4dad50d368ca69","modified":1512468451390},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"a98ad885ee4f48d85b2578a0b9c2bbf166e96733","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"31127dcbf4c7b4ada53ffbf1638b5fe325b7cbc0","modified":1512468451450},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"748dbfbf9c08e719ddc775958003c64b00d39dab","modified":1512468451450},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"d0bfd1bef988c76f7d7dd72d88af6f0908a8b0db","modified":1512468451440},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"25d5e45a355ee2093f3b8b8eeac125ebf3905026","modified":1512468451440},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1512468451440},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"9c99034f8e00d47e978b3959f51eb4a9ded0fcc8","modified":1512468451450},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1512468451450},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9b913b73d31d21f057f97115ffab93cfa578b884","modified":1512468451450},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"26666c1f472bf5f3fb9bc62081cca22b4de15ccb","modified":1512468451450},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"60fa84aa7731760f05f52dd7d8f79b5f74ac478d","modified":1512468451440},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"f1d0b5d7af32c423eaa8bb93ab6a0b45655645dc","modified":1512468451490},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"bce344d3a665b4c55230d2a91eac2ad16d6f32fd","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"416988dca389e6e2fdfa51fa7f4ee07eb53f82fb","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"4642e30010af8b2b037f5b43146b10a934941958","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"30561ed60fc64f3e4ce85143bdb55faa814743a6","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"86197902dfd3bededba10ba62b8f9f22e0420bde","modified":1512468451461},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"6c26cdb36687d4f0a11dabf5290a909c3506be5c","modified":1512468451503},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"6d586bfcfb7ae48f1b12f76eec82d3ad31947501","modified":1512468451523},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"16b03db23a52623348f37c04544f2792032c1fb6","modified":1512468451523},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1512468451533},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1512468451533},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1512468451533},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1512468451533},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1512468451533},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1512468451533},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"82f33ad0842aa9c154d029e0dada2497d4eb1d57","modified":1512468451543},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"d71602cbca33b9ecdb7ab291b7f86a49530f3601","modified":1512468451543},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"ae6318aeb62ad4ce7a7e9a4cdacd93ffb004f0fb","modified":1512468451543},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1512468451563},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1512468451563},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1512468451563},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"1d6aeda0480d0e4cb6198edf7719d601d4ae2ccc","modified":1512468451553},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1512468451553},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1512468451609},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1512468451610},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"4ac683b2bc8531c84d98f51b86957be0e6f830f3","modified":1512468451513},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1512468451574},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1512468451574},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1512468451615},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"9f73c4696f0907aa451a855444f88fc0698fa472","modified":1512468451357},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"53cde051e0337f4bf42fb8d6d7a79fa3fa6d4ef2","modified":1512468451367},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"1a0d059799a298fe17c49a44298d32cebde93785","modified":1512468451367},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1512468451367},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1512468451367},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"b6f3a06a94a6ee5470c956663164d58eda818a64","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"0656e753f182c9f47fef7304c847b7587a85ef0d","modified":1512468451367},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"f9760ecf186954cee3ba4a149be334e9ba296b89","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"8cf318644acc8b4978537c263290363e21c7f5af","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"167986d0f649516671ddf7193eebba7b421cd115","modified":1512468451367},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"50450d9fdc8a2b2be8cfca51e3e1a01ffd636c0b","modified":1512468451367},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1512468451367},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"875cbe88d5c7f6248990e2beb97c9828920e7e24","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a200c0a1c5a895ac9dc41e0641a5dfcd766be99b","modified":1512468451369},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"caf263d1928496688c0e1419801eafd7e6919ce5","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"d0d7a5c90d62b685520d2b47fea8ba6019ff5402","modified":1512468451371},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"a6c6eb8adba0a090ad1f4b9124e866887f20d10d","modified":1512468451370},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"27deb3d3a243d30022055dac7dad851024099a8b","modified":1512468451372},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"cd9e214e502697f2f2db84eb721bac57a49b0fce","modified":1512468451371},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ca88ea6999a61fb905eb6e72eba5f92d4ee31e6e","modified":1512468451373},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1512468451374},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1512468451375},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"ccb34c52be8adba5996c6b94f9e723bd07d34c16","modified":1512468451376},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1512468451377},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"7968343e41f8b94b318c36289dff1196c3eb1791","modified":1512468451378},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"89d6c3b697efc63de42afd2e89194b1be14152af","modified":1512468451379},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"39f04c4c7237a4e10acd3002331992b79945d241","modified":1512468451381},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"8dd9a1c6f4f6baa00c2cf01837e7617120cf9660","modified":1512468451383},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"761eba9811b050b25d548cc0854de4824b41eb08","modified":1512468451382},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"61f8cea3c01acd600e90e1bc2a07def405503748","modified":1512468451384},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"1153bb71edf253765145559674390e16dd67c633","modified":1512468451385},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"28a8737c090fbffd188d73a00b42e90b9ee57df2","modified":1512468451386},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"5ef6343835f484a2c0770bd1eb9cc443609e4c39","modified":1512468451388},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a1521d48bb06d8d703753f52a198baa197af7da2","modified":1512468451387},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"e71652d3216e289c8548b1ea2357822c1476a425","modified":1512468451389},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1512468451395},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"3159b55f35c40bd08e55b00148c523760a708c51","modified":1512468451393},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1512468451391},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1512468451394},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"b7076e58d647265ee0ad2b461fe8ce72c9373bc5","modified":1512468451396},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"9a409b798decdefdaf7a23f0b11004a8c27e82f3","modified":1512468451397},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"154a87a32d2fead480d5e909c37f6c476671c5e6","modified":1512468451398},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"b80604868e4f5cf20fccafd7ee415c20c804f700","modified":1512468451399},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"bba4f3bdb7517cd85376df3e1209b570c0548c69","modified":1512468451410},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"a6e7d698702c2e383dde3fde2abde27951679084","modified":1512468451420},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"5dbeed535d63a50265d96b396a5440f9bb31e4ba","modified":1512468451410},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"10599e16414a8b7a76c4e79e6617b5fe3d4d1adf","modified":1512468451420},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"874278147115601d2abf15987f5f7a84ada1ac6b","modified":1512468451420},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"717cc7f82be9cc151e23a7678601ff2fd3a7fa1d","modified":1512468451420},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"15975ba7456b96916b1dbac448a1a0d2c38b8f3d","modified":1512468451420},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"9c8196394a89dfa40b87bf0019e80144365a9c93","modified":1512468451420},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1512468451450},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1512468451450},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"11c22f0fb3f6beb13e5a425ec064a4ff974c13b7","modified":1512468451383},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"a3bdd71237afc112b2aa255f278cab6baeb25351","modified":1512468451392},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"16087276945fa038f199692e3eabb1c52b8ea633","modified":1512468451420},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"28825ae15fa20ae3942cdaa7bcc1f3523ce59acc","modified":1512468451420},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1512468451498},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1512468451500},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1512468451499},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1512468451501},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1512468451502},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1512468451543},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"6394c48092085788a8c0ef72670b0652006231a1","modified":1512468451543},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"ee948b4489aedeb548a77c9e45d8c7c5732fd62d","modified":1512468451543},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"51139a4c79573d372a347ef01a493222a1eaf10a","modified":1512468451543},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1512468451543},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1512468451573},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1512468451573},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1512468451573},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"d22b1629cb23a6181bebb70d0cf653ffe4b835c8","modified":1512468451543},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"90a1b22129efc172e2dfcceeeb76bff58bc3192f","modified":1512468451523},{"_id":"themes/next/source/lib/three/three.min.js","hash":"26273b1cb4914850a89529b48091dc584f2c57b8","modified":1512468451606},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1512468451573},{"_id":"public/search.xml","hash":"1565f2fa5226b2450ddd70d88ca87a696870ac2a","modified":1513675385755},{"_id":"public/tags/index.html","hash":"13d00529d92e3c5d200ac113d89dc1725b8e64fc","modified":1513675385892},{"_id":"public/about/index.html","hash":"548cf883fb979b42e03e9f9a5354d5e1e724cfe2","modified":1513675385892},{"_id":"public/categories/index.html","hash":"39b67a778c260c564e9971b29cc391e8bf1fe495","modified":1513675385892},{"_id":"public/blog/Practical-introduction-to-data-structures-and-algorithm-analysis.html","hash":"86b65a5be11c894e8dddb37fdc4ad031e667f61c","modified":1513675385892},{"_id":"public/blog/改善深层神经网络：超参数调试、正则化以及优化.html","hash":"3ad40172b0155e3e4a8db2a1b0b733874f780ca2","modified":1513675385893},{"_id":"public/blog/神经网络与深度学习.html","hash":"83dfb174ec37d44b5f98d7b1bc9d2402db15a2a5","modified":1513675385893},{"_id":"public/blog/action-in-machine-learning.html","hash":"d01387bb37087a7adb1d8e3c08be6cda62a6c06a","modified":1513675385893},{"_id":"public/blog/Neural-Networks-for-Machine-Learning.html","hash":"346e246f17fb31ebebc75403d733e166f12e6259","modified":1513675385893},{"_id":"public/archives/index.html","hash":"ad76908c89629e5b8a7cf02ccabadd30ebb54843","modified":1513675385893},{"_id":"public/archives/2016/index.html","hash":"2ac17c425ad794b19983dbee0acaf1a889e3beaa","modified":1513675385893},{"_id":"public/archives/2016/06/index.html","hash":"83d22267902e90bbb79b7b28704416740a6a01da","modified":1513675385893},{"_id":"public/archives/2017/index.html","hash":"2bfabcfa3a5a28991b79726adee7e72a82b492f1","modified":1513675385893},{"_id":"public/archives/2017/08/index.html","hash":"ed240ad0de7e8ce960b3cdc6caae056c4f3c1088","modified":1513675385893},{"_id":"public/archives/2017/09/index.html","hash":"4444862333a2bdaddd98aeaf00ed2369bab2f49f","modified":1513675385893},{"_id":"public/categories/AI梦/index.html","hash":"81c17b23c8fc440b079284b16967004f8c31142c","modified":1513675385893},{"_id":"public/categories/深耕码农/index.html","hash":"98745435fbb5f7e93656456c75958422a0141202","modified":1513675385893},{"_id":"public/index.html","hash":"a345350184d460014b75cf22bc52b76933cd917e","modified":1513675385893},{"_id":"public/tags/深度学习/index.html","hash":"64388539381a87e55785e6f667b0608d46ca1006","modified":1513675385893},{"_id":"public/tags/Andrew-NG/index.html","hash":"751ba22ec053290cc0d0436ba7f5d0c8a4b0f393","modified":1513675385893},{"_id":"public/tags/公开课/index.html","hash":"fe652174e71632da582c391f265f4641a8c86371","modified":1513675385893},{"_id":"public/tags/数据结构/index.html","hash":"f8af9a229f6129e327fefb740f83025b418390c3","modified":1513675385893},{"_id":"public/tags/经典著作/index.html","hash":"aea3ef83d4ef3280d85a62b6403f9bb556c6869d","modified":1513675385893},{"_id":"public/tags/机器学习/index.html","hash":"228a96ffa02aaf7f645f71b6a61f45d2570c749d","modified":1513675385894},{"_id":"public/tags/神经网络/index.html","hash":"c8cf7c112faa9830e6f4acfff8416c22ab9f6407","modified":1513675385894},{"_id":"public/tags/Geoffrey-hinton/index.html","hash":"caebc26fcf3d07795d422fc48173599eee7bb2fa","modified":1513675385894}],"Category":[{"name":"AI梦","_id":"cjbdf8izf0004icn7f17zty9r"},{"name":"深耕码农","_id":"cjbdf8izl000aicn7c665rfsh"}],"Data":[],"Page":[{"title":"tags","date":"2017-12-06T09:18:31.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2017-12-06 17:18:31\ntype: \"tags\"\n---\n","updated":"2017-12-06T09:19:04.816Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjbdf8izb0001icn73mwzrhy1","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"about","date":"2017-12-06T09:20:41.000Z","_content":"\n\n## 关于我\n\n\nQQ：563593589\nEmail: zw_kprs@126.com","source":"about/index.md","raw":"---\ntitle: about\ndate: 2017-12-06 17:20:41\n---\n\n\n## 关于我\n\n\nQQ：563593589\nEmail: zw_kprs@126.com","updated":"2017-12-06T09:22:43.070Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjbdf8izd0003icn7cysr7m3r","content":"<h2 id=\"关于我\"><a href=\"#关于我\" class=\"headerlink\" title=\"关于我\"></a>关于我</h2><p>QQ：563593589<br>Email: zw_kprs@126.com</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"关于我\"><a href=\"#关于我\" class=\"headerlink\" title=\"关于我\"></a>关于我</h2><p>QQ：563593589<br>Email: zw_kprs@126.com</p>\n"},{"title":"categories","date":"2017-12-06T09:19:28.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2017-12-06 17:19:28\ntype: \"categories\"\n---\n","updated":"2017-12-06T09:22:45.295Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjbdf8izi0007icn7h05z34wd","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"神经网络与深度学习","date":"2017-08-30T09:20:41.000Z","mathjax":true,"_content":"\n\n### 第一、二周：概论与基础\n* sigmoid在逻辑回归里为什么没用被ReLU替代\n\n* LR中损失函数不用平方误差，为什么它是非凸的。\n  >因为加入了Sigmoid函数，\n  >$$ a= \\sigma (z) $$\n  >Sigmoid几个特点：\n  >$$ da/dz = z(1-z), dL/dz= a - y \\quad  if \\quad L=-( y log a + (1-y)log(1-a) ) $$\n\n* 避免矩阵运算使用For循环，100m时差300倍效率，因为 numpy使用到了CPU\\GPU的SIMD指令，运行速度更快。同时，对于numpy有内置的函数也一样，尽量使用，如log,max  etc..\n\n* 在python中，z= np.dot(w.T, x) + b , b会自动扩展成一个n维向量，这在python称为broadcast，一般性则:http://localhost:8888/?token=3d28e3222659669a680a9f9e82221436ef9\n\n* reshape是o(1)操作，多多益善，尽量不使用rank为1的数组。\n\n### 第三周 浅层神经网络\n1. 二级神经网络，即只有一个隐藏层的神经网络。\n2. 输出层通常使用sigmoid函数，而隐藏层不用，一般至少使用tanh来代替，因为它的输出介于-1与1之间，这样均值为0，便于下一层的计算。考虑到梯度下降，sigmoid与tanh在最大小值处导数变化小，reLU及leaky reLU更常用。当然这一切根据实际情况调整。\n3. 隐藏层不应使用线性激活函数，若用则相当于没有隐藏层，输出层若要出回归值，则可使用。\n4. 随机w,并乘以较小的系数，一般让随机参数比较小。\n\n### 第四周 深层神经网络\n1. hyper parameter：比如层数、迭代次数、下降速率、激活函数的选择、batch size等等\n2. parameter: w,b  被hyper control\n","source":"_posts/神经网络与深度学习.md","raw":"---\ntitle: 神经网络与深度学习\ndate: 2017-08-30 17:20:41\ntags:\n      - 深度学习\n      - Andrew NG\n      - 公开课\ncategories: AI梦\nmathjax: true\n---\n\n\n### 第一、二周：概论与基础\n* sigmoid在逻辑回归里为什么没用被ReLU替代\n\n* LR中损失函数不用平方误差，为什么它是非凸的。\n  >因为加入了Sigmoid函数，\n  >$$ a= \\sigma (z) $$\n  >Sigmoid几个特点：\n  >$$ da/dz = z(1-z), dL/dz= a - y \\quad  if \\quad L=-( y log a + (1-y)log(1-a) ) $$\n\n* 避免矩阵运算使用For循环，100m时差300倍效率，因为 numpy使用到了CPU\\GPU的SIMD指令，运行速度更快。同时，对于numpy有内置的函数也一样，尽量使用，如log,max  etc..\n\n* 在python中，z= np.dot(w.T, x) + b , b会自动扩展成一个n维向量，这在python称为broadcast，一般性则:http://localhost:8888/?token=3d28e3222659669a680a9f9e82221436ef9\n\n* reshape是o(1)操作，多多益善，尽量不使用rank为1的数组。\n\n### 第三周 浅层神经网络\n1. 二级神经网络，即只有一个隐藏层的神经网络。\n2. 输出层通常使用sigmoid函数，而隐藏层不用，一般至少使用tanh来代替，因为它的输出介于-1与1之间，这样均值为0，便于下一层的计算。考虑到梯度下降，sigmoid与tanh在最大小值处导数变化小，reLU及leaky reLU更常用。当然这一切根据实际情况调整。\n3. 隐藏层不应使用线性激活函数，若用则相当于没有隐藏层，输出层若要出回归值，则可使用。\n4. 随机w,并乘以较小的系数，一般让随机参数比较小。\n\n### 第四周 深层神经网络\n1. hyper parameter：比如层数、迭代次数、下降速率、激活函数的选择、batch size等等\n2. parameter: w,b  被hyper control\n","slug":"神经网络与深度学习","published":1,"updated":"2017-12-18T05:22:35.120Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbdf8iz70000icn77a5j9k1h","content":"<h3 id=\"第一、二周：概论与基础\"><a href=\"#第一、二周：概论与基础\" class=\"headerlink\" title=\"第一、二周：概论与基础\"></a>第一、二周：概论与基础</h3><ul>\n<li><p>sigmoid在逻辑回归里为什么没用被ReLU替代</p>\n</li>\n<li><p>LR中损失函数不用平方误差，为什么它是非凸的。</p>\n<blockquote>\n<p>因为加入了Sigmoid函数，</p>\n<script type=\"math/tex; mode=display\">a= \\sigma (z)</script><p>Sigmoid几个特点：</p>\n<script type=\"math/tex; mode=display\">da/dz = z(1-z), dL/dz= a - y \\quad  if \\quad L=-( y log a + (1-y)log(1-a) )</script></blockquote>\n</li>\n<li><p>避免矩阵运算使用For循环，100m时差300倍效率，因为 numpy使用到了CPU\\GPU的SIMD指令，运行速度更快。同时，对于numpy有内置的函数也一样，尽量使用，如log,max  etc..</p>\n</li>\n<li><p>在python中，z= np.dot(w.T, x) + b , b会自动扩展成一个n维向量，这在python称为broadcast，一般性则:<a href=\"http://localhost:8888/?token=3d28e3222659669a680a9f9e82221436ef9\" target=\"_blank\" rel=\"noopener\">http://localhost:8888/?token=3d28e3222659669a680a9f9e82221436ef9</a></p>\n</li>\n<li><p>reshape是o(1)操作，多多益善，尽量不使用rank为1的数组。</p>\n</li>\n</ul>\n<h3 id=\"第三周-浅层神经网络\"><a href=\"#第三周-浅层神经网络\" class=\"headerlink\" title=\"第三周 浅层神经网络\"></a>第三周 浅层神经网络</h3><ol>\n<li>二级神经网络，即只有一个隐藏层的神经网络。</li>\n<li>输出层通常使用sigmoid函数，而隐藏层不用，一般至少使用tanh来代替，因为它的输出介于-1与1之间，这样均值为0，便于下一层的计算。考虑到梯度下降，sigmoid与tanh在最大小值处导数变化小，reLU及leaky reLU更常用。当然这一切根据实际情况调整。</li>\n<li>隐藏层不应使用线性激活函数，若用则相当于没有隐藏层，输出层若要出回归值，则可使用。</li>\n<li>随机w,并乘以较小的系数，一般让随机参数比较小。</li>\n</ol>\n<h3 id=\"第四周-深层神经网络\"><a href=\"#第四周-深层神经网络\" class=\"headerlink\" title=\"第四周 深层神经网络\"></a>第四周 深层神经网络</h3><ol>\n<li>hyper parameter：比如层数、迭代次数、下降速率、激活函数的选择、batch size等等</li>\n<li>parameter: w,b  被hyper control</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"第一、二周：概论与基础\"><a href=\"#第一、二周：概论与基础\" class=\"headerlink\" title=\"第一、二周：概论与基础\"></a>第一、二周：概论与基础</h3><ul>\n<li><p>sigmoid在逻辑回归里为什么没用被ReLU替代</p>\n</li>\n<li><p>LR中损失函数不用平方误差，为什么它是非凸的。</p>\n<blockquote>\n<p>因为加入了Sigmoid函数，</p>\n<script type=\"math/tex; mode=display\">a= \\sigma (z)</script><p>Sigmoid几个特点：</p>\n<script type=\"math/tex; mode=display\">da/dz = z(1-z), dL/dz= a - y \\quad  if \\quad L=-( y log a + (1-y)log(1-a) )</script></blockquote>\n</li>\n<li><p>避免矩阵运算使用For循环，100m时差300倍效率，因为 numpy使用到了CPU\\GPU的SIMD指令，运行速度更快。同时，对于numpy有内置的函数也一样，尽量使用，如log,max  etc..</p>\n</li>\n<li><p>在python中，z= np.dot(w.T, x) + b , b会自动扩展成一个n维向量，这在python称为broadcast，一般性则:<a href=\"http://localhost:8888/?token=3d28e3222659669a680a9f9e82221436ef9\" target=\"_blank\" rel=\"noopener\">http://localhost:8888/?token=3d28e3222659669a680a9f9e82221436ef9</a></p>\n</li>\n<li><p>reshape是o(1)操作，多多益善，尽量不使用rank为1的数组。</p>\n</li>\n</ul>\n<h3 id=\"第三周-浅层神经网络\"><a href=\"#第三周-浅层神经网络\" class=\"headerlink\" title=\"第三周 浅层神经网络\"></a>第三周 浅层神经网络</h3><ol>\n<li>二级神经网络，即只有一个隐藏层的神经网络。</li>\n<li>输出层通常使用sigmoid函数，而隐藏层不用，一般至少使用tanh来代替，因为它的输出介于-1与1之间，这样均值为0，便于下一层的计算。考虑到梯度下降，sigmoid与tanh在最大小值处导数变化小，reLU及leaky reLU更常用。当然这一切根据实际情况调整。</li>\n<li>隐藏层不应使用线性激活函数，若用则相当于没有隐藏层，输出层若要出回归值，则可使用。</li>\n<li>随机w,并乘以较小的系数，一般让随机参数比较小。</li>\n</ol>\n<h3 id=\"第四周-深层神经网络\"><a href=\"#第四周-深层神经网络\" class=\"headerlink\" title=\"第四周 深层神经网络\"></a>第四周 深层神经网络</h3><ol>\n<li>hyper parameter：比如层数、迭代次数、下降速率、激活函数的选择、batch size等等</li>\n<li>parameter: w,b  被hyper control</li>\n</ol>\n"},{"title":"Practical introduction to data structures and algorithm analysis","date":"2017-09-28T03:12:18.000Z","mathjax":true,"_content":"\n# 概率论及一些基础思考\n## 代价与效益\n在有效的资源下将问题解决，称该算法是有效的。一个好的数据结构（解决方案）设计首先要考虑到业务场景，在操作上（增删改查），它有什么需求。比如查询上要求并发高、但在插入速度上并无太高的要求，或者反之。\n## 计算复杂度\n* 通常我们考虑算法的在一定数据规模下的平均复杂度，但平均复杂度分析并不是一定可靠的，这建立在输入数据概率分布已知的情况下考虑的，通常这种假设每个元素出现的概率均等。但是不正确的假设也可能带来灾难的性的后果，特别是在搜索类算法上，如散列和搜索树，具体事例可参考9.4、5.4节。有时间考虑特殊的数据分布，也会带来好处，如9.2节（自组织线性表）\n* 在实时系统中不考虑平均情况，而考虑最差的情况。\n* 复杂度分析，通常采用渐近分析的方法，使用化简的方法计算上界，下界和平均情况。\n\n## 最完美算法\n联机算法——举例而言，最大子序列求和的问题\n## 运行时间中的对数\n* 如二叉树，contains操作，较明显，在常数据O(1)时间问题的大小缩减为其一部分（通过是一半或小于一半），则该算法是O(logN)\n* 最大公因(约）数，即使用欧几里德/辗转相除法计算gcd(M,N)。由定理，若M>N，则有M mod N < M/2，可得该算法迭代次数至多是O(logN)\n* 幂次运算，使用递归分解，将乘法降到O(logN)\n\n# 基本数据结构\n## 表、栈、队列\n## 树\n\n### 二叉树\n\n可以使用栈来实现表达式树，也可用递归，这样会方便些。基本操作包括增(insert)、删(remove,两种可能性)、查(contains, findmin,findmax)，平均计算复杂度：\n\n得O(nlogN)，但这无法控制上界，**为控制任意次连续M次操作在最坏的情况下花费时间为O(MlogN)，引入AVL平衡树，当然它无法保证单次的操作的时间界。最后根据二八法则，将引入伸展树**\n### AVL平衡树\n\n  使用单旋和双旋调整高度，结构上每个节点信息加入了高度。核心插入代码：\n```C++\n    void insert( const Comparable & x, AvlNode * & t )\n    {\n        if( t == NULL )\n            t = new AvlNode( x, NULL, NULL );\n        else if( x < t->element )\n        {\n            insert( x, t->left );\n            if( height( t->left ) - height( t->right ) == 2 )\n                if( x < t->left->element )\n                    rotateWithLeftChild( t );\n                else\n                    doubleWithLeftChild( t );\n        }\n        else if( t->element < x )\n        {\n            insert( x, t->right );\n            if( height( t->right ) - height( t->left ) == 2 )\n                if( t->right->element < x )\n                    rotateWithRightChild( t );\n                else\n                    doubleWithRightChild( t );\n        }\n        else\n            ;  // Duplicate; do nothing\n        t->height = max( height( t->left ), height( t->right ) ) + 1;\n    }\n```\n### 伸展树\n之形伸展与一字伸展，目标是沿访问路径旋转  \n### 树的遍历\n### B树——数据库的索引树，用于解决大量数据磁盘检索的问题\n注意B树的定义，5个特性，M\\L参数的确定。如一个区块8K，每个键32B，在一颗M阶的B树中有M-1个键，则可算出M。对于叶子节点若每个记录是256字节，区块是8K，则可算出L=32。再回来看定义：\n1. 数据在叶子节点，并在相同深度上，且数据项介于L/2到L之间；\n2. 非叶节点，存储M-1个键，指示查找方向，其儿子数在M/2到M之间;\n注意B树插入时的分裂操作和删除的认领操作。\n\n### STL中set和map的树形实现\n### 散列\n1. 散列函数:利用horner法则，计算多项式函数$h_k=k_0+37k_1+37k_2=(37+37k_2)k_1+k_0$。通常建议桶大小为素数\n2. 冲突解决方法：分离链接、线性/平方探测、双散列。分离链接法因子可以到1，性能下降不明显，探测方法最好不超过0.5，双散列宜用质数。\n3. 再散列：一般使用途中策略，到达装载因子再散列\n4. hashmap, hashset\n5. 散列比起二叉树而言速度虽有所提升但缺乏排序，通常用在比如编译器的符号表，拼写检查等等  \n\n### 优先队列（堆）\n\n##  排序与检索\n## 高级话题\n","source":"_posts/Practical-introduction-to-data-structures-and-algorithm-analysis.md","raw":"---\ntitle: Practical introduction to data structures and algorithm analysis\ndate: 2017-09-28 11:12:18\ntags:\n      - 数据结构\n      - 经典著作\ncategories: 深耕码农\nmathjax: true\n---\n\n# 概率论及一些基础思考\n## 代价与效益\n在有效的资源下将问题解决，称该算法是有效的。一个好的数据结构（解决方案）设计首先要考虑到业务场景，在操作上（增删改查），它有什么需求。比如查询上要求并发高、但在插入速度上并无太高的要求，或者反之。\n## 计算复杂度\n* 通常我们考虑算法的在一定数据规模下的平均复杂度，但平均复杂度分析并不是一定可靠的，这建立在输入数据概率分布已知的情况下考虑的，通常这种假设每个元素出现的概率均等。但是不正确的假设也可能带来灾难的性的后果，特别是在搜索类算法上，如散列和搜索树，具体事例可参考9.4、5.4节。有时间考虑特殊的数据分布，也会带来好处，如9.2节（自组织线性表）\n* 在实时系统中不考虑平均情况，而考虑最差的情况。\n* 复杂度分析，通常采用渐近分析的方法，使用化简的方法计算上界，下界和平均情况。\n\n## 最完美算法\n联机算法——举例而言，最大子序列求和的问题\n## 运行时间中的对数\n* 如二叉树，contains操作，较明显，在常数据O(1)时间问题的大小缩减为其一部分（通过是一半或小于一半），则该算法是O(logN)\n* 最大公因(约）数，即使用欧几里德/辗转相除法计算gcd(M,N)。由定理，若M>N，则有M mod N < M/2，可得该算法迭代次数至多是O(logN)\n* 幂次运算，使用递归分解，将乘法降到O(logN)\n\n# 基本数据结构\n## 表、栈、队列\n## 树\n\n### 二叉树\n\n可以使用栈来实现表达式树，也可用递归，这样会方便些。基本操作包括增(insert)、删(remove,两种可能性)、查(contains, findmin,findmax)，平均计算复杂度：\n\n得O(nlogN)，但这无法控制上界，**为控制任意次连续M次操作在最坏的情况下花费时间为O(MlogN)，引入AVL平衡树，当然它无法保证单次的操作的时间界。最后根据二八法则，将引入伸展树**\n### AVL平衡树\n\n  使用单旋和双旋调整高度，结构上每个节点信息加入了高度。核心插入代码：\n```C++\n    void insert( const Comparable & x, AvlNode * & t )\n    {\n        if( t == NULL )\n            t = new AvlNode( x, NULL, NULL );\n        else if( x < t->element )\n        {\n            insert( x, t->left );\n            if( height( t->left ) - height( t->right ) == 2 )\n                if( x < t->left->element )\n                    rotateWithLeftChild( t );\n                else\n                    doubleWithLeftChild( t );\n        }\n        else if( t->element < x )\n        {\n            insert( x, t->right );\n            if( height( t->right ) - height( t->left ) == 2 )\n                if( t->right->element < x )\n                    rotateWithRightChild( t );\n                else\n                    doubleWithRightChild( t );\n        }\n        else\n            ;  // Duplicate; do nothing\n        t->height = max( height( t->left ), height( t->right ) ) + 1;\n    }\n```\n### 伸展树\n之形伸展与一字伸展，目标是沿访问路径旋转  \n### 树的遍历\n### B树——数据库的索引树，用于解决大量数据磁盘检索的问题\n注意B树的定义，5个特性，M\\L参数的确定。如一个区块8K，每个键32B，在一颗M阶的B树中有M-1个键，则可算出M。对于叶子节点若每个记录是256字节，区块是8K，则可算出L=32。再回来看定义：\n1. 数据在叶子节点，并在相同深度上，且数据项介于L/2到L之间；\n2. 非叶节点，存储M-1个键，指示查找方向，其儿子数在M/2到M之间;\n注意B树插入时的分裂操作和删除的认领操作。\n\n### STL中set和map的树形实现\n### 散列\n1. 散列函数:利用horner法则，计算多项式函数$h_k=k_0+37k_1+37k_2=(37+37k_2)k_1+k_0$。通常建议桶大小为素数\n2. 冲突解决方法：分离链接、线性/平方探测、双散列。分离链接法因子可以到1，性能下降不明显，探测方法最好不超过0.5，双散列宜用质数。\n3. 再散列：一般使用途中策略，到达装载因子再散列\n4. hashmap, hashset\n5. 散列比起二叉树而言速度虽有所提升但缺乏排序，通常用在比如编译器的符号表，拼写检查等等  \n\n### 优先队列（堆）\n\n##  排序与检索\n## 高级话题\n","slug":"Practical-introduction-to-data-structures-and-algorithm-analysis","published":1,"updated":"2017-12-18T05:22:35.120Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbdf8izc0002icn7fp0zjzj3","content":"<h1 id=\"概率论及一些基础思考\"><a href=\"#概率论及一些基础思考\" class=\"headerlink\" title=\"概率论及一些基础思考\"></a>概率论及一些基础思考</h1><h2 id=\"代价与效益\"><a href=\"#代价与效益\" class=\"headerlink\" title=\"代价与效益\"></a>代价与效益</h2><p>在有效的资源下将问题解决，称该算法是有效的。一个好的数据结构（解决方案）设计首先要考虑到业务场景，在操作上（增删改查），它有什么需求。比如查询上要求并发高、但在插入速度上并无太高的要求，或者反之。</p>\n<h2 id=\"计算复杂度\"><a href=\"#计算复杂度\" class=\"headerlink\" title=\"计算复杂度\"></a>计算复杂度</h2><ul>\n<li>通常我们考虑算法的在一定数据规模下的平均复杂度，但平均复杂度分析并不是一定可靠的，这建立在输入数据概率分布已知的情况下考虑的，通常这种假设每个元素出现的概率均等。但是不正确的假设也可能带来灾难的性的后果，特别是在搜索类算法上，如散列和搜索树，具体事例可参考9.4、5.4节。有时间考虑特殊的数据分布，也会带来好处，如9.2节（自组织线性表）</li>\n<li>在实时系统中不考虑平均情况，而考虑最差的情况。</li>\n<li>复杂度分析，通常采用渐近分析的方法，使用化简的方法计算上界，下界和平均情况。</li>\n</ul>\n<h2 id=\"最完美算法\"><a href=\"#最完美算法\" class=\"headerlink\" title=\"最完美算法\"></a>最完美算法</h2><p>联机算法——举例而言，最大子序列求和的问题</p>\n<h2 id=\"运行时间中的对数\"><a href=\"#运行时间中的对数\" class=\"headerlink\" title=\"运行时间中的对数\"></a>运行时间中的对数</h2><ul>\n<li>如二叉树，contains操作，较明显，在常数据O(1)时间问题的大小缩减为其一部分（通过是一半或小于一半），则该算法是O(logN)</li>\n<li>最大公因(约）数，即使用欧几里德/辗转相除法计算gcd(M,N)。由定理，若M&gt;N，则有M mod N &lt; M/2，可得该算法迭代次数至多是O(logN)</li>\n<li>幂次运算，使用递归分解，将乘法降到O(logN)</li>\n</ul>\n<h1 id=\"基本数据结构\"><a href=\"#基本数据结构\" class=\"headerlink\" title=\"基本数据结构\"></a>基本数据结构</h1><h2 id=\"表、栈、队列\"><a href=\"#表、栈、队列\" class=\"headerlink\" title=\"表、栈、队列\"></a>表、栈、队列</h2><h2 id=\"树\"><a href=\"#树\" class=\"headerlink\" title=\"树\"></a>树</h2><h3 id=\"二叉树\"><a href=\"#二叉树\" class=\"headerlink\" title=\"二叉树\"></a>二叉树</h3><p>可以使用栈来实现表达式树，也可用递归，这样会方便些。基本操作包括增(insert)、删(remove,两种可能性)、查(contains, findmin,findmax)，平均计算复杂度：</p>\n<p>得O(nlogN)，但这无法控制上界，<strong>为控制任意次连续M次操作在最坏的情况下花费时间为O(MlogN)，引入AVL平衡树，当然它无法保证单次的操作的时间界。最后根据二八法则，将引入伸展树</strong></p>\n<h3 id=\"AVL平衡树\"><a href=\"#AVL平衡树\" class=\"headerlink\" title=\"AVL平衡树\"></a>AVL平衡树</h3><p>  使用单旋和双旋调整高度，结构上每个节点信息加入了高度。核心插入代码：<br><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">insert</span><span class=\"params\">( <span class=\"keyword\">const</span> Comparable &amp; x, AvlNode * &amp; t )</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>( t == <span class=\"literal\">NULL</span> )</span><br><span class=\"line\">        t = <span class=\"keyword\">new</span> AvlNode( x, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span> );</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>( x &lt; t-&gt;element )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        insert( x, t-&gt;left );</span><br><span class=\"line\">        <span class=\"keyword\">if</span>( height( t-&gt;left ) - height( t-&gt;right ) == <span class=\"number\">2</span> )</span><br><span class=\"line\">            <span class=\"keyword\">if</span>( x &lt; t-&gt;left-&gt;element )</span><br><span class=\"line\">                rotateWithLeftChild( t );</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                doubleWithLeftChild( t );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>( t-&gt;element &lt; x )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        insert( x, t-&gt;right );</span><br><span class=\"line\">        <span class=\"keyword\">if</span>( height( t-&gt;right ) - height( t-&gt;left ) == <span class=\"number\">2</span> )</span><br><span class=\"line\">            <span class=\"keyword\">if</span>( t-&gt;right-&gt;element &lt; x )</span><br><span class=\"line\">                rotateWithRightChild( t );</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                doubleWithRightChild( t );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        ;  <span class=\"comment\">// Duplicate; do nothing</span></span><br><span class=\"line\">    t-&gt;height = max( height( t-&gt;left ), height( t-&gt;right ) ) + <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"伸展树\"><a href=\"#伸展树\" class=\"headerlink\" title=\"伸展树\"></a>伸展树</h3><p>之形伸展与一字伸展，目标是沿访问路径旋转  </p>\n<h3 id=\"树的遍历\"><a href=\"#树的遍历\" class=\"headerlink\" title=\"树的遍历\"></a>树的遍历</h3><h3 id=\"B树——数据库的索引树，用于解决大量数据磁盘检索的问题\"><a href=\"#B树——数据库的索引树，用于解决大量数据磁盘检索的问题\" class=\"headerlink\" title=\"B树——数据库的索引树，用于解决大量数据磁盘检索的问题\"></a>B树——数据库的索引树，用于解决大量数据磁盘检索的问题</h3><p>注意B树的定义，5个特性，M\\L参数的确定。如一个区块8K，每个键32B，在一颗M阶的B树中有M-1个键，则可算出M。对于叶子节点若每个记录是256字节，区块是8K，则可算出L=32。再回来看定义：</p>\n<ol>\n<li>数据在叶子节点，并在相同深度上，且数据项介于L/2到L之间；</li>\n<li>非叶节点，存储M-1个键，指示查找方向，其儿子数在M/2到M之间;<br>注意B树插入时的分裂操作和删除的认领操作。</li>\n</ol>\n<h3 id=\"STL中set和map的树形实现\"><a href=\"#STL中set和map的树形实现\" class=\"headerlink\" title=\"STL中set和map的树形实现\"></a>STL中set和map的树形实现</h3><h3 id=\"散列\"><a href=\"#散列\" class=\"headerlink\" title=\"散列\"></a>散列</h3><ol>\n<li>散列函数:利用horner法则，计算多项式函数$h_k=k_0+37k_1+37k_2=(37+37k_2)k_1+k_0$。通常建议桶大小为素数</li>\n<li>冲突解决方法：分离链接、线性/平方探测、双散列。分离链接法因子可以到1，性能下降不明显，探测方法最好不超过0.5，双散列宜用质数。</li>\n<li>再散列：一般使用途中策略，到达装载因子再散列</li>\n<li>hashmap, hashset</li>\n<li>散列比起二叉树而言速度虽有所提升但缺乏排序，通常用在比如编译器的符号表，拼写检查等等  </li>\n</ol>\n<h3 id=\"优先队列（堆）\"><a href=\"#优先队列（堆）\" class=\"headerlink\" title=\"优先队列（堆）\"></a>优先队列（堆）</h3><h2 id=\"排序与检索\"><a href=\"#排序与检索\" class=\"headerlink\" title=\"排序与检索\"></a>排序与检索</h2><h2 id=\"高级话题\"><a href=\"#高级话题\" class=\"headerlink\" title=\"高级话题\"></a>高级话题</h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"概率论及一些基础思考\"><a href=\"#概率论及一些基础思考\" class=\"headerlink\" title=\"概率论及一些基础思考\"></a>概率论及一些基础思考</h1><h2 id=\"代价与效益\"><a href=\"#代价与效益\" class=\"headerlink\" title=\"代价与效益\"></a>代价与效益</h2><p>在有效的资源下将问题解决，称该算法是有效的。一个好的数据结构（解决方案）设计首先要考虑到业务场景，在操作上（增删改查），它有什么需求。比如查询上要求并发高、但在插入速度上并无太高的要求，或者反之。</p>\n<h2 id=\"计算复杂度\"><a href=\"#计算复杂度\" class=\"headerlink\" title=\"计算复杂度\"></a>计算复杂度</h2><ul>\n<li>通常我们考虑算法的在一定数据规模下的平均复杂度，但平均复杂度分析并不是一定可靠的，这建立在输入数据概率分布已知的情况下考虑的，通常这种假设每个元素出现的概率均等。但是不正确的假设也可能带来灾难的性的后果，特别是在搜索类算法上，如散列和搜索树，具体事例可参考9.4、5.4节。有时间考虑特殊的数据分布，也会带来好处，如9.2节（自组织线性表）</li>\n<li>在实时系统中不考虑平均情况，而考虑最差的情况。</li>\n<li>复杂度分析，通常采用渐近分析的方法，使用化简的方法计算上界，下界和平均情况。</li>\n</ul>\n<h2 id=\"最完美算法\"><a href=\"#最完美算法\" class=\"headerlink\" title=\"最完美算法\"></a>最完美算法</h2><p>联机算法——举例而言，最大子序列求和的问题</p>\n<h2 id=\"运行时间中的对数\"><a href=\"#运行时间中的对数\" class=\"headerlink\" title=\"运行时间中的对数\"></a>运行时间中的对数</h2><ul>\n<li>如二叉树，contains操作，较明显，在常数据O(1)时间问题的大小缩减为其一部分（通过是一半或小于一半），则该算法是O(logN)</li>\n<li>最大公因(约）数，即使用欧几里德/辗转相除法计算gcd(M,N)。由定理，若M&gt;N，则有M mod N &lt; M/2，可得该算法迭代次数至多是O(logN)</li>\n<li>幂次运算，使用递归分解，将乘法降到O(logN)</li>\n</ul>\n<h1 id=\"基本数据结构\"><a href=\"#基本数据结构\" class=\"headerlink\" title=\"基本数据结构\"></a>基本数据结构</h1><h2 id=\"表、栈、队列\"><a href=\"#表、栈、队列\" class=\"headerlink\" title=\"表、栈、队列\"></a>表、栈、队列</h2><h2 id=\"树\"><a href=\"#树\" class=\"headerlink\" title=\"树\"></a>树</h2><h3 id=\"二叉树\"><a href=\"#二叉树\" class=\"headerlink\" title=\"二叉树\"></a>二叉树</h3><p>可以使用栈来实现表达式树，也可用递归，这样会方便些。基本操作包括增(insert)、删(remove,两种可能性)、查(contains, findmin,findmax)，平均计算复杂度：</p>\n<p>得O(nlogN)，但这无法控制上界，<strong>为控制任意次连续M次操作在最坏的情况下花费时间为O(MlogN)，引入AVL平衡树，当然它无法保证单次的操作的时间界。最后根据二八法则，将引入伸展树</strong></p>\n<h3 id=\"AVL平衡树\"><a href=\"#AVL平衡树\" class=\"headerlink\" title=\"AVL平衡树\"></a>AVL平衡树</h3><p>  使用单旋和双旋调整高度，结构上每个节点信息加入了高度。核心插入代码：<br><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">insert</span><span class=\"params\">( <span class=\"keyword\">const</span> Comparable &amp; x, AvlNode * &amp; t )</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>( t == <span class=\"literal\">NULL</span> )</span><br><span class=\"line\">        t = <span class=\"keyword\">new</span> AvlNode( x, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span> );</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>( x &lt; t-&gt;element )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        insert( x, t-&gt;left );</span><br><span class=\"line\">        <span class=\"keyword\">if</span>( height( t-&gt;left ) - height( t-&gt;right ) == <span class=\"number\">2</span> )</span><br><span class=\"line\">            <span class=\"keyword\">if</span>( x &lt; t-&gt;left-&gt;element )</span><br><span class=\"line\">                rotateWithLeftChild( t );</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                doubleWithLeftChild( t );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>( t-&gt;element &lt; x )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        insert( x, t-&gt;right );</span><br><span class=\"line\">        <span class=\"keyword\">if</span>( height( t-&gt;right ) - height( t-&gt;left ) == <span class=\"number\">2</span> )</span><br><span class=\"line\">            <span class=\"keyword\">if</span>( t-&gt;right-&gt;element &lt; x )</span><br><span class=\"line\">                rotateWithRightChild( t );</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                doubleWithRightChild( t );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        ;  <span class=\"comment\">// Duplicate; do nothing</span></span><br><span class=\"line\">    t-&gt;height = max( height( t-&gt;left ), height( t-&gt;right ) ) + <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"伸展树\"><a href=\"#伸展树\" class=\"headerlink\" title=\"伸展树\"></a>伸展树</h3><p>之形伸展与一字伸展，目标是沿访问路径旋转  </p>\n<h3 id=\"树的遍历\"><a href=\"#树的遍历\" class=\"headerlink\" title=\"树的遍历\"></a>树的遍历</h3><h3 id=\"B树——数据库的索引树，用于解决大量数据磁盘检索的问题\"><a href=\"#B树——数据库的索引树，用于解决大量数据磁盘检索的问题\" class=\"headerlink\" title=\"B树——数据库的索引树，用于解决大量数据磁盘检索的问题\"></a>B树——数据库的索引树，用于解决大量数据磁盘检索的问题</h3><p>注意B树的定义，5个特性，M\\L参数的确定。如一个区块8K，每个键32B，在一颗M阶的B树中有M-1个键，则可算出M。对于叶子节点若每个记录是256字节，区块是8K，则可算出L=32。再回来看定义：</p>\n<ol>\n<li>数据在叶子节点，并在相同深度上，且数据项介于L/2到L之间；</li>\n<li>非叶节点，存储M-1个键，指示查找方向，其儿子数在M/2到M之间;<br>注意B树插入时的分裂操作和删除的认领操作。</li>\n</ol>\n<h3 id=\"STL中set和map的树形实现\"><a href=\"#STL中set和map的树形实现\" class=\"headerlink\" title=\"STL中set和map的树形实现\"></a>STL中set和map的树形实现</h3><h3 id=\"散列\"><a href=\"#散列\" class=\"headerlink\" title=\"散列\"></a>散列</h3><ol>\n<li>散列函数:利用horner法则，计算多项式函数$h_k=k_0+37k_1+37k_2=(37+37k_2)k_1+k_0$。通常建议桶大小为素数</li>\n<li>冲突解决方法：分离链接、线性/平方探测、双散列。分离链接法因子可以到1，性能下降不明显，探测方法最好不超过0.5，双散列宜用质数。</li>\n<li>再散列：一般使用途中策略，到达装载因子再散列</li>\n<li>hashmap, hashset</li>\n<li>散列比起二叉树而言速度虽有所提升但缺乏排序，通常用在比如编译器的符号表，拼写检查等等  </li>\n</ol>\n<h3 id=\"优先队列（堆）\"><a href=\"#优先队列（堆）\" class=\"headerlink\" title=\"优先队列（堆）\"></a>优先队列（堆）</h3><h2 id=\"排序与检索\"><a href=\"#排序与检索\" class=\"headerlink\" title=\"排序与检索\"></a>排序与检索</h2><h2 id=\"高级话题\"><a href=\"#高级话题\" class=\"headerlink\" title=\"高级话题\"></a>高级话题</h2>"},{"title":"action in machine learning","date":"2017-08-29T10:21:54.000Z","mathjax":true,"_content":"\n\n###pca\\svd\nhttp://blog.csdn.net/jacke121/article/details/59057192\npca是单方向上的降维，SVD是两个方向上的降维。\nSVD表示是$A=U\\Sigma V^T$，其中U,V是正交的。\n若要降维：使用这个公式，其具体数学推导，暂不明。现实含义可以这样理解：\n在SVD的三个矩阵（假设叫1，2，3）中，1-2表示用户降维转换，2-3表示物品降维转换，在用户推荐那个场景，由于是使用基于物品的推荐，要计算两物品的相似度，则可理解为只需要用户角度降维，即1-2的转换。即有\n$$U_k \\Sigma V=> A^T U_k \\Sigma$$\n推导点：**如如此在用户上降维，那是否可以直接通过PCA在用户角度降维？**\n###关联分析\n支持度，可信度\n* Apriori\n\n### 树回归\n* 连续型数值的混乱度：总平方误差来表示，即方差*m\n* 离散型的标称的混乱度可以用香农熵表示\n* 若叶节使用的模型是分段常数则称为回归树，若叶节点使用的模型是线性回归方程则称为模型树\n* 使用CART构建出的树倾向于过拟合，分别使用预剪枝和后剪枝解决这个问题。预剪枝设置停止条件来调整拟合问题，这需要不断调整，见P168。而后剪枝首先构造足够复杂的树，然后使用测试集来判断叶节点合并能否降低误差\n* 决策树通过迭代实现分类，离散情况下，叶节点的类别即预测的类别，连续情况下，若是回归树则是该子树叶子节点的平均值，若是模型树，则是该（线性）模型的值\n\n### Random Forest/GBDT\n[参考介绍一：二分类](http://blog.csdn.net/google19890102/article/details/51746402)\n[参考介绍一:多分类](http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html)\n*  随机建立树的方式，随机采样包括行采样（样本），列采样（特征）\n* 即本质是每颗树是窄领域内的专家，众专家一起投票\n\n###预测数值型数据:回归\n* 线性回归\n使用最小二乘法，得$若X非奇异，则有w=(X^tX)^{-1}X^ty$\n* 局部加权线性回归（Local Weighted Leaner Regression）\n线性回归的问题是容易欠拟合，因此引入局部加权\n$\\hat{w}=(X^twX)^{-1}X^twy$\n通常使用高斯核用来赋于权重$w(i,i)=e^{\\frac {x^{(i)}-x}{-2k^2}}$，即有离预测点越近的点权重越大，同时参数K控制了参与运算的数据，这样就有如下图。\n\n\n\n\n此方法缺点在于**计算量增加，每次预测必须在全集上运行**，好处在于可以控制拟合程度\n\n * 缩减系数——用于更好的理解数据并处理特征比样本多的问题\n对于$X^TX$为奇异矩阵，在$X^TX$基础加上$\\lambda I$使得矩阵非奇异，这种方法称为*岭回归*。此方法有三个用途：\n    * 用于处理特征比样本多的情况\n    * 用于在估计中加入偏差，从而得到更好的估计\n    * 通过引入   $\\lambda$ 来限制w之和，通过引入惩罚，能够减少不重要的参数，这类技术在统计学称为衰减。（迹不变）\n    * 此公式$$等价于最小二乘加入约束\n\n###附加篇：各类方法的一个归纳\n* 树模型：DT, GBDT, RF\n* 概率模型：Bayes\n* 最优化：LR, SVM\n    * LR：\n    * SVM：简单、多分类问题比SVM好，不好解释；\n* 距离划分，判别模型\n    * KNN：简单、多分类问题比SVM好，不好解释；\n* 集成方法：Adaboost\n* 方法比较\n    * NB与LR:相同点，都是特征的线性表述，解释性对较好； 不同点：NB要求严格独立，NB有利于并行化，但随着数据量和维度增多，LR更合适\n* 可持久化的模型：决策树\n\n图径图\n###附加篇：机器学习算法路径图\n这里写图片描述\n\n\n\n###决策树|随机森林|adaboost|GBDT\n\nhttp://www.cnblogs.com/wentingtu/archive/2011/12/13/2286212.html\n\nhttp://blog.csdn.net/xlinsist/article/details/51475345 Bagging（Bootstrap aggregating）、随机森林（random forests）、AdaBoost\n\nhttp://blog.csdn.net/xlinsist/article/details/51468741 决策树ID3、C4.5、C5.0以及CART算法之间的比较-并用scikit-learn决策树拟合Iris数据集\n\nID3:\n1. 对于多值的属性非常之敏感\n2. 无法处理连续值\n3. 容易产生过拟合（即训练数据训练太彻底，但大一点数据集错误率上升，关注了些局部信息，）\nC4.5---C5.0:解决上述1，2的问题，核心思想是加入了增益率\nCART:能预测出值（回归），同时能全用训练和交叉验证不断评估，修剪决策树\n\n###K-nearest\n思路：基于现有已知标签的样本数据集，寻找被预测样本的最近的K个样本，返回K个里频率最多的那个标签。\n优点：最简单有效的分类、对异常值不敏感\n缺点：解释性差、类别评分无规格化，不平衡问题\n适应：http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors              brute force\\kd_tree\\balltree的选择\n\n###附加篇：\n> 张的问题：SVM中，如下代码中\n```python\ndef calcWs(alphas, dataArr, classLabels):\n    \"\"\"\n    输入：alphas, 数据集, 类别标签\n    输出：目标w\n    \"\"\"\n    X = mat(dataArr)\n    labelMat = mat(classLabels).transpose()\n    m, n = shape(X)\n    w = zeros((n, 1))\n    for i in range(m):\n        w += multiply(alphas[i] * labelMat[i], X[i, :].T)\n    return w\n```\n> 此处权重 $w$ 为何不能做特征的权重值。\n\n对于线性情况而言,有\n$$ w= \\Sigma \\alpha y_i x_i$$\n```python\nw += multiply(alphas[i] * labelMat[i], X[i, :].T)\n```\n$$ y= wx+b $$\n\n对于映射核空间的情况\n\n $$ y=\\Sigma \\alpha y_i k(\\phi(x_i),\\phi(x_j))  +b $$\n\n\n\n此时，特征向量 $w$是无法拆出来的。\n> weights = weights + alpha * dataMatrix.transpose() * error 这一行的理解\n* 角度一：http://blog.csdn.net/lu597203933/article/details/38468303  AndrewNg\n* 角度二：矩阵微分的角度理解 http://blog.csdn.net/aichipmunk/article/details/9382503\n","source":"_posts/action-in-machine-learning.md","raw":"---\ntitle: action in machine learning\ndate: 2017-08-29 18:21:54\ntags:\n      - 机器学习\n      - 经典著作\ncategories: AI梦\nmathjax: true\n---\n\n\n###pca\\svd\nhttp://blog.csdn.net/jacke121/article/details/59057192\npca是单方向上的降维，SVD是两个方向上的降维。\nSVD表示是$A=U\\Sigma V^T$，其中U,V是正交的。\n若要降维：使用这个公式，其具体数学推导，暂不明。现实含义可以这样理解：\n在SVD的三个矩阵（假设叫1，2，3）中，1-2表示用户降维转换，2-3表示物品降维转换，在用户推荐那个场景，由于是使用基于物品的推荐，要计算两物品的相似度，则可理解为只需要用户角度降维，即1-2的转换。即有\n$$U_k \\Sigma V=> A^T U_k \\Sigma$$\n推导点：**如如此在用户上降维，那是否可以直接通过PCA在用户角度降维？**\n###关联分析\n支持度，可信度\n* Apriori\n\n### 树回归\n* 连续型数值的混乱度：总平方误差来表示，即方差*m\n* 离散型的标称的混乱度可以用香农熵表示\n* 若叶节使用的模型是分段常数则称为回归树，若叶节点使用的模型是线性回归方程则称为模型树\n* 使用CART构建出的树倾向于过拟合，分别使用预剪枝和后剪枝解决这个问题。预剪枝设置停止条件来调整拟合问题，这需要不断调整，见P168。而后剪枝首先构造足够复杂的树，然后使用测试集来判断叶节点合并能否降低误差\n* 决策树通过迭代实现分类，离散情况下，叶节点的类别即预测的类别，连续情况下，若是回归树则是该子树叶子节点的平均值，若是模型树，则是该（线性）模型的值\n\n### Random Forest/GBDT\n[参考介绍一：二分类](http://blog.csdn.net/google19890102/article/details/51746402)\n[参考介绍一:多分类](http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html)\n*  随机建立树的方式，随机采样包括行采样（样本），列采样（特征）\n* 即本质是每颗树是窄领域内的专家，众专家一起投票\n\n###预测数值型数据:回归\n* 线性回归\n使用最小二乘法，得$若X非奇异，则有w=(X^tX)^{-1}X^ty$\n* 局部加权线性回归（Local Weighted Leaner Regression）\n线性回归的问题是容易欠拟合，因此引入局部加权\n$\\hat{w}=(X^twX)^{-1}X^twy$\n通常使用高斯核用来赋于权重$w(i,i)=e^{\\frac {x^{(i)}-x}{-2k^2}}$，即有离预测点越近的点权重越大，同时参数K控制了参与运算的数据，这样就有如下图。\n\n\n\n\n此方法缺点在于**计算量增加，每次预测必须在全集上运行**，好处在于可以控制拟合程度\n\n * 缩减系数——用于更好的理解数据并处理特征比样本多的问题\n对于$X^TX$为奇异矩阵，在$X^TX$基础加上$\\lambda I$使得矩阵非奇异，这种方法称为*岭回归*。此方法有三个用途：\n    * 用于处理特征比样本多的情况\n    * 用于在估计中加入偏差，从而得到更好的估计\n    * 通过引入   $\\lambda$ 来限制w之和，通过引入惩罚，能够减少不重要的参数，这类技术在统计学称为衰减。（迹不变）\n    * 此公式$$等价于最小二乘加入约束\n\n###附加篇：各类方法的一个归纳\n* 树模型：DT, GBDT, RF\n* 概率模型：Bayes\n* 最优化：LR, SVM\n    * LR：\n    * SVM：简单、多分类问题比SVM好，不好解释；\n* 距离划分，判别模型\n    * KNN：简单、多分类问题比SVM好，不好解释；\n* 集成方法：Adaboost\n* 方法比较\n    * NB与LR:相同点，都是特征的线性表述，解释性对较好； 不同点：NB要求严格独立，NB有利于并行化，但随着数据量和维度增多，LR更合适\n* 可持久化的模型：决策树\n\n图径图\n###附加篇：机器学习算法路径图\n这里写图片描述\n\n\n\n###决策树|随机森林|adaboost|GBDT\n\nhttp://www.cnblogs.com/wentingtu/archive/2011/12/13/2286212.html\n\nhttp://blog.csdn.net/xlinsist/article/details/51475345 Bagging（Bootstrap aggregating）、随机森林（random forests）、AdaBoost\n\nhttp://blog.csdn.net/xlinsist/article/details/51468741 决策树ID3、C4.5、C5.0以及CART算法之间的比较-并用scikit-learn决策树拟合Iris数据集\n\nID3:\n1. 对于多值的属性非常之敏感\n2. 无法处理连续值\n3. 容易产生过拟合（即训练数据训练太彻底，但大一点数据集错误率上升，关注了些局部信息，）\nC4.5---C5.0:解决上述1，2的问题，核心思想是加入了增益率\nCART:能预测出值（回归），同时能全用训练和交叉验证不断评估，修剪决策树\n\n###K-nearest\n思路：基于现有已知标签的样本数据集，寻找被预测样本的最近的K个样本，返回K个里频率最多的那个标签。\n优点：最简单有效的分类、对异常值不敏感\n缺点：解释性差、类别评分无规格化，不平衡问题\n适应：http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors              brute force\\kd_tree\\balltree的选择\n\n###附加篇：\n> 张的问题：SVM中，如下代码中\n```python\ndef calcWs(alphas, dataArr, classLabels):\n    \"\"\"\n    输入：alphas, 数据集, 类别标签\n    输出：目标w\n    \"\"\"\n    X = mat(dataArr)\n    labelMat = mat(classLabels).transpose()\n    m, n = shape(X)\n    w = zeros((n, 1))\n    for i in range(m):\n        w += multiply(alphas[i] * labelMat[i], X[i, :].T)\n    return w\n```\n> 此处权重 $w$ 为何不能做特征的权重值。\n\n对于线性情况而言,有\n$$ w= \\Sigma \\alpha y_i x_i$$\n```python\nw += multiply(alphas[i] * labelMat[i], X[i, :].T)\n```\n$$ y= wx+b $$\n\n对于映射核空间的情况\n\n $$ y=\\Sigma \\alpha y_i k(\\phi(x_i),\\phi(x_j))  +b $$\n\n\n\n此时，特征向量 $w$是无法拆出来的。\n> weights = weights + alpha * dataMatrix.transpose() * error 这一行的理解\n* 角度一：http://blog.csdn.net/lu597203933/article/details/38468303  AndrewNg\n* 角度二：矩阵微分的角度理解 http://blog.csdn.net/aichipmunk/article/details/9382503\n","slug":"action-in-machine-learning","published":1,"updated":"2017-12-18T05:22:35.120Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbdf8izh0006icn775v5oi1p","content":"<h3 id=\"pca-svd\"><a href=\"#pca-svd\" class=\"headerlink\" title=\"pca\\svd\"></a>pca\\svd</h3><p><a href=\"http://blog.csdn.net/jacke121/article/details/59057192\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/jacke121/article/details/59057192</a><br>pca是单方向上的降维，SVD是两个方向上的降维。<br>SVD表示是$A=U\\Sigma V^T$，其中U,V是正交的。<br>若要降维：使用这个公式，其具体数学推导，暂不明。现实含义可以这样理解：<br>在SVD的三个矩阵（假设叫1，2，3）中，1-2表示用户降维转换，2-3表示物品降维转换，在用户推荐那个场景，由于是使用基于物品的推荐，要计算两物品的相似度，则可理解为只需要用户角度降维，即1-2的转换。即有</p>\n<script type=\"math/tex; mode=display\">U_k \\Sigma V=> A^T U_k \\Sigma</script><p>推导点：<strong>如如此在用户上降维，那是否可以直接通过PCA在用户角度降维？</strong></p>\n<h3 id=\"关联分析\"><a href=\"#关联分析\" class=\"headerlink\" title=\"关联分析\"></a>关联分析</h3><p>支持度，可信度</p>\n<ul>\n<li>Apriori</li>\n</ul>\n<h3 id=\"树回归\"><a href=\"#树回归\" class=\"headerlink\" title=\"树回归\"></a>树回归</h3><ul>\n<li>连续型数值的混乱度：总平方误差来表示，即方差*m</li>\n<li>离散型的标称的混乱度可以用香农熵表示</li>\n<li>若叶节使用的模型是分段常数则称为回归树，若叶节点使用的模型是线性回归方程则称为模型树</li>\n<li>使用CART构建出的树倾向于过拟合，分别使用预剪枝和后剪枝解决这个问题。预剪枝设置停止条件来调整拟合问题，这需要不断调整，见P168。而后剪枝首先构造足够复杂的树，然后使用测试集来判断叶节点合并能否降低误差</li>\n<li>决策树通过迭代实现分类，离散情况下，叶节点的类别即预测的类别，连续情况下，若是回归树则是该子树叶子节点的平均值，若是模型树，则是该（线性）模型的值</li>\n</ul>\n<h3 id=\"Random-Forest-GBDT\"><a href=\"#Random-Forest-GBDT\" class=\"headerlink\" title=\"Random Forest/GBDT\"></a>Random Forest/GBDT</h3><p><a href=\"http://blog.csdn.net/google19890102/article/details/51746402\" target=\"_blank\" rel=\"noopener\">参考介绍一：二分类</a><br><a href=\"http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html\" target=\"_blank\" rel=\"noopener\">参考介绍一:多分类</a></p>\n<ul>\n<li>随机建立树的方式，随机采样包括行采样（样本），列采样（特征）</li>\n<li>即本质是每颗树是窄领域内的专家，众专家一起投票</li>\n</ul>\n<h3 id=\"预测数值型数据-回归\"><a href=\"#预测数值型数据-回归\" class=\"headerlink\" title=\"预测数值型数据:回归\"></a>预测数值型数据:回归</h3><ul>\n<li>线性回归<br>使用最小二乘法，得$若X非奇异，则有w=(X^tX)^{-1}X^ty$</li>\n<li>局部加权线性回归（Local Weighted Leaner Regression）<br>线性回归的问题是容易欠拟合，因此引入局部加权<br>$\\hat{w}=(X^twX)^{-1}X^twy$<br>通常使用高斯核用来赋于权重$w(i,i)=e^{\\frac {x^{(i)}-x}{-2k^2}}$，即有离预测点越近的点权重越大，同时参数K控制了参与运算的数据，这样就有如下图。</li>\n</ul>\n<p>此方法缺点在于<strong>计算量增加，每次预测必须在全集上运行</strong>，好处在于可以控制拟合程度</p>\n<ul>\n<li>缩减系数——用于更好的理解数据并处理特征比样本多的问题<br>对于$X^TX$为奇异矩阵，在$X^TX$基础加上$\\lambda I$使得矩阵非奇异，这种方法称为<em>岭回归</em>。此方法有三个用途：<ul>\n<li>用于处理特征比样本多的情况</li>\n<li>用于在估计中加入偏差，从而得到更好的估计</li>\n<li>通过引入   $\\lambda$ 来限制w之和，通过引入惩罚，能够减少不重要的参数，这类技术在统计学称为衰减。（迹不变）</li>\n<li>此公式$$等价于最小二乘加入约束</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"附加篇：各类方法的一个归纳\"><a href=\"#附加篇：各类方法的一个归纳\" class=\"headerlink\" title=\"附加篇：各类方法的一个归纳\"></a>附加篇：各类方法的一个归纳</h3><ul>\n<li>树模型：DT, GBDT, RF</li>\n<li>概率模型：Bayes</li>\n<li>最优化：LR, SVM<ul>\n<li>LR：</li>\n<li>SVM：简单、多分类问题比SVM好，不好解释；</li>\n</ul>\n</li>\n<li>距离划分，判别模型<ul>\n<li>KNN：简单、多分类问题比SVM好，不好解释；</li>\n</ul>\n</li>\n<li>集成方法：Adaboost</li>\n<li>方法比较<ul>\n<li>NB与LR:相同点，都是特征的线性表述，解释性对较好； 不同点：NB要求严格独立，NB有利于并行化，但随着数据量和维度增多，LR更合适</li>\n</ul>\n</li>\n<li>可持久化的模型：决策树</li>\n</ul>\n<p>图径图</p>\n<h3 id=\"附加篇：机器学习算法路径图\"><a href=\"#附加篇：机器学习算法路径图\" class=\"headerlink\" title=\"附加篇：机器学习算法路径图\"></a>附加篇：机器学习算法路径图</h3><p>这里写图片描述</p>\n<h3 id=\"决策树-随机森林-adaboost-GBDT\"><a href=\"#决策树-随机森林-adaboost-GBDT\" class=\"headerlink\" title=\"决策树|随机森林|adaboost|GBDT\"></a>决策树|随机森林|adaboost|GBDT</h3><p><a href=\"http://www.cnblogs.com/wentingtu/archive/2011/12/13/2286212.html\" target=\"_blank\" rel=\"noopener\">http://www.cnblogs.com/wentingtu/archive/2011/12/13/2286212.html</a></p>\n<p><a href=\"http://blog.csdn.net/xlinsist/article/details/51475345\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/xlinsist/article/details/51475345</a> Bagging（Bootstrap aggregating）、随机森林（random forests）、AdaBoost</p>\n<p><a href=\"http://blog.csdn.net/xlinsist/article/details/51468741\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/xlinsist/article/details/51468741</a> 决策树ID3、C4.5、C5.0以及CART算法之间的比较-并用scikit-learn决策树拟合Iris数据集</p>\n<p>ID3:</p>\n<ol>\n<li>对于多值的属性非常之敏感</li>\n<li>无法处理连续值</li>\n<li>容易产生过拟合（即训练数据训练太彻底，但大一点数据集错误率上升，关注了些局部信息，）<br>C4.5—-C5.0:解决上述1，2的问题，核心思想是加入了增益率<br>CART:能预测出值（回归），同时能全用训练和交叉验证不断评估，修剪决策树</li>\n</ol>\n<h3 id=\"K-nearest\"><a href=\"#K-nearest\" class=\"headerlink\" title=\"K-nearest\"></a>K-nearest</h3><p>思路：基于现有已知标签的样本数据集，寻找被预测样本的最近的K个样本，返回K个里频率最多的那个标签。<br>优点：最简单有效的分类、对异常值不敏感<br>缺点：解释性差、类别评分无规格化，不平衡问题<br>适应：<a href=\"http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors\" target=\"_blank\" rel=\"noopener\">http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors</a>              brute force\\kd_tree\\balltree的选择</p>\n<h3 id=\"附加篇：\"><a href=\"#附加篇：\" class=\"headerlink\" title=\"附加篇：\"></a>附加篇：</h3><blockquote>\n<p>张的问题：SVM中，如下代码中<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calcWs</span><span class=\"params\">(alphas, dataArr, classLabels)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    输入：alphas, 数据集, 类别标签</span></span><br><span class=\"line\"><span class=\"string\">    输出：目标w</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    X = mat(dataArr)</span><br><span class=\"line\">    labelMat = mat(classLabels).transpose()</span><br><span class=\"line\">    m, n = shape(X)</span><br><span class=\"line\">    w = zeros((n, <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(m):</span><br><span class=\"line\">        w += multiply(alphas[i] * labelMat[i], X[i, :].T)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> w</span><br></pre></td></tr></table></figure></p>\n<p>此处权重 $w$ 为何不能做特征的权重值。</p>\n</blockquote>\n<p>对于线性情况而言,有</p>\n<script type=\"math/tex; mode=display\">w= \\Sigma \\alpha y_i x_i</script><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w += multiply(alphas[i] * labelMat[i], X[i, :].T)</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">y= wx+b</script><p>对于映射核空间的情况</p>\n<script type=\"math/tex; mode=display\">y=\\Sigma \\alpha y_i k(\\phi(x_i),\\phi(x_j))  +b</script><p>此时，特征向量 $w$是无法拆出来的。</p>\n<blockquote>\n<p>weights = weights + alpha <em> dataMatrix.transpose() </em> error 这一行的理解</p>\n<ul>\n<li>角度一：<a href=\"http://blog.csdn.net/lu597203933/article/details/38468303\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/lu597203933/article/details/38468303</a>  AndrewNg</li>\n<li>角度二：矩阵微分的角度理解 <a href=\"http://blog.csdn.net/aichipmunk/article/details/9382503\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/aichipmunk/article/details/9382503</a></li>\n</ul>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"pca-svd\"><a href=\"#pca-svd\" class=\"headerlink\" title=\"pca\\svd\"></a>pca\\svd</h3><p><a href=\"http://blog.csdn.net/jacke121/article/details/59057192\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/jacke121/article/details/59057192</a><br>pca是单方向上的降维，SVD是两个方向上的降维。<br>SVD表示是$A=U\\Sigma V^T$，其中U,V是正交的。<br>若要降维：使用这个公式，其具体数学推导，暂不明。现实含义可以这样理解：<br>在SVD的三个矩阵（假设叫1，2，3）中，1-2表示用户降维转换，2-3表示物品降维转换，在用户推荐那个场景，由于是使用基于物品的推荐，要计算两物品的相似度，则可理解为只需要用户角度降维，即1-2的转换。即有</p>\n<script type=\"math/tex; mode=display\">U_k \\Sigma V=> A^T U_k \\Sigma</script><p>推导点：<strong>如如此在用户上降维，那是否可以直接通过PCA在用户角度降维？</strong></p>\n<h3 id=\"关联分析\"><a href=\"#关联分析\" class=\"headerlink\" title=\"关联分析\"></a>关联分析</h3><p>支持度，可信度</p>\n<ul>\n<li>Apriori</li>\n</ul>\n<h3 id=\"树回归\"><a href=\"#树回归\" class=\"headerlink\" title=\"树回归\"></a>树回归</h3><ul>\n<li>连续型数值的混乱度：总平方误差来表示，即方差*m</li>\n<li>离散型的标称的混乱度可以用香农熵表示</li>\n<li>若叶节使用的模型是分段常数则称为回归树，若叶节点使用的模型是线性回归方程则称为模型树</li>\n<li>使用CART构建出的树倾向于过拟合，分别使用预剪枝和后剪枝解决这个问题。预剪枝设置停止条件来调整拟合问题，这需要不断调整，见P168。而后剪枝首先构造足够复杂的树，然后使用测试集来判断叶节点合并能否降低误差</li>\n<li>决策树通过迭代实现分类，离散情况下，叶节点的类别即预测的类别，连续情况下，若是回归树则是该子树叶子节点的平均值，若是模型树，则是该（线性）模型的值</li>\n</ul>\n<h3 id=\"Random-Forest-GBDT\"><a href=\"#Random-Forest-GBDT\" class=\"headerlink\" title=\"Random Forest/GBDT\"></a>Random Forest/GBDT</h3><p><a href=\"http://blog.csdn.net/google19890102/article/details/51746402\" target=\"_blank\" rel=\"noopener\">参考介绍一：二分类</a><br><a href=\"http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html\" target=\"_blank\" rel=\"noopener\">参考介绍一:多分类</a></p>\n<ul>\n<li>随机建立树的方式，随机采样包括行采样（样本），列采样（特征）</li>\n<li>即本质是每颗树是窄领域内的专家，众专家一起投票</li>\n</ul>\n<h3 id=\"预测数值型数据-回归\"><a href=\"#预测数值型数据-回归\" class=\"headerlink\" title=\"预测数值型数据:回归\"></a>预测数值型数据:回归</h3><ul>\n<li>线性回归<br>使用最小二乘法，得$若X非奇异，则有w=(X^tX)^{-1}X^ty$</li>\n<li>局部加权线性回归（Local Weighted Leaner Regression）<br>线性回归的问题是容易欠拟合，因此引入局部加权<br>$\\hat{w}=(X^twX)^{-1}X^twy$<br>通常使用高斯核用来赋于权重$w(i,i)=e^{\\frac {x^{(i)}-x}{-2k^2}}$，即有离预测点越近的点权重越大，同时参数K控制了参与运算的数据，这样就有如下图。</li>\n</ul>\n<p>此方法缺点在于<strong>计算量增加，每次预测必须在全集上运行</strong>，好处在于可以控制拟合程度</p>\n<ul>\n<li>缩减系数——用于更好的理解数据并处理特征比样本多的问题<br>对于$X^TX$为奇异矩阵，在$X^TX$基础加上$\\lambda I$使得矩阵非奇异，这种方法称为<em>岭回归</em>。此方法有三个用途：<ul>\n<li>用于处理特征比样本多的情况</li>\n<li>用于在估计中加入偏差，从而得到更好的估计</li>\n<li>通过引入   $\\lambda$ 来限制w之和，通过引入惩罚，能够减少不重要的参数，这类技术在统计学称为衰减。（迹不变）</li>\n<li>此公式$$等价于最小二乘加入约束</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"附加篇：各类方法的一个归纳\"><a href=\"#附加篇：各类方法的一个归纳\" class=\"headerlink\" title=\"附加篇：各类方法的一个归纳\"></a>附加篇：各类方法的一个归纳</h3><ul>\n<li>树模型：DT, GBDT, RF</li>\n<li>概率模型：Bayes</li>\n<li>最优化：LR, SVM<ul>\n<li>LR：</li>\n<li>SVM：简单、多分类问题比SVM好，不好解释；</li>\n</ul>\n</li>\n<li>距离划分，判别模型<ul>\n<li>KNN：简单、多分类问题比SVM好，不好解释；</li>\n</ul>\n</li>\n<li>集成方法：Adaboost</li>\n<li>方法比较<ul>\n<li>NB与LR:相同点，都是特征的线性表述，解释性对较好； 不同点：NB要求严格独立，NB有利于并行化，但随着数据量和维度增多，LR更合适</li>\n</ul>\n</li>\n<li>可持久化的模型：决策树</li>\n</ul>\n<p>图径图</p>\n<h3 id=\"附加篇：机器学习算法路径图\"><a href=\"#附加篇：机器学习算法路径图\" class=\"headerlink\" title=\"附加篇：机器学习算法路径图\"></a>附加篇：机器学习算法路径图</h3><p>这里写图片描述</p>\n<h3 id=\"决策树-随机森林-adaboost-GBDT\"><a href=\"#决策树-随机森林-adaboost-GBDT\" class=\"headerlink\" title=\"决策树|随机森林|adaboost|GBDT\"></a>决策树|随机森林|adaboost|GBDT</h3><p><a href=\"http://www.cnblogs.com/wentingtu/archive/2011/12/13/2286212.html\" target=\"_blank\" rel=\"noopener\">http://www.cnblogs.com/wentingtu/archive/2011/12/13/2286212.html</a></p>\n<p><a href=\"http://blog.csdn.net/xlinsist/article/details/51475345\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/xlinsist/article/details/51475345</a> Bagging（Bootstrap aggregating）、随机森林（random forests）、AdaBoost</p>\n<p><a href=\"http://blog.csdn.net/xlinsist/article/details/51468741\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/xlinsist/article/details/51468741</a> 决策树ID3、C4.5、C5.0以及CART算法之间的比较-并用scikit-learn决策树拟合Iris数据集</p>\n<p>ID3:</p>\n<ol>\n<li>对于多值的属性非常之敏感</li>\n<li>无法处理连续值</li>\n<li>容易产生过拟合（即训练数据训练太彻底，但大一点数据集错误率上升，关注了些局部信息，）<br>C4.5—-C5.0:解决上述1，2的问题，核心思想是加入了增益率<br>CART:能预测出值（回归），同时能全用训练和交叉验证不断评估，修剪决策树</li>\n</ol>\n<h3 id=\"K-nearest\"><a href=\"#K-nearest\" class=\"headerlink\" title=\"K-nearest\"></a>K-nearest</h3><p>思路：基于现有已知标签的样本数据集，寻找被预测样本的最近的K个样本，返回K个里频率最多的那个标签。<br>优点：最简单有效的分类、对异常值不敏感<br>缺点：解释性差、类别评分无规格化，不平衡问题<br>适应：<a href=\"http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors\" target=\"_blank\" rel=\"noopener\">http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors</a>              brute force\\kd_tree\\balltree的选择</p>\n<h3 id=\"附加篇：\"><a href=\"#附加篇：\" class=\"headerlink\" title=\"附加篇：\"></a>附加篇：</h3><blockquote>\n<p>张的问题：SVM中，如下代码中<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calcWs</span><span class=\"params\">(alphas, dataArr, classLabels)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    输入：alphas, 数据集, 类别标签</span></span><br><span class=\"line\"><span class=\"string\">    输出：目标w</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    X = mat(dataArr)</span><br><span class=\"line\">    labelMat = mat(classLabels).transpose()</span><br><span class=\"line\">    m, n = shape(X)</span><br><span class=\"line\">    w = zeros((n, <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(m):</span><br><span class=\"line\">        w += multiply(alphas[i] * labelMat[i], X[i, :].T)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> w</span><br></pre></td></tr></table></figure></p>\n<p>此处权重 $w$ 为何不能做特征的权重值。</p>\n</blockquote>\n<p>对于线性情况而言,有</p>\n<script type=\"math/tex; mode=display\">w= \\Sigma \\alpha y_i x_i</script><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w += multiply(alphas[i] * labelMat[i], X[i, :].T)</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">y= wx+b</script><p>对于映射核空间的情况</p>\n<script type=\"math/tex; mode=display\">y=\\Sigma \\alpha y_i k(\\phi(x_i),\\phi(x_j))  +b</script><p>此时，特征向量 $w$是无法拆出来的。</p>\n<blockquote>\n<p>weights = weights + alpha <em> dataMatrix.transpose() </em> error 这一行的理解</p>\n<ul>\n<li>角度一：<a href=\"http://blog.csdn.net/lu597203933/article/details/38468303\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/lu597203933/article/details/38468303</a>  AndrewNg</li>\n<li>角度二：矩阵微分的角度理解 <a href=\"http://blog.csdn.net/aichipmunk/article/details/9382503\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/aichipmunk/article/details/9382503</a></li>\n</ul>\n</blockquote>\n"},{"title":"Neural Networks for Machine Learning","date":"2016-06-07T05:26:48.000Z","mathjax":true,"_content":"\n\n\n# 机器学习：不必为特定任务编写程序\n## 场景落地\n- Recognizing pattern     \n  - Objects    \n  - facial    \n  - Spoken word\n- Recognizing anomalies    \n  - credit card transaction    \n  - Sensor reality\n- Prediction    \n  - stock price,\n  - exchange rate   \n  - movie like\n\n##几种简单模型\n- Linear neurons\n$ y=b+\\sum x_iw_i $\n- Binary threshold neurons\n$$\nz=\\Sigma x_i w_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt 0\\\\\n0&otherwise\n\\end{cases} \\quad or \\qquad\n\\theta=-b \\quad\nz=b+\\Sigma x_i y_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt \\theta\\\\\n0&otherwise\n\\end{cases}\n$$\n- Rectified Linear Neurons 2\n$$z=\\Sigma x_i w_i, y=\n\\begin{cases}\nz,\t&if\\;z \\gt 0 \\\\\n0,\t&\\mbox{otherwise}\n\\end{cases}$$\n- sigmoid neurons\n$$ z=b+\\Sigma x_i w_i  \\quad y=\\frac {1} {1+e^{-z}} $$\n![nnml-sigmoid](http://p15i7i801.bkt.clouddn.com/86069ad24cfd3541f4b33070b67fd749.png)\n- Stochastic binary neurons\n$$z=b+\\Sigma x_i w_i  \\quad p(s=1)=\\frac {1} {1+e^{-z}}$$\n\n## 学习的类型\n- supervised learning\n  * Regression\n    * model class: $y=f(x;w)$\n    * 1/2这个系数在求导时被抵消\n![nnml-regression](http://p15i7i801.bkt.clouddn.com/93512d5be2aafca06ff3d058aff511b7.png)\n  * classification\n- reinforced learning\n\n  the output is an action or sequence of actions and the only supervisory is an occasional scalar reward._This is difficult_, The rewads are typically delayed so its had to know where we went wrong( or right)\n- unsupervised learning\n\n  ![nnml-unsupervised](http://p15i7i801.bkt.clouddn.com/ada9c5e7fe1e06957b0797be4022d724.png)\n## 神经网络的类型\n- Feed-Forward neural network\n\n  ![nnml-feedforward](http://p15i7i801.bkt.clouddn.com/3053d06760561e6169f20829df15a358.png)\n\n  if there is more than one hidden layer, we call them \"deep\" neural networks.\n\n- Recurrent network\n\n  It is hard to train but natural way for modeling sequence, recent algorithm can be able to do this, this also can be used to predict stock price.\n\n  ![nnml-recurrent](http://p15i7i801.bkt.clouddn.com/83abfe6f730b3ae61d9099ff0d5ff300.png)\n![nnml-rnn](http://p15i7i801.bkt.clouddn.com/3a688df70ed6bf1e46d62a03402e2a39.png)\n> 20171218 LSTM 现在貌似更流行\n\n- Symmetrically connected networks\n\n## 感知器\n![nnml-perceptron-arch](http://p15i7i801.bkt.clouddn.com/a1ba9669ccb220d05a8140bc8e0ca797.png)\n> how to learn biases using the same rule as we use for learning weights\n\n  ![nnml-handlebias](http://p15i7i801.bkt.clouddn.com/d73047067e05f31a249dc755851602aa.png)\n\n> 学习的过程：if the output unit is correct, leave its weights alone, if the output unit incorrectly output\n\n![nnml-per-train](http://p15i7i801.bkt.clouddn.com/6b72ab6f556908380fcd6d5561d2e650.png)\n  - __错误的输出了零，则Z需要新增以致其逐渐大于零而产生y=1（往正面），反之则需要减__ ？？此偏移向量为负如何解释，或者必须保证 该向量是指向正确的分类超平面的（perpendicular，正交）。\n  - $(w,b) \\leftarrow (w,b)-(x,1)$\n    $(w,b) \\leftarrow (w,b)+(x,1)$\n![nnml-adpateside](http://p15i7i801.bkt.clouddn.com/b109e1d1e4424d180575bb0f74e75906.png)\n  - *just like penalty function*\n\n> 为什么学习有效: _非正式的收敛证明_\n\n  1. Each time the perceptron makes a mistake, the current weight vector moves to decrease its squared distance from every weight vector in the \"generously feasible\" region.\n  2. <strong> The squared distance decreases by at least the squared length of the input vector. </strong>（向量的运算，几何解释）\n  3. So after a finite number of mistakes, the weight vector must lie in the feasible region if the region exist.\n\n## 感知器的缺陷\n> **right feature**\n\n  Once the hand-coded features have been determined, there are very strong limitations on what a perceptron can learn.\n  _The main question is : This type of table look-up won't generalize, it need too many feature_\n  _此篇习题未理解_\n\n> Group Invariance Theorem---Minsky and Papert\n\ncan't learn to do this if the transformations from a group{欧氏变换之内}，so it needs to use multiple feature unit to recognize transformations of informative sub-patterns.\nSo,more  important was all about how you learn **feature detectors, that's hidden units**. After 20 years, We know,\nwe need multiple layer of adaptive, non-linear hidden units.\n","source":"_posts/Neural-Networks-for-Machine-Learning.md","raw":"---\ntitle: Neural Networks for Machine Learning\ndate: 2016-06-07 13:26:48\ntags:\n      - 神经网络\n      - Geoffrey hinton\n      - 公开课\ncategories: AI梦\nmathjax: true\n---\n\n\n\n# 机器学习：不必为特定任务编写程序\n## 场景落地\n- Recognizing pattern     \n  - Objects    \n  - facial    \n  - Spoken word\n- Recognizing anomalies    \n  - credit card transaction    \n  - Sensor reality\n- Prediction    \n  - stock price,\n  - exchange rate   \n  - movie like\n\n##几种简单模型\n- Linear neurons\n$ y=b+\\sum x_iw_i $\n- Binary threshold neurons\n$$\nz=\\Sigma x_i w_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt 0\\\\\n0&otherwise\n\\end{cases} \\quad or \\qquad\n\\theta=-b \\quad\nz=b+\\Sigma x_i y_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt \\theta\\\\\n0&otherwise\n\\end{cases}\n$$\n- Rectified Linear Neurons 2\n$$z=\\Sigma x_i w_i, y=\n\\begin{cases}\nz,\t&if\\;z \\gt 0 \\\\\n0,\t&\\mbox{otherwise}\n\\end{cases}$$\n- sigmoid neurons\n$$ z=b+\\Sigma x_i w_i  \\quad y=\\frac {1} {1+e^{-z}} $$\n![nnml-sigmoid](http://p15i7i801.bkt.clouddn.com/86069ad24cfd3541f4b33070b67fd749.png)\n- Stochastic binary neurons\n$$z=b+\\Sigma x_i w_i  \\quad p(s=1)=\\frac {1} {1+e^{-z}}$$\n\n## 学习的类型\n- supervised learning\n  * Regression\n    * model class: $y=f(x;w)$\n    * 1/2这个系数在求导时被抵消\n![nnml-regression](http://p15i7i801.bkt.clouddn.com/93512d5be2aafca06ff3d058aff511b7.png)\n  * classification\n- reinforced learning\n\n  the output is an action or sequence of actions and the only supervisory is an occasional scalar reward._This is difficult_, The rewads are typically delayed so its had to know where we went wrong( or right)\n- unsupervised learning\n\n  ![nnml-unsupervised](http://p15i7i801.bkt.clouddn.com/ada9c5e7fe1e06957b0797be4022d724.png)\n## 神经网络的类型\n- Feed-Forward neural network\n\n  ![nnml-feedforward](http://p15i7i801.bkt.clouddn.com/3053d06760561e6169f20829df15a358.png)\n\n  if there is more than one hidden layer, we call them \"deep\" neural networks.\n\n- Recurrent network\n\n  It is hard to train but natural way for modeling sequence, recent algorithm can be able to do this, this also can be used to predict stock price.\n\n  ![nnml-recurrent](http://p15i7i801.bkt.clouddn.com/83abfe6f730b3ae61d9099ff0d5ff300.png)\n![nnml-rnn](http://p15i7i801.bkt.clouddn.com/3a688df70ed6bf1e46d62a03402e2a39.png)\n> 20171218 LSTM 现在貌似更流行\n\n- Symmetrically connected networks\n\n## 感知器\n![nnml-perceptron-arch](http://p15i7i801.bkt.clouddn.com/a1ba9669ccb220d05a8140bc8e0ca797.png)\n> how to learn biases using the same rule as we use for learning weights\n\n  ![nnml-handlebias](http://p15i7i801.bkt.clouddn.com/d73047067e05f31a249dc755851602aa.png)\n\n> 学习的过程：if the output unit is correct, leave its weights alone, if the output unit incorrectly output\n\n![nnml-per-train](http://p15i7i801.bkt.clouddn.com/6b72ab6f556908380fcd6d5561d2e650.png)\n  - __错误的输出了零，则Z需要新增以致其逐渐大于零而产生y=1（往正面），反之则需要减__ ？？此偏移向量为负如何解释，或者必须保证 该向量是指向正确的分类超平面的（perpendicular，正交）。\n  - $(w,b) \\leftarrow (w,b)-(x,1)$\n    $(w,b) \\leftarrow (w,b)+(x,1)$\n![nnml-adpateside](http://p15i7i801.bkt.clouddn.com/b109e1d1e4424d180575bb0f74e75906.png)\n  - *just like penalty function*\n\n> 为什么学习有效: _非正式的收敛证明_\n\n  1. Each time the perceptron makes a mistake, the current weight vector moves to decrease its squared distance from every weight vector in the \"generously feasible\" region.\n  2. <strong> The squared distance decreases by at least the squared length of the input vector. </strong>（向量的运算，几何解释）\n  3. So after a finite number of mistakes, the weight vector must lie in the feasible region if the region exist.\n\n## 感知器的缺陷\n> **right feature**\n\n  Once the hand-coded features have been determined, there are very strong limitations on what a perceptron can learn.\n  _The main question is : This type of table look-up won't generalize, it need too many feature_\n  _此篇习题未理解_\n\n> Group Invariance Theorem---Minsky and Papert\n\ncan't learn to do this if the transformations from a group{欧氏变换之内}，so it needs to use multiple feature unit to recognize transformations of informative sub-patterns.\nSo,more  important was all about how you learn **feature detectors, that's hidden units**. After 20 years, We know,\nwe need multiple layer of adaptive, non-linear hidden units.\n","slug":"Neural-Networks-for-Machine-Learning","published":1,"updated":"2017-12-19T09:22:41.763Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbdf8izj0008icn7un3zurq3","content":"<h1 id=\"机器学习：不必为特定任务编写程序\"><a href=\"#机器学习：不必为特定任务编写程序\" class=\"headerlink\" title=\"机器学习：不必为特定任务编写程序\"></a>机器学习：不必为特定任务编写程序</h1><h2 id=\"场景落地\"><a href=\"#场景落地\" class=\"headerlink\" title=\"场景落地\"></a>场景落地</h2><ul>\n<li>Recognizing pattern     <ul>\n<li>Objects    </li>\n<li>facial    </li>\n<li>Spoken word</li>\n</ul>\n</li>\n<li>Recognizing anomalies    <ul>\n<li>credit card transaction    </li>\n<li>Sensor reality</li>\n</ul>\n</li>\n<li>Prediction    <ul>\n<li>stock price,</li>\n<li>exchange rate   </li>\n<li>movie like</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"几种简单模型\"><a href=\"#几种简单模型\" class=\"headerlink\" title=\"几种简单模型\"></a>几种简单模型</h2><ul>\n<li>Linear neurons<br>$ y=b+\\sum x_iw_i $</li>\n<li>Binary threshold neurons<script type=\"math/tex; mode=display\">\nz=\\Sigma x_i w_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt 0\\\\\n0&otherwise\n\\end{cases} \\quad or \\qquad\n\\theta=-b \\quad\nz=b+\\Sigma x_i y_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt \\theta\\\\\n0&otherwise\n\\end{cases}</script></li>\n<li>Rectified Linear Neurons 2<script type=\"math/tex; mode=display\">z=\\Sigma x_i w_i, y=\n\\begin{cases}\nz,    &if\\;z \\gt 0 \\\\\n0,    &\\mbox{otherwise}\n\\end{cases}</script></li>\n<li>sigmoid neurons<script type=\"math/tex; mode=display\">z=b+\\Sigma x_i w_i  \\quad y=\\frac {1} {1+e^{-z}}</script><img src=\"http://p15i7i801.bkt.clouddn.com/86069ad24cfd3541f4b33070b67fd749.png\" alt=\"nnml-sigmoid\"></li>\n<li>Stochastic binary neurons<script type=\"math/tex; mode=display\">z=b+\\Sigma x_i w_i  \\quad p(s=1)=\\frac {1} {1+e^{-z}}</script></li>\n</ul>\n<h2 id=\"学习的类型\"><a href=\"#学习的类型\" class=\"headerlink\" title=\"学习的类型\"></a>学习的类型</h2><ul>\n<li>supervised learning<ul>\n<li>Regression<ul>\n<li>model class: $y=f(x;w)$</li>\n<li>1/2这个系数在求导时被抵消<br><img src=\"http://p15i7i801.bkt.clouddn.com/93512d5be2aafca06ff3d058aff511b7.png\" alt=\"nnml-regression\"></li>\n</ul>\n</li>\n<li>classification</li>\n</ul>\n</li>\n<li><p>reinforced learning</p>\n<p>the output is an action or sequence of actions and the only supervisory is an occasional scalar reward.<em>This is difficult</em>, The rewads are typically delayed so its had to know where we went wrong( or right)</p>\n</li>\n<li><p>unsupervised learning</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/ada9c5e7fe1e06957b0797be4022d724.png\" alt=\"nnml-unsupervised\"></p>\n<h2 id=\"神经网络的类型\"><a href=\"#神经网络的类型\" class=\"headerlink\" title=\"神经网络的类型\"></a>神经网络的类型</h2></li>\n<li><p>Feed-Forward neural network</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/3053d06760561e6169f20829df15a358.png\" alt=\"nnml-feedforward\"></p>\n<p>if there is more than one hidden layer, we call them “deep” neural networks.</p>\n</li>\n<li><p>Recurrent network</p>\n<p>It is hard to train but natural way for modeling sequence, recent algorithm can be able to do this, this also can be used to predict stock price.</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/83abfe6f730b3ae61d9099ff0d5ff300.png\" alt=\"nnml-recurrent\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/3a688df70ed6bf1e46d62a03402e2a39.png\" alt=\"nnml-rnn\"></p>\n<blockquote>\n<p>20171218 LSTM 现在貌似更流行</p>\n</blockquote>\n</li>\n<li><p>Symmetrically connected networks</p>\n</li>\n</ul>\n<h2 id=\"感知器\"><a href=\"#感知器\" class=\"headerlink\" title=\"感知器\"></a>感知器</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/a1ba9669ccb220d05a8140bc8e0ca797.png\" alt=\"nnml-perceptron-arch\"></p>\n<blockquote>\n<p>how to learn biases using the same rule as we use for learning weights</p>\n</blockquote>\n<p>  <img src=\"http://p15i7i801.bkt.clouddn.com/d73047067e05f31a249dc755851602aa.png\" alt=\"nnml-handlebias\"></p>\n<blockquote>\n<p>学习的过程：if the output unit is correct, leave its weights alone, if the output unit incorrectly output</p>\n</blockquote>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/6b72ab6f556908380fcd6d5561d2e650.png\" alt=\"nnml-per-train\"></p>\n<ul>\n<li><strong>错误的输出了零，则Z需要新增以致其逐渐大于零而产生y=1（往正面），反之则需要减</strong> ？？此偏移向量为负如何解释，或者必须保证 该向量是指向正确的分类超平面的（perpendicular，正交）。</li>\n<li>$(w,b) \\leftarrow (w,b)-(x,1)$<br>$(w,b) \\leftarrow (w,b)+(x,1)$<br><img src=\"http://p15i7i801.bkt.clouddn.com/b109e1d1e4424d180575bb0f74e75906.png\" alt=\"nnml-adpateside\"></li>\n<li><em>just like penalty function</em></li>\n</ul>\n<blockquote>\n<p>为什么学习有效: <em>非正式的收敛证明</em></p>\n</blockquote>\n<ol>\n<li>Each time the perceptron makes a mistake, the current weight vector moves to decrease its squared distance from every weight vector in the “generously feasible” region.</li>\n<li><strong> The squared distance decreases by at least the squared length of the input vector. </strong>（向量的运算，几何解释）</li>\n<li>So after a finite number of mistakes, the weight vector must lie in the feasible region if the region exist.</li>\n</ol>\n<h2 id=\"感知器的缺陷\"><a href=\"#感知器的缺陷\" class=\"headerlink\" title=\"感知器的缺陷\"></a>感知器的缺陷</h2><blockquote>\n<p><strong>right feature</strong></p>\n</blockquote>\n<p>  Once the hand-coded features have been determined, there are very strong limitations on what a perceptron can learn.<br>  <em>The main question is : This type of table look-up won’t generalize, it need too many feature</em><br>  <em>此篇习题未理解</em></p>\n<blockquote>\n<p>Group Invariance Theorem—-Minsky and Papert</p>\n</blockquote>\n<p>can’t learn to do this if the transformations from a group{欧氏变换之内}，so it needs to use multiple feature unit to recognize transformations of informative sub-patterns.<br>So,more  important was all about how you learn <strong>feature detectors, that’s hidden units</strong>. After 20 years, We know,<br>we need multiple layer of adaptive, non-linear hidden units.</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"机器学习：不必为特定任务编写程序\"><a href=\"#机器学习：不必为特定任务编写程序\" class=\"headerlink\" title=\"机器学习：不必为特定任务编写程序\"></a>机器学习：不必为特定任务编写程序</h1><h2 id=\"场景落地\"><a href=\"#场景落地\" class=\"headerlink\" title=\"场景落地\"></a>场景落地</h2><ul>\n<li>Recognizing pattern     <ul>\n<li>Objects    </li>\n<li>facial    </li>\n<li>Spoken word</li>\n</ul>\n</li>\n<li>Recognizing anomalies    <ul>\n<li>credit card transaction    </li>\n<li>Sensor reality</li>\n</ul>\n</li>\n<li>Prediction    <ul>\n<li>stock price,</li>\n<li>exchange rate   </li>\n<li>movie like</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"几种简单模型\"><a href=\"#几种简单模型\" class=\"headerlink\" title=\"几种简单模型\"></a>几种简单模型</h2><ul>\n<li>Linear neurons<br>$ y=b+\\sum x_iw_i $</li>\n<li>Binary threshold neurons<script type=\"math/tex; mode=display\">\nz=\\Sigma x_i w_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt 0\\\\\n0&otherwise\n\\end{cases} \\quad or \\qquad\n\\theta=-b \\quad\nz=b+\\Sigma x_i y_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt \\theta\\\\\n0&otherwise\n\\end{cases}</script></li>\n<li>Rectified Linear Neurons 2<script type=\"math/tex; mode=display\">z=\\Sigma x_i w_i, y=\n\\begin{cases}\nz,    &if\\;z \\gt 0 \\\\\n0,    &\\mbox{otherwise}\n\\end{cases}</script></li>\n<li>sigmoid neurons<script type=\"math/tex; mode=display\">z=b+\\Sigma x_i w_i  \\quad y=\\frac {1} {1+e^{-z}}</script><img src=\"http://p15i7i801.bkt.clouddn.com/86069ad24cfd3541f4b33070b67fd749.png\" alt=\"nnml-sigmoid\"></li>\n<li>Stochastic binary neurons<script type=\"math/tex; mode=display\">z=b+\\Sigma x_i w_i  \\quad p(s=1)=\\frac {1} {1+e^{-z}}</script></li>\n</ul>\n<h2 id=\"学习的类型\"><a href=\"#学习的类型\" class=\"headerlink\" title=\"学习的类型\"></a>学习的类型</h2><ul>\n<li>supervised learning<ul>\n<li>Regression<ul>\n<li>model class: $y=f(x;w)$</li>\n<li>1/2这个系数在求导时被抵消<br><img src=\"http://p15i7i801.bkt.clouddn.com/93512d5be2aafca06ff3d058aff511b7.png\" alt=\"nnml-regression\"></li>\n</ul>\n</li>\n<li>classification</li>\n</ul>\n</li>\n<li><p>reinforced learning</p>\n<p>the output is an action or sequence of actions and the only supervisory is an occasional scalar reward.<em>This is difficult</em>, The rewads are typically delayed so its had to know where we went wrong( or right)</p>\n</li>\n<li><p>unsupervised learning</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/ada9c5e7fe1e06957b0797be4022d724.png\" alt=\"nnml-unsupervised\"></p>\n<h2 id=\"神经网络的类型\"><a href=\"#神经网络的类型\" class=\"headerlink\" title=\"神经网络的类型\"></a>神经网络的类型</h2></li>\n<li><p>Feed-Forward neural network</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/3053d06760561e6169f20829df15a358.png\" alt=\"nnml-feedforward\"></p>\n<p>if there is more than one hidden layer, we call them “deep” neural networks.</p>\n</li>\n<li><p>Recurrent network</p>\n<p>It is hard to train but natural way for modeling sequence, recent algorithm can be able to do this, this also can be used to predict stock price.</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/83abfe6f730b3ae61d9099ff0d5ff300.png\" alt=\"nnml-recurrent\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/3a688df70ed6bf1e46d62a03402e2a39.png\" alt=\"nnml-rnn\"></p>\n<blockquote>\n<p>20171218 LSTM 现在貌似更流行</p>\n</blockquote>\n</li>\n<li><p>Symmetrically connected networks</p>\n</li>\n</ul>\n<h2 id=\"感知器\"><a href=\"#感知器\" class=\"headerlink\" title=\"感知器\"></a>感知器</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/a1ba9669ccb220d05a8140bc8e0ca797.png\" alt=\"nnml-perceptron-arch\"></p>\n<blockquote>\n<p>how to learn biases using the same rule as we use for learning weights</p>\n</blockquote>\n<p>  <img src=\"http://p15i7i801.bkt.clouddn.com/d73047067e05f31a249dc755851602aa.png\" alt=\"nnml-handlebias\"></p>\n<blockquote>\n<p>学习的过程：if the output unit is correct, leave its weights alone, if the output unit incorrectly output</p>\n</blockquote>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/6b72ab6f556908380fcd6d5561d2e650.png\" alt=\"nnml-per-train\"></p>\n<ul>\n<li><strong>错误的输出了零，则Z需要新增以致其逐渐大于零而产生y=1（往正面），反之则需要减</strong> ？？此偏移向量为负如何解释，或者必须保证 该向量是指向正确的分类超平面的（perpendicular，正交）。</li>\n<li>$(w,b) \\leftarrow (w,b)-(x,1)$<br>$(w,b) \\leftarrow (w,b)+(x,1)$<br><img src=\"http://p15i7i801.bkt.clouddn.com/b109e1d1e4424d180575bb0f74e75906.png\" alt=\"nnml-adpateside\"></li>\n<li><em>just like penalty function</em></li>\n</ul>\n<blockquote>\n<p>为什么学习有效: <em>非正式的收敛证明</em></p>\n</blockquote>\n<ol>\n<li>Each time the perceptron makes a mistake, the current weight vector moves to decrease its squared distance from every weight vector in the “generously feasible” region.</li>\n<li><strong> The squared distance decreases by at least the squared length of the input vector. </strong>（向量的运算，几何解释）</li>\n<li>So after a finite number of mistakes, the weight vector must lie in the feasible region if the region exist.</li>\n</ol>\n<h2 id=\"感知器的缺陷\"><a href=\"#感知器的缺陷\" class=\"headerlink\" title=\"感知器的缺陷\"></a>感知器的缺陷</h2><blockquote>\n<p><strong>right feature</strong></p>\n</blockquote>\n<p>  Once the hand-coded features have been determined, there are very strong limitations on what a perceptron can learn.<br>  <em>The main question is : This type of table look-up won’t generalize, it need too many feature</em><br>  <em>此篇习题未理解</em></p>\n<blockquote>\n<p>Group Invariance Theorem—-Minsky and Papert</p>\n</blockquote>\n<p>can’t learn to do this if the transformations from a group{欧氏变换之内}，so it needs to use multiple feature unit to recognize transformations of informative sub-patterns.<br>So,more  important was all about how you learn <strong>feature detectors, that’s hidden units</strong>. After 20 years, We know,<br>we need multiple layer of adaptive, non-linear hidden units.</p>\n"},{"title":"改善深层神经网络：超参数调试、正则化以及优化","date":"2017-09-15T09:45:29.000Z","mathjax":true,"_content":"\n\n#  第一周 深度学习的实用层面\n## 训练、验证、测试集\n* 在小数时代，比例一般为6：2：2，大数据时代可适当调整，验证及测试集为10000左右即可。\n* train set 最好和后两者有相当的分布，比如猫的问题，在网上爬的猫有许多精良的照片，而用户实际上传的可能比较糙。\n* test set是为了拿到测试的无偏估计\n* ML是一个高度迭代的过程，即使最牛的专家也是如此。\n\n## 偏差与方差\n* 偏差代表训练集的错误率有多高，方差代表dev set 和train set的错误率的差别\n\n## 正则化\nregularizer，规则化，通过先验的知识来最大化贝叶斯最大后验估计，本质上是贝叶斯最大后验估计的**似然函数**\n* L1\n* L2：Frobenius Norm，直观上Lambda越大，权重矩阵被设为零\n![nn_reg_12181821](http://p15i7i801.bkt.clouddn.com/2d81fcb8cfe1cb048929564aa91f9c4f.png)\n\n* dropout\n* 其它减少过拟合的方法\n    * Data augmentation\n    * early stop: 此种方法相对L2来说减少lambda的尝试计算量\n\n## 正则化输入（normalize)\n使用零均值，主要是用来提升速度，范围尺度类似即可，也不是一定要0-1。如下图，若不同特征相差尺度很大，则会形成如左不规则超球体，这样w的迭代次数会大大增加，对应的迭代步长也应设置偏小。\n## 梯度消失及爆炸\n由于层数太多，梯度指数级变化，这容易导致步长会非常小，训练时间会变得很长。传统激活函数都存在这一缺点，正向时爆炸，反向时消失，为防止上述问题，权重的初始值的选择有：\n* 若用RELU作为激活函数（[种类及特点](http://blog.csdn.net/mzpmzk/article/details/77418030)）， $w$初始值可以使用$np.sqrt(\\frac{2}{n^{[l-1]}+n^{[l]}})$\n\n# 第二周 优化算法\n* Min-batch梯度下降：避免需要处理所有数据才能进行下一步\n* 指数加权平均\n* 动量（Momentum）梯度下降:类似移动平均线减缓摆动\n$$ v_{dw} = \\beta v_{dw} + (1-\\beta)dw $$\n$$ v_{db}=\\beta v_{db} + (1-\\beta)db $$\n$$ w=w-\\alpha v_{dw} $$\n$$b=b-\\alpha v_{db}$$\n* RMSprop：让幅度大的参数变缓，让幅度小的参数变大\n\n$$ S_{dw} = \\beta S_{dw} + (1-\\beta){dw}^2 $$\n$$ S_{db}=\\beta S_{db} + (1-\\beta){db}^2$$\n$$ w=w-\\alpha \\frac{dw}{\\sqrt{S_{dw}}}$$\n$$ b=b-\\alpha \\frac{db}{\\sqrt{S_{db}}}$$\n* Adam(Adaptive Moment Estimation): 各算法适应于不同的神经网络结构，该算法结合Momentum and RMSprop，非常常用。\n  * 公式\n  $$ v_{dw} = \\beta v_{dw} + (1-\\beta_1)dw $$\n  $$ v_{db}=\\beta v_{db} + (1-\\beta_1)db$$\n  $$ S_{dw} = \\beta S_{dw} + (1-\\beta_2){dw}^2 $$\n  $$ S_{db}=\\beta S_{db} + (1-\\beta_2){db}^2$$\n  $$ w=w-\\alpha \\frac{v_{dw}}{\\sqrt{S_{dw}}}$$\n  $$ b=b-\\alpha \\frac{v_{db}}{\\sqrt{S_{db}}}$$\n  * 超参数一般值：\n  $$\\alpha:needs to be tune \\quad  \\beta_1(first moment):0.9 \\quad \\beta_2(second momnet):0.99$$\n* 学习率衰减：多个方法，如倒数、指数等等\n* 关于局部最优：在大量参数情况下可能会出现saddle问题，这样问题容易到达plateau状态，较难走出。这种可能性较小，通常使用Adam之类的优化方法解决\n\n# 第三周 超参数调度、Batch正则与程序框架\n\n## 超参数调整\n* 首先是红色的 $\\alpha$，再者是黄色的学习率，第一、二动量及min-batch，最后再考虑调整层数及学习率延时 $\\varepsilon$。\n* 使用随机数、从粗到细\n* 使用合适的分布尺寸，而不是使用均匀分布，比如对于$\\alpha$可以使用a log scala，举例而言：\n  ```python\n  r = -4*np.random.rand()\n  alpha = 10^r\n  ```\n\n## 实践经验：两个流派\n* 使用各领域的常见参数（**这意味需要对各个智能领域有的了解**）\n* 直觉很有效，**定期更新参数，比如几个月**\n* babysitting（pandan）: 长期照看，评估调整——（**不断的持久化模型吗？，一直迭代下去，还是每次都重新训练，模型可以以直训练下去吗？**）\n* train in paraller: Caviar(鱼子酱)\n\n## 网络内激活函数的normalizaion\n请注意以下三点：\n* 通过 $\\alpha、\\beta$两个超参数，可以控制 $Z^i$ 的范围，如右上图所示\n* 单层网络中我们通过normalizaion来加快梯度下降的速度，那么也可以通过控制隐藏层的输入值来控制\n* andrew的推荐，normalize z rather than a\n\n## 将Batch Norm拟合进神经网络\n* 偏执项b从此被平均，即式子可简化为 $z^l=w^l a^{l-1}$\n\n## Batch Norm奏效的原因\n* 加速GD\n* It makes weights, later or deeper than your network, more robost to changes to weights in earlier layers of neural netowork.\n举例而言，对于不带颜色的猫可能训练得很好，当带上颜色后，对于同样判断是否为猫的模型而言，输入数据的分布变化了，导致模型需要重新训练。这也称为**covariate shift**，为解决这个问题，可以使用Batch Norm。\n* 顺便的作用，如dropout，引入了噪声，相当于加入了regularization.\n\n## 测试模型时BatchNorm的计算\n* 需要估算平均值及标准差\n* 简单靠谱的处理方式是用指数加权平均去**估算**\n* 跑全量数据batchnorm也可以\n\n## softmax分类:LR的一般形式\n","source":"_posts/改善深层神经网络：超参数调试、正则化以及优化.md","raw":"---\ntitle: 改善深层神经网络：超参数调试、正则化以及优化\ndate: 2017-09-15 17:45:29\ntags:\n      - 深度学习\n      - Andrew NG\n      - 公开课\ncategories: AI梦\nmathjax: true\n---\n\n\n#  第一周 深度学习的实用层面\n## 训练、验证、测试集\n* 在小数时代，比例一般为6：2：2，大数据时代可适当调整，验证及测试集为10000左右即可。\n* train set 最好和后两者有相当的分布，比如猫的问题，在网上爬的猫有许多精良的照片，而用户实际上传的可能比较糙。\n* test set是为了拿到测试的无偏估计\n* ML是一个高度迭代的过程，即使最牛的专家也是如此。\n\n## 偏差与方差\n* 偏差代表训练集的错误率有多高，方差代表dev set 和train set的错误率的差别\n\n## 正则化\nregularizer，规则化，通过先验的知识来最大化贝叶斯最大后验估计，本质上是贝叶斯最大后验估计的**似然函数**\n* L1\n* L2：Frobenius Norm，直观上Lambda越大，权重矩阵被设为零\n![nn_reg_12181821](http://p15i7i801.bkt.clouddn.com/2d81fcb8cfe1cb048929564aa91f9c4f.png)\n\n* dropout\n* 其它减少过拟合的方法\n    * Data augmentation\n    * early stop: 此种方法相对L2来说减少lambda的尝试计算量\n\n## 正则化输入（normalize)\n使用零均值，主要是用来提升速度，范围尺度类似即可，也不是一定要0-1。如下图，若不同特征相差尺度很大，则会形成如左不规则超球体，这样w的迭代次数会大大增加，对应的迭代步长也应设置偏小。\n## 梯度消失及爆炸\n由于层数太多，梯度指数级变化，这容易导致步长会非常小，训练时间会变得很长。传统激活函数都存在这一缺点，正向时爆炸，反向时消失，为防止上述问题，权重的初始值的选择有：\n* 若用RELU作为激活函数（[种类及特点](http://blog.csdn.net/mzpmzk/article/details/77418030)）， $w$初始值可以使用$np.sqrt(\\frac{2}{n^{[l-1]}+n^{[l]}})$\n\n# 第二周 优化算法\n* Min-batch梯度下降：避免需要处理所有数据才能进行下一步\n* 指数加权平均\n* 动量（Momentum）梯度下降:类似移动平均线减缓摆动\n$$ v_{dw} = \\beta v_{dw} + (1-\\beta)dw $$\n$$ v_{db}=\\beta v_{db} + (1-\\beta)db $$\n$$ w=w-\\alpha v_{dw} $$\n$$b=b-\\alpha v_{db}$$\n* RMSprop：让幅度大的参数变缓，让幅度小的参数变大\n\n$$ S_{dw} = \\beta S_{dw} + (1-\\beta){dw}^2 $$\n$$ S_{db}=\\beta S_{db} + (1-\\beta){db}^2$$\n$$ w=w-\\alpha \\frac{dw}{\\sqrt{S_{dw}}}$$\n$$ b=b-\\alpha \\frac{db}{\\sqrt{S_{db}}}$$\n* Adam(Adaptive Moment Estimation): 各算法适应于不同的神经网络结构，该算法结合Momentum and RMSprop，非常常用。\n  * 公式\n  $$ v_{dw} = \\beta v_{dw} + (1-\\beta_1)dw $$\n  $$ v_{db}=\\beta v_{db} + (1-\\beta_1)db$$\n  $$ S_{dw} = \\beta S_{dw} + (1-\\beta_2){dw}^2 $$\n  $$ S_{db}=\\beta S_{db} + (1-\\beta_2){db}^2$$\n  $$ w=w-\\alpha \\frac{v_{dw}}{\\sqrt{S_{dw}}}$$\n  $$ b=b-\\alpha \\frac{v_{db}}{\\sqrt{S_{db}}}$$\n  * 超参数一般值：\n  $$\\alpha:needs to be tune \\quad  \\beta_1(first moment):0.9 \\quad \\beta_2(second momnet):0.99$$\n* 学习率衰减：多个方法，如倒数、指数等等\n* 关于局部最优：在大量参数情况下可能会出现saddle问题，这样问题容易到达plateau状态，较难走出。这种可能性较小，通常使用Adam之类的优化方法解决\n\n# 第三周 超参数调度、Batch正则与程序框架\n\n## 超参数调整\n* 首先是红色的 $\\alpha$，再者是黄色的学习率，第一、二动量及min-batch，最后再考虑调整层数及学习率延时 $\\varepsilon$。\n* 使用随机数、从粗到细\n* 使用合适的分布尺寸，而不是使用均匀分布，比如对于$\\alpha$可以使用a log scala，举例而言：\n  ```python\n  r = -4*np.random.rand()\n  alpha = 10^r\n  ```\n\n## 实践经验：两个流派\n* 使用各领域的常见参数（**这意味需要对各个智能领域有的了解**）\n* 直觉很有效，**定期更新参数，比如几个月**\n* babysitting（pandan）: 长期照看，评估调整——（**不断的持久化模型吗？，一直迭代下去，还是每次都重新训练，模型可以以直训练下去吗？**）\n* train in paraller: Caviar(鱼子酱)\n\n## 网络内激活函数的normalizaion\n请注意以下三点：\n* 通过 $\\alpha、\\beta$两个超参数，可以控制 $Z^i$ 的范围，如右上图所示\n* 单层网络中我们通过normalizaion来加快梯度下降的速度，那么也可以通过控制隐藏层的输入值来控制\n* andrew的推荐，normalize z rather than a\n\n## 将Batch Norm拟合进神经网络\n* 偏执项b从此被平均，即式子可简化为 $z^l=w^l a^{l-1}$\n\n## Batch Norm奏效的原因\n* 加速GD\n* It makes weights, later or deeper than your network, more robost to changes to weights in earlier layers of neural netowork.\n举例而言，对于不带颜色的猫可能训练得很好，当带上颜色后，对于同样判断是否为猫的模型而言，输入数据的分布变化了，导致模型需要重新训练。这也称为**covariate shift**，为解决这个问题，可以使用Batch Norm。\n* 顺便的作用，如dropout，引入了噪声，相当于加入了regularization.\n\n## 测试模型时BatchNorm的计算\n* 需要估算平均值及标准差\n* 简单靠谱的处理方式是用指数加权平均去**估算**\n* 跑全量数据batchnorm也可以\n\n## softmax分类:LR的一般形式\n","slug":"改善深层神经网络：超参数调试、正则化以及优化","published":1,"updated":"2017-12-18T10:27:55.793Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbdf8izk0009icn7zetq7zg4","content":"<h1 id=\"第一周-深度学习的实用层面\"><a href=\"#第一周-深度学习的实用层面\" class=\"headerlink\" title=\"第一周 深度学习的实用层面\"></a>第一周 深度学习的实用层面</h1><h2 id=\"训练、验证、测试集\"><a href=\"#训练、验证、测试集\" class=\"headerlink\" title=\"训练、验证、测试集\"></a>训练、验证、测试集</h2><ul>\n<li>在小数时代，比例一般为6：2：2，大数据时代可适当调整，验证及测试集为10000左右即可。</li>\n<li>train set 最好和后两者有相当的分布，比如猫的问题，在网上爬的猫有许多精良的照片，而用户实际上传的可能比较糙。</li>\n<li>test set是为了拿到测试的无偏估计</li>\n<li>ML是一个高度迭代的过程，即使最牛的专家也是如此。</li>\n</ul>\n<h2 id=\"偏差与方差\"><a href=\"#偏差与方差\" class=\"headerlink\" title=\"偏差与方差\"></a>偏差与方差</h2><ul>\n<li>偏差代表训练集的错误率有多高，方差代表dev set 和train set的错误率的差别</li>\n</ul>\n<h2 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h2><p>regularizer，规则化，通过先验的知识来最大化贝叶斯最大后验估计，本质上是贝叶斯最大后验估计的<strong>似然函数</strong></p>\n<ul>\n<li>L1</li>\n<li><p>L2：Frobenius Norm，直观上Lambda越大，权重矩阵被设为零<br><img src=\"http://p15i7i801.bkt.clouddn.com/2d81fcb8cfe1cb048929564aa91f9c4f.png\" alt=\"nn_reg_12181821\"></p>\n</li>\n<li><p>dropout</p>\n</li>\n<li>其它减少过拟合的方法<ul>\n<li>Data augmentation</li>\n<li>early stop: 此种方法相对L2来说减少lambda的尝试计算量</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"正则化输入（normalize\"><a href=\"#正则化输入（normalize\" class=\"headerlink\" title=\"正则化输入（normalize)\"></a>正则化输入（normalize)</h2><p>使用零均值，主要是用来提升速度，范围尺度类似即可，也不是一定要0-1。如下图，若不同特征相差尺度很大，则会形成如左不规则超球体，这样w的迭代次数会大大增加，对应的迭代步长也应设置偏小。</p>\n<h2 id=\"梯度消失及爆炸\"><a href=\"#梯度消失及爆炸\" class=\"headerlink\" title=\"梯度消失及爆炸\"></a>梯度消失及爆炸</h2><p>由于层数太多，梯度指数级变化，这容易导致步长会非常小，训练时间会变得很长。传统激活函数都存在这一缺点，正向时爆炸，反向时消失，为防止上述问题，权重的初始值的选择有：</p>\n<ul>\n<li>若用RELU作为激活函数（<a href=\"http://blog.csdn.net/mzpmzk/article/details/77418030\" target=\"_blank\" rel=\"noopener\">种类及特点</a>）， $w$初始值可以使用$np.sqrt(\\frac{2}{n^{[l-1]}+n^{[l]}})$</li>\n</ul>\n<h1 id=\"第二周-优化算法\"><a href=\"#第二周-优化算法\" class=\"headerlink\" title=\"第二周 优化算法\"></a>第二周 优化算法</h1><ul>\n<li>Min-batch梯度下降：避免需要处理所有数据才能进行下一步</li>\n<li>指数加权平均</li>\n<li>动量（Momentum）梯度下降:类似移动平均线减缓摆动<script type=\"math/tex; mode=display\">v_{dw} = \\beta v_{dw} + (1-\\beta)dw</script><script type=\"math/tex; mode=display\">v_{db}=\\beta v_{db} + (1-\\beta)db</script><script type=\"math/tex; mode=display\">w=w-\\alpha v_{dw}</script><script type=\"math/tex; mode=display\">b=b-\\alpha v_{db}</script></li>\n<li>RMSprop：让幅度大的参数变缓，让幅度小的参数变大</li>\n</ul>\n<script type=\"math/tex; mode=display\">S_{dw} = \\beta S_{dw} + (1-\\beta){dw}^2</script><script type=\"math/tex; mode=display\">S_{db}=\\beta S_{db} + (1-\\beta){db}^2</script><script type=\"math/tex; mode=display\">w=w-\\alpha \\frac{dw}{\\sqrt{S_{dw}}}</script><script type=\"math/tex; mode=display\">b=b-\\alpha \\frac{db}{\\sqrt{S_{db}}}</script><ul>\n<li>Adam(Adaptive Moment Estimation): 各算法适应于不同的神经网络结构，该算法结合Momentum and RMSprop，非常常用。<ul>\n<li>公式<script type=\"math/tex; mode=display\">v_{dw} = \\beta v_{dw} + (1-\\beta_1)dw</script><script type=\"math/tex; mode=display\">v_{db}=\\beta v_{db} + (1-\\beta_1)db</script><script type=\"math/tex; mode=display\">S_{dw} = \\beta S_{dw} + (1-\\beta_2){dw}^2</script><script type=\"math/tex; mode=display\">S_{db}=\\beta S_{db} + (1-\\beta_2){db}^2</script><script type=\"math/tex; mode=display\">w=w-\\alpha \\frac{v_{dw}}{\\sqrt{S_{dw}}}</script><script type=\"math/tex; mode=display\">b=b-\\alpha \\frac{v_{db}}{\\sqrt{S_{db}}}</script></li>\n<li>超参数一般值：<script type=\"math/tex; mode=display\">\\alpha:needs to be tune \\quad  \\beta_1(first moment):0.9 \\quad \\beta_2(second momnet):0.99</script></li>\n</ul>\n</li>\n<li>学习率衰减：多个方法，如倒数、指数等等</li>\n<li>关于局部最优：在大量参数情况下可能会出现saddle问题，这样问题容易到达plateau状态，较难走出。这种可能性较小，通常使用Adam之类的优化方法解决</li>\n</ul>\n<h1 id=\"第三周-超参数调度、Batch正则与程序框架\"><a href=\"#第三周-超参数调度、Batch正则与程序框架\" class=\"headerlink\" title=\"第三周 超参数调度、Batch正则与程序框架\"></a>第三周 超参数调度、Batch正则与程序框架</h1><h2 id=\"超参数调整\"><a href=\"#超参数调整\" class=\"headerlink\" title=\"超参数调整\"></a>超参数调整</h2><ul>\n<li>首先是红色的 $\\alpha$，再者是黄色的学习率，第一、二动量及min-batch，最后再考虑调整层数及学习率延时 $\\varepsilon$。</li>\n<li>使用随机数、从粗到细</li>\n<li>使用合适的分布尺寸，而不是使用均匀分布，比如对于$\\alpha$可以使用a log scala，举例而言：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">r = <span class=\"number\">-4</span>*np.random.rand()</span><br><span class=\"line\">alpha = <span class=\"number\">10</span>^r</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"实践经验：两个流派\"><a href=\"#实践经验：两个流派\" class=\"headerlink\" title=\"实践经验：两个流派\"></a>实践经验：两个流派</h2><ul>\n<li>使用各领域的常见参数（<strong>这意味需要对各个智能领域有的了解</strong>）</li>\n<li>直觉很有效，<strong>定期更新参数，比如几个月</strong></li>\n<li>babysitting（pandan）: 长期照看，评估调整——（<strong>不断的持久化模型吗？，一直迭代下去，还是每次都重新训练，模型可以以直训练下去吗？</strong>）</li>\n<li>train in paraller: Caviar(鱼子酱)</li>\n</ul>\n<h2 id=\"网络内激活函数的normalizaion\"><a href=\"#网络内激活函数的normalizaion\" class=\"headerlink\" title=\"网络内激活函数的normalizaion\"></a>网络内激活函数的normalizaion</h2><p>请注意以下三点：</p>\n<ul>\n<li>通过 $\\alpha、\\beta$两个超参数，可以控制 $Z^i$ 的范围，如右上图所示</li>\n<li>单层网络中我们通过normalizaion来加快梯度下降的速度，那么也可以通过控制隐藏层的输入值来控制</li>\n<li>andrew的推荐，normalize z rather than a</li>\n</ul>\n<h2 id=\"将Batch-Norm拟合进神经网络\"><a href=\"#将Batch-Norm拟合进神经网络\" class=\"headerlink\" title=\"将Batch Norm拟合进神经网络\"></a>将Batch Norm拟合进神经网络</h2><ul>\n<li>偏执项b从此被平均，即式子可简化为 $z^l=w^l a^{l-1}$</li>\n</ul>\n<h2 id=\"Batch-Norm奏效的原因\"><a href=\"#Batch-Norm奏效的原因\" class=\"headerlink\" title=\"Batch Norm奏效的原因\"></a>Batch Norm奏效的原因</h2><ul>\n<li>加速GD</li>\n<li>It makes weights, later or deeper than your network, more robost to changes to weights in earlier layers of neural netowork.<br>举例而言，对于不带颜色的猫可能训练得很好，当带上颜色后，对于同样判断是否为猫的模型而言，输入数据的分布变化了，导致模型需要重新训练。这也称为<strong>covariate shift</strong>，为解决这个问题，可以使用Batch Norm。</li>\n<li>顺便的作用，如dropout，引入了噪声，相当于加入了regularization.</li>\n</ul>\n<h2 id=\"测试模型时BatchNorm的计算\"><a href=\"#测试模型时BatchNorm的计算\" class=\"headerlink\" title=\"测试模型时BatchNorm的计算\"></a>测试模型时BatchNorm的计算</h2><ul>\n<li>需要估算平均值及标准差</li>\n<li>简单靠谱的处理方式是用指数加权平均去<strong>估算</strong></li>\n<li>跑全量数据batchnorm也可以</li>\n</ul>\n<h2 id=\"softmax分类-LR的一般形式\"><a href=\"#softmax分类-LR的一般形式\" class=\"headerlink\" title=\"softmax分类:LR的一般形式\"></a>softmax分类:LR的一般形式</h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"第一周-深度学习的实用层面\"><a href=\"#第一周-深度学习的实用层面\" class=\"headerlink\" title=\"第一周 深度学习的实用层面\"></a>第一周 深度学习的实用层面</h1><h2 id=\"训练、验证、测试集\"><a href=\"#训练、验证、测试集\" class=\"headerlink\" title=\"训练、验证、测试集\"></a>训练、验证、测试集</h2><ul>\n<li>在小数时代，比例一般为6：2：2，大数据时代可适当调整，验证及测试集为10000左右即可。</li>\n<li>train set 最好和后两者有相当的分布，比如猫的问题，在网上爬的猫有许多精良的照片，而用户实际上传的可能比较糙。</li>\n<li>test set是为了拿到测试的无偏估计</li>\n<li>ML是一个高度迭代的过程，即使最牛的专家也是如此。</li>\n</ul>\n<h2 id=\"偏差与方差\"><a href=\"#偏差与方差\" class=\"headerlink\" title=\"偏差与方差\"></a>偏差与方差</h2><ul>\n<li>偏差代表训练集的错误率有多高，方差代表dev set 和train set的错误率的差别</li>\n</ul>\n<h2 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h2><p>regularizer，规则化，通过先验的知识来最大化贝叶斯最大后验估计，本质上是贝叶斯最大后验估计的<strong>似然函数</strong></p>\n<ul>\n<li>L1</li>\n<li><p>L2：Frobenius Norm，直观上Lambda越大，权重矩阵被设为零<br><img src=\"http://p15i7i801.bkt.clouddn.com/2d81fcb8cfe1cb048929564aa91f9c4f.png\" alt=\"nn_reg_12181821\"></p>\n</li>\n<li><p>dropout</p>\n</li>\n<li>其它减少过拟合的方法<ul>\n<li>Data augmentation</li>\n<li>early stop: 此种方法相对L2来说减少lambda的尝试计算量</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"正则化输入（normalize\"><a href=\"#正则化输入（normalize\" class=\"headerlink\" title=\"正则化输入（normalize)\"></a>正则化输入（normalize)</h2><p>使用零均值，主要是用来提升速度，范围尺度类似即可，也不是一定要0-1。如下图，若不同特征相差尺度很大，则会形成如左不规则超球体，这样w的迭代次数会大大增加，对应的迭代步长也应设置偏小。</p>\n<h2 id=\"梯度消失及爆炸\"><a href=\"#梯度消失及爆炸\" class=\"headerlink\" title=\"梯度消失及爆炸\"></a>梯度消失及爆炸</h2><p>由于层数太多，梯度指数级变化，这容易导致步长会非常小，训练时间会变得很长。传统激活函数都存在这一缺点，正向时爆炸，反向时消失，为防止上述问题，权重的初始值的选择有：</p>\n<ul>\n<li>若用RELU作为激活函数（<a href=\"http://blog.csdn.net/mzpmzk/article/details/77418030\" target=\"_blank\" rel=\"noopener\">种类及特点</a>）， $w$初始值可以使用$np.sqrt(\\frac{2}{n^{[l-1]}+n^{[l]}})$</li>\n</ul>\n<h1 id=\"第二周-优化算法\"><a href=\"#第二周-优化算法\" class=\"headerlink\" title=\"第二周 优化算法\"></a>第二周 优化算法</h1><ul>\n<li>Min-batch梯度下降：避免需要处理所有数据才能进行下一步</li>\n<li>指数加权平均</li>\n<li>动量（Momentum）梯度下降:类似移动平均线减缓摆动<script type=\"math/tex; mode=display\">v_{dw} = \\beta v_{dw} + (1-\\beta)dw</script><script type=\"math/tex; mode=display\">v_{db}=\\beta v_{db} + (1-\\beta)db</script><script type=\"math/tex; mode=display\">w=w-\\alpha v_{dw}</script><script type=\"math/tex; mode=display\">b=b-\\alpha v_{db}</script></li>\n<li>RMSprop：让幅度大的参数变缓，让幅度小的参数变大</li>\n</ul>\n<script type=\"math/tex; mode=display\">S_{dw} = \\beta S_{dw} + (1-\\beta){dw}^2</script><script type=\"math/tex; mode=display\">S_{db}=\\beta S_{db} + (1-\\beta){db}^2</script><script type=\"math/tex; mode=display\">w=w-\\alpha \\frac{dw}{\\sqrt{S_{dw}}}</script><script type=\"math/tex; mode=display\">b=b-\\alpha \\frac{db}{\\sqrt{S_{db}}}</script><ul>\n<li>Adam(Adaptive Moment Estimation): 各算法适应于不同的神经网络结构，该算法结合Momentum and RMSprop，非常常用。<ul>\n<li>公式<script type=\"math/tex; mode=display\">v_{dw} = \\beta v_{dw} + (1-\\beta_1)dw</script><script type=\"math/tex; mode=display\">v_{db}=\\beta v_{db} + (1-\\beta_1)db</script><script type=\"math/tex; mode=display\">S_{dw} = \\beta S_{dw} + (1-\\beta_2){dw}^2</script><script type=\"math/tex; mode=display\">S_{db}=\\beta S_{db} + (1-\\beta_2){db}^2</script><script type=\"math/tex; mode=display\">w=w-\\alpha \\frac{v_{dw}}{\\sqrt{S_{dw}}}</script><script type=\"math/tex; mode=display\">b=b-\\alpha \\frac{v_{db}}{\\sqrt{S_{db}}}</script></li>\n<li>超参数一般值：<script type=\"math/tex; mode=display\">\\alpha:needs to be tune \\quad  \\beta_1(first moment):0.9 \\quad \\beta_2(second momnet):0.99</script></li>\n</ul>\n</li>\n<li>学习率衰减：多个方法，如倒数、指数等等</li>\n<li>关于局部最优：在大量参数情况下可能会出现saddle问题，这样问题容易到达plateau状态，较难走出。这种可能性较小，通常使用Adam之类的优化方法解决</li>\n</ul>\n<h1 id=\"第三周-超参数调度、Batch正则与程序框架\"><a href=\"#第三周-超参数调度、Batch正则与程序框架\" class=\"headerlink\" title=\"第三周 超参数调度、Batch正则与程序框架\"></a>第三周 超参数调度、Batch正则与程序框架</h1><h2 id=\"超参数调整\"><a href=\"#超参数调整\" class=\"headerlink\" title=\"超参数调整\"></a>超参数调整</h2><ul>\n<li>首先是红色的 $\\alpha$，再者是黄色的学习率，第一、二动量及min-batch，最后再考虑调整层数及学习率延时 $\\varepsilon$。</li>\n<li>使用随机数、从粗到细</li>\n<li>使用合适的分布尺寸，而不是使用均匀分布，比如对于$\\alpha$可以使用a log scala，举例而言：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">r = <span class=\"number\">-4</span>*np.random.rand()</span><br><span class=\"line\">alpha = <span class=\"number\">10</span>^r</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"实践经验：两个流派\"><a href=\"#实践经验：两个流派\" class=\"headerlink\" title=\"实践经验：两个流派\"></a>实践经验：两个流派</h2><ul>\n<li>使用各领域的常见参数（<strong>这意味需要对各个智能领域有的了解</strong>）</li>\n<li>直觉很有效，<strong>定期更新参数，比如几个月</strong></li>\n<li>babysitting（pandan）: 长期照看，评估调整——（<strong>不断的持久化模型吗？，一直迭代下去，还是每次都重新训练，模型可以以直训练下去吗？</strong>）</li>\n<li>train in paraller: Caviar(鱼子酱)</li>\n</ul>\n<h2 id=\"网络内激活函数的normalizaion\"><a href=\"#网络内激活函数的normalizaion\" class=\"headerlink\" title=\"网络内激活函数的normalizaion\"></a>网络内激活函数的normalizaion</h2><p>请注意以下三点：</p>\n<ul>\n<li>通过 $\\alpha、\\beta$两个超参数，可以控制 $Z^i$ 的范围，如右上图所示</li>\n<li>单层网络中我们通过normalizaion来加快梯度下降的速度，那么也可以通过控制隐藏层的输入值来控制</li>\n<li>andrew的推荐，normalize z rather than a</li>\n</ul>\n<h2 id=\"将Batch-Norm拟合进神经网络\"><a href=\"#将Batch-Norm拟合进神经网络\" class=\"headerlink\" title=\"将Batch Norm拟合进神经网络\"></a>将Batch Norm拟合进神经网络</h2><ul>\n<li>偏执项b从此被平均，即式子可简化为 $z^l=w^l a^{l-1}$</li>\n</ul>\n<h2 id=\"Batch-Norm奏效的原因\"><a href=\"#Batch-Norm奏效的原因\" class=\"headerlink\" title=\"Batch Norm奏效的原因\"></a>Batch Norm奏效的原因</h2><ul>\n<li>加速GD</li>\n<li>It makes weights, later or deeper than your network, more robost to changes to weights in earlier layers of neural netowork.<br>举例而言，对于不带颜色的猫可能训练得很好，当带上颜色后，对于同样判断是否为猫的模型而言，输入数据的分布变化了，导致模型需要重新训练。这也称为<strong>covariate shift</strong>，为解决这个问题，可以使用Batch Norm。</li>\n<li>顺便的作用，如dropout，引入了噪声，相当于加入了regularization.</li>\n</ul>\n<h2 id=\"测试模型时BatchNorm的计算\"><a href=\"#测试模型时BatchNorm的计算\" class=\"headerlink\" title=\"测试模型时BatchNorm的计算\"></a>测试模型时BatchNorm的计算</h2><ul>\n<li>需要估算平均值及标准差</li>\n<li>简单靠谱的处理方式是用指数加权平均去<strong>估算</strong></li>\n<li>跑全量数据batchnorm也可以</li>\n</ul>\n<h2 id=\"softmax分类-LR的一般形式\"><a href=\"#softmax分类-LR的一般形式\" class=\"headerlink\" title=\"softmax分类:LR的一般形式\"></a>softmax分类:LR的一般形式</h2>"}],"PostAsset":[],"PostCategory":[{"post_id":"cjbdf8izj0008icn7un3zurq3","category_id":"cjbdf8izf0004icn7f17zty9r","_id":"cjbdf8izm000cicn7czw3zzfc"},{"post_id":"cjbdf8iz70000icn77a5j9k1h","category_id":"cjbdf8izf0004icn7f17zty9r","_id":"cjbdf8izo000ficn7bry6ihpp"},{"post_id":"cjbdf8izk0009icn7zetq7zg4","category_id":"cjbdf8izf0004icn7f17zty9r","_id":"cjbdf8izp000gicn7oja5qvuw"},{"post_id":"cjbdf8izc0002icn7fp0zjzj3","category_id":"cjbdf8izl000aicn7c665rfsh","_id":"cjbdf8izq000iicn7m9i8l5i1"},{"post_id":"cjbdf8izh0006icn775v5oi1p","category_id":"cjbdf8izf0004icn7f17zty9r","_id":"cjbdf8izr000kicn7qaepj2e5"}],"PostTag":[{"post_id":"cjbdf8iz70000icn77a5j9k1h","tag_id":"cjbdf8izh0005icn7rskky9i3","_id":"cjbdf8izq000jicn726hm62v6"},{"post_id":"cjbdf8iz70000icn77a5j9k1h","tag_id":"cjbdf8izl000bicn7owbr9qma","_id":"cjbdf8izr000licn7ovn68tqw"},{"post_id":"cjbdf8iz70000icn77a5j9k1h","tag_id":"cjbdf8izn000eicn7bqvnxpdg","_id":"cjbdf8izr000nicn7sn6zcr32"},{"post_id":"cjbdf8izc0002icn7fp0zjzj3","tag_id":"cjbdf8izq000hicn7dzd2zksx","_id":"cjbdf8izs000picn7dchoqhfp"},{"post_id":"cjbdf8izc0002icn7fp0zjzj3","tag_id":"cjbdf8izr000micn79sydkdq9","_id":"cjbdf8izs000qicn7ds2g1fqu"},{"post_id":"cjbdf8izh0006icn775v5oi1p","tag_id":"cjbdf8izs000oicn7tk65djx0","_id":"cjbdf8izu000ticn79svv0htq"},{"post_id":"cjbdf8izh0006icn775v5oi1p","tag_id":"cjbdf8izr000micn79sydkdq9","_id":"cjbdf8izv000uicn729qrjnvc"},{"post_id":"cjbdf8izj0008icn7un3zurq3","tag_id":"cjbdf8izt000sicn7f5tlzdqg","_id":"cjbdf8izx000yicn7py7bm5oq"},{"post_id":"cjbdf8izj0008icn7un3zurq3","tag_id":"cjbdf8izv000vicn7jpepu0kl","_id":"cjbdf8izy000zicn7byn38qbt"},{"post_id":"cjbdf8izj0008icn7un3zurq3","tag_id":"cjbdf8izn000eicn7bqvnxpdg","_id":"cjbdf8izy0011icn7v9uims5z"},{"post_id":"cjbdf8izk0009icn7zetq7zg4","tag_id":"cjbdf8izh0005icn7rskky9i3","_id":"cjbdf8izz0012icn7okgl2l4b"},{"post_id":"cjbdf8izk0009icn7zetq7zg4","tag_id":"cjbdf8izl000bicn7owbr9qma","_id":"cjbdf8izz0013icn7abwdag8u"},{"post_id":"cjbdf8izk0009icn7zetq7zg4","tag_id":"cjbdf8izn000eicn7bqvnxpdg","_id":"cjbdf8izz0014icn7muumsedx"}],"Tag":[{"name":"深度学习","_id":"cjbdf8izh0005icn7rskky9i3"},{"name":"Andrew NG","_id":"cjbdf8izl000bicn7owbr9qma"},{"name":"公开课","_id":"cjbdf8izn000eicn7bqvnxpdg"},{"name":"数据结构","_id":"cjbdf8izq000hicn7dzd2zksx"},{"name":"经典著作","_id":"cjbdf8izr000micn79sydkdq9"},{"name":"机器学习","_id":"cjbdf8izs000oicn7tk65djx0"},{"name":"神经网络","_id":"cjbdf8izt000sicn7f5tlzdqg"},{"name":"Geoffrey hinton","_id":"cjbdf8izv000vicn7jpepu0kl"}]}}