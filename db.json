{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1516280633000},{"_id":"source/CNAME","hash":"014beaa2b2313177e6b23b36f31252660427c565","modified":1512824761000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1512824956000},{"_id":"themes/next/.git","hash":"042ff34da0707513a5681580b37513c890c671ef","modified":1512824956000},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1512824956000},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1512824956000},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1512824956000},{"_id":"themes/next/.gitignore","hash":"0b5c2ffd41f66eb1849d6426ba8cf9649eeed329","modified":1512824956000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1512824956000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1512824956000},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1512824956000},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1512824956000},{"_id":"themes/next/README.md","hash":"529d53dfa97678f8ce4c95620b26e61154162a29","modified":1512824956000},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1512824956000},{"_id":"themes/next/_config.yml","hash":"8986866f11c460cb1e2d6844dd36035dd9e4f046","modified":1516366976000},{"_id":"themes/next/README.cn.md","hash":"02713071ef9e260b3fe77f4403942189d55a00e9","modified":1512824956000},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1512824956000},{"_id":"themes/next/bower.json","hash":"6d6ae7531cf3fedc97c58cdad664f5793eb3cc88","modified":1512824956000},{"_id":"themes/next/package.json","hash":"93a74dbc0fe3a1208a02e9cec3c15c2375339cc1","modified":1512824956000},{"_id":"source/about/index.md","hash":"9433482f87818b221e552b822e9fcd5876191868","modified":1512824761000},{"_id":"source/tags/index.md","hash":"047bc48ea113247ae48eece5212d62ccae459654","modified":1512824761000},{"_id":"source/_posts/Computer-System-A-Programmer-s-Perspective.md","hash":"fc300385a77e399f6c8b9a050195a3d66ba776c3","modified":1514378172000},{"_id":"source/_posts/Neural-Networks-for-Machine-Learning.md","hash":"46b4b4cc42b99a0000469b5f1f6ba126c6b30d9e","modified":1513605032000},{"_id":"source/_posts/Thinking-in-Java.md","hash":"f05d7bdf99ed8b60c817647964c7525bc7c60996","modified":1514378172000},{"_id":"source/_posts/draft.md","hash":"5431d1d432f5a1a6aac82d7d148d942191204e4f","modified":1514378921000},{"_id":"source/_posts/Practical-introduction-to-data-structures-and-algorithm-analysis.md","hash":"ecc7ae00e10114e880ea04b610b21af3c240c70b","modified":1513419233000},{"_id":"source/_posts/action-in-machine-learning.md","hash":"817e5afeedfcb658c8a05c088a798405ba0d52a8","modified":1513681951000},{"_id":"source/_posts/卷积神经网络.md","hash":"988993e464e41ef68d837e1c80280374aa0378c7","modified":1517146614000},{"_id":"source/_posts/神经网络与深度学习.md","hash":"f166dbdfab8a43e9388e8b6ce9279e8810bdd5ba","modified":1516456750000},{"_id":"source/_posts/改善深层神经网络：超参数调试、正则化以及优化.md","hash":"8520eec8a357d881e2e9ca3207a1deff4531c0d1","modified":1515407741000},{"_id":"source/_posts/集体智慧编程.md","hash":"16bc4669f333c7b22e8214ae0442ae0833f1fc2a","modified":1514378172000},{"_id":"source/_posts/结构化机器学习项目.md","hash":"5eb10db54c3049ecd7d9ed1f7628da8ef1b69a46","modified":1516021744000},{"_id":"source/categories/index.md","hash":"050f7dc3ea25c3fa162d9720fa885e69fdc2962e","modified":1512824761000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"352093a1b210c72136687fd2eee649244cee402c","modified":1512824956000},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"902f627155a65099e0a37842ff396a58d0dc306f","modified":1512824956000},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1512824956000},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1512824956000},{"_id":"themes/next/languages/de.yml","hash":"057e7df11ddeb1c8c15a5d7c5ff29430d725ec6b","modified":1512824956000},{"_id":"themes/next/languages/default.yml","hash":"44ef3f26917f467459326c2c8be2f73e4d947f35","modified":1512824956000},{"_id":"themes/next/languages/en.yml","hash":"7e680d9bb8f3a3a9d1ba1c9d312b3d257183dded","modified":1512824956000},{"_id":"themes/next/languages/fr-FR.yml","hash":"7e4eb7011b8feee641cfb11c6e73180b0ded1c0f","modified":1512824956000},{"_id":"themes/next/languages/id.yml","hash":"b5de1ea66dd9ef54cac9a1440eaa4e3f5fc011f5","modified":1512824956000},{"_id":"themes/next/languages/it.yml","hash":"aa595f2bda029f73ef7bfa104b4c55c3f4e9fb4c","modified":1512824956000},{"_id":"themes/next/languages/ja.yml","hash":"3c76e16fd19b262864475faa6854b718bc08c4d8","modified":1512824956000},{"_id":"themes/next/languages/pt-BR.yml","hash":"b1694ae766ed90277bcc4daca4b1cfa19cdcb72b","modified":1512824956000},{"_id":"themes/next/languages/ko.yml","hash":"ea5b46056e73ebcee121d5551627af35cbffc900","modified":1512824956000},{"_id":"themes/next/languages/nl-NL.yml","hash":"edca4f3598857dbc3cbf19ed412213329b6edd47","modified":1512824956000},{"_id":"themes/next/languages/pt.yml","hash":"44b61f2d085b827b507909a0b8f8ce31c6ef5d04","modified":1512824956000},{"_id":"themes/next/languages/vi.yml","hash":"fd08d3c8d2c62965a98ac420fdaf95e54c25d97c","modified":1512824956000},{"_id":"themes/next/languages/ru.yml","hash":"98ec6f0b7183282e11cffc7ff586ceb82400dd75","modified":1512824956000},{"_id":"themes/next/languages/zh-hk.yml","hash":"9396f41ae76e4fef99b257c93c7354e661f6e0fa","modified":1512824956000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"bab48ef972db736dedf338dade2afa605de18e7c","modified":1513518491000},{"_id":"themes/next/languages/zh-tw.yml","hash":"50b71abb3ecc0686f9739e179e2f829cd074ecd9","modified":1512824956000},{"_id":"themes/next/layout/_layout.swig","hash":"da0929166674ea637e0ad454f85ad0d7bac4aff2","modified":1512824956000},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1512824956000},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1512824956000},{"_id":"themes/next/layout/page.swig","hash":"969caaee05bdea725e99016eb63d810893a73e99","modified":1512824956000},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1512824956000},{"_id":"themes/next/scripts/merge-configs.js","hash":"cb617ddf692f56e6b6129564d52e302f50b28243","modified":1512824956000},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1512824956000},{"_id":"themes/next/layout/post.swig","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1512824956000},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1512824956000},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1512824956000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1512824956000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1512824956000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1512824956000},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512824956000},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1512824956000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1512824956000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1512824956000},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1512824956000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"9efc455894921a66bbc074055d3b39c8a34a48a4","modified":1512824956000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1512824956000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"56e8d8556cf474c56ae1bef9cb7bbd26554adb07","modified":1512824956000},{"_id":"themes/next/layout/_macro/post.swig","hash":"446a35a2cd389f8cfc3aa38973a9b44ad0740134","modified":1512824956000},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1512824956000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1512824956000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1512824956000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1512824956000},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1512824956000},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1512824956000},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1512824956000},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1512824956000},{"_id":"themes/next/scripts/tags/button.js","hash":"d023f10a00077f47082b0517e2ad666e6e994f60","modified":1512824956000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1512824956000},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1512824956000},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1512824956000},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"5fe0447cc88a5a63b530cf0426f93c4634811876","modified":1512824956000},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1512824956000},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1512824956000},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1512824956000},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1512824956000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1512824956000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1512824956000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"cf7615552bf1bec77e449e8e2cbbc764ef2518d3","modified":1516367568000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"a266f96ad06ee87bdeae6e105a4b53cd587bbd04","modified":1512824956000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4a6f5b1792b2e5262b7fdab9a716b3108e2f09c7","modified":1512824956000},{"_id":"themes/next/layout/_partials/head.swig","hash":"6b94fe8f3279daea5623c49ef4bb35917ba57510","modified":1512824956000},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1512824956000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1512824956000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1512824956000},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1512824956000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1512824956000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1512824956000},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1512824956000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1512824956000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1512824956000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1512824956000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1512824956000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1512824956000},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1512824956000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1512824956000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1512824956000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1512824956000},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1512824956000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1512824956000},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1512824956000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1512824956000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1512824956000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1512824956000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512824956000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512824956000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512824956000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512824956000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512824956000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512824956000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512824956000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1512824956000},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1512824956000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"f29165e36489a87ba32d17dddfd2720d84e3f3ec","modified":1512824956000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"29c261fa6b4046322559074d75239c6b272fb8a3","modified":1512824956000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1512824956000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1512824956000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1512824956000},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1512824956000},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1512824956000},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1512824956000},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1512824956000},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1512824956000},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1512824956000},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"1cd01c6e92ab1913d48e556a92bb4f28b6dc4996","modified":1512824956000},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1512824956000},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1512824956000},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1512824956000},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1512824956000},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1512824956000},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1512824956000},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1512824956000},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1512824956000},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1512824956000},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1512824956000},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1512824956000},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"10160daceaa6f1ecf632323d422ebe2caae49ddf","modified":1512824956000},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"aa0629277d751c55c6d973e7691bf84af9b17a60","modified":1512824956000},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1512824956000},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"fcabbb241f894c9a6309c44e126cf3e8fea81fd4","modified":1512824956000},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1512824956000},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1512824956000},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1512824956000},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1512824956000},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1512824956000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1512824956000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1512824956000},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1512824956000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1512824956000},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1512824956000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1512824956000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1512824956000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1512824956000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1512824956000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1512824956000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1512824956000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1512824956000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1512824956000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1512824956000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1512824956000},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1512824956000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1512824956000},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1512824956000},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1512824956000},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1512824956000},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1512824956000},{"_id":"themes/next/source/js/src/utils.js","hash":"dbdc3d1300eec7da9632608ebc0e5b697779dad7","modified":1512824956000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1512824956000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1512824956000},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1512824956000},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1512824956000},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1512824956000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1512824956000},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1512824956000},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1512824956000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1512824956000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1512824956000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1512824956000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1512824956000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1512824956000},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1512824956000},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1512824956000},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1512824956000},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1512824956000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1512824956000},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1512824956000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1512824956000},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1512824956000},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1512824956000},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1512824956000},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1512824956000},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1512824956000},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1512824956000},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1512824956000},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1512824956000},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1512824956000},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1512824956000},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1512824956000},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1512824956000},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1512824956000},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1512824956000},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1512824956000},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1512824956000},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1512824956000},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1512824956000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1512824956000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1512824956000},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1512824956000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1512824956000},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1512824956000},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1512824956000},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1512824956000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1512824956000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1512824956000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1512824956000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1512824956000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1512824956000},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1512824956000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1512824956000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"3b25edfa187d1bbbd0d38b50dd013cef54758abf","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"bcf52192942c0afc410c74a0fb458e7936ddc3d5","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"02fb8fa6b6c252b6bed469539cd057716606a787","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1512824956000},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1512824956000},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1512824956000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1512824956000},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1512824956000},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1512824956000},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1512824956000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1512824956000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1512824956000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1512824956000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1512824956000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1512824956000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1512824956000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1512824956000},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1512824956000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1512824956000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1512824956000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1512824956000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"77c92a449ce84d558d26d052681f2e0dd77c70c9","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1ccfbd4d0f5754b2dc2719a91245c95f547a7652","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"535b3b4f8cb1eec2558e094320e7dfb01f94c0e7","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1512824956000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"262debfd4442fa03d9919ceb88b948339df03fb0","modified":1512824956000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1512824956000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1512824956000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1512824956000},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1512824956000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1512824956000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1512824956000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1512824956000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1512824956000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1512824956000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1512824956000},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1512824956000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1512824956000},{"_id":"public/baidusitemap.xml","hash":"790fb381f9eb37c7a9c4c6b3593d15ae9be751df","modified":1517146734135},{"_id":"public/search.xml","hash":"91f75daf0e76bdaa2e547808887335ea07156113","modified":1517146734375},{"_id":"public/sitemap.xml","hash":"97e6eed9ebe9cfb82cefb3ae2e6c44348fcf166c","modified":1517146734376},{"_id":"public/about/index.html","hash":"5ec20f254dd41c23412670307d131c36a664ead0","modified":1516367996095},{"_id":"public/tags/index.html","hash":"1922075f14bafe3f30a25fbe9bcf2300932d2926","modified":1516367996098},{"_id":"public/categories/index.html","hash":"d0c8f58fd517e867601b3708850cafd19a3eeed7","modified":1516367996098},{"_id":"public/blog/卷积神经网络.html","hash":"25c488b93453bab439f36dd247054783665ab2b4","modified":1517146736775},{"_id":"public/blog/结构化机器学习项目.html","hash":"121ce796d5ea9d786f2fad948063d2ee58a014f0","modified":1516367996099},{"_id":"public/blog/Practical-introduction-to-data-structures-and-algorithm-analysis.html","hash":"06ca646a69a73e5deadbc93499f83d0ce34cb341","modified":1516367996099},{"_id":"public/blog/改善深层神经网络：超参数调试、正则化以及优化.html","hash":"4cbbfbd9492a334da509086e4ae74fffffcfd8f7","modified":1516367996099},{"_id":"public/blog/神经网络与深度学习.html","hash":"66d6843c258cbdd5607c9f6e6dec8bdb7aebc14a","modified":1516515536820},{"_id":"public/blog/action-in-machine-learning.html","hash":"685f230ba1df5214ba73942471d8b45dd972fca2","modified":1516367996099},{"_id":"public/blog/Computer-System-A-Programmer-s-Perspective.html","hash":"a5cc204e072fd95fa08c99eead31ff222e9710ca","modified":1516367996099},{"_id":"public/blog/集体智慧编程.html","hash":"9560202546261eb032c8478e247b310795ec6382","modified":1516367996099},{"_id":"public/blog/Thinking-in-Java.html","hash":"29ac00f0855428478bf5a2733d029c12b6fdd173","modified":1516367996099},{"_id":"public/blog/Neural-Networks-for-Machine-Learning.html","hash":"be58c128f156f31bd06e5a56d08d3f83c6b51cee","modified":1516367996099},{"_id":"public/blog/draft.html","hash":"895f1d9be4c08b909710b46a90c7e6e98d5df276","modified":1516367996100},{"_id":"public/archives/index.html","hash":"c1e9dd1a1551d196efb53c74fd52c15b1e11b9f3","modified":1516367996100},{"_id":"public/archives/page/2/index.html","hash":"f73455f8eefe4bcf1078156857c896fa703a9ab0","modified":1516367996100},{"_id":"public/archives/1900/index.html","hash":"a5b0db8642cdb5c3e65b22ea977dc7ca10b27a68","modified":1516367996100},{"_id":"public/archives/1900/12/index.html","hash":"b9ffdd9fe59a6a6e61d308e538c5668fefe27fc3","modified":1516367996100},{"_id":"public/archives/2016/index.html","hash":"2688026a7fa52a26d0195bda7182da22f74ed92c","modified":1516367996100},{"_id":"public/archives/2016/06/index.html","hash":"3f48e2544e63ec2f73358ab2924ea682fd026386","modified":1516367996100},{"_id":"public/archives/2017/index.html","hash":"b8a895b5aeed663d3fb65c50f04be3dae295c459","modified":1516367996100},{"_id":"public/archives/2017/04/index.html","hash":"b7a417eb0a71dd525dca8c94c77353cae6a55b12","modified":1516367996101},{"_id":"public/archives/2017/07/index.html","hash":"463512a9ba3f6f8844e0f060f2c699cb86fed657","modified":1516367996101},{"_id":"public/archives/2017/08/index.html","hash":"db7ab18f39566d4d0fd188557db0f32e07e91476","modified":1516367996101},{"_id":"public/archives/2017/09/index.html","hash":"7c8fa6efda27161deccc95b62f19491fac720a82","modified":1516367996101},{"_id":"public/archives/2018/index.html","hash":"2b81066504995128a309ba2c183f674fd9776516","modified":1516367996101},{"_id":"public/archives/2018/01/index.html","hash":"9ba5f59eb9d2a0d79d4bc2c28fb5c7ecea57dc5e","modified":1516367996101},{"_id":"public/categories/深耕码农/index.html","hash":"acb7c7ef56985447870fb01834592f3249c92f14","modified":1516367996100},{"_id":"public/categories/AI梦/index.html","hash":"21e8ad22e67180d7a30d4d8035321136dcd53bde","modified":1516367996100},{"_id":"public/tags/系统结构/index.html","hash":"151776fc40b1b949652382f561aadeb3f1031c9e","modified":1516367996101},{"_id":"public/tags/经典著作/index.html","hash":"629e4bf34ae3549949f2fdcd70dba72c9613b46c","modified":1516367996101},{"_id":"public/tags/神经网络/index.html","hash":"ded55d14fc0243ccd8c9c37490046fe884312d2c","modified":1516367996102},{"_id":"public/tags/Geoffrey-hinton/index.html","hash":"078ff0d4d4817b7b6dbed14d69eb4ae8609b72df","modified":1516367996102},{"_id":"public/tags/公开课/index.html","hash":"f824b9489b715c7e6b0d44c283fb28e050a9c295","modified":1516367996102},{"_id":"public/tags/计算机语言/index.html","hash":"b44924096fd1758bd8492b2750a748a2c03eaac0","modified":1516367996105},{"_id":"public/tags/draft/index.html","hash":"0c35a678cb055c219bbc9ff5af4e8039eae44bc2","modified":1516367996106},{"_id":"public/tags/数据结构/index.html","hash":"0c8ed4ed58205466d9c80e0a86180361ae53fcfe","modified":1516367996106},{"_id":"public/tags/机器学习/index.html","hash":"19997e9d30a5f38028e45f6635b7d6941f86e519","modified":1516367996106},{"_id":"public/tags/深度学习/index.html","hash":"302977bd4a2939457dc30c2591973e43551c7e6b","modified":1516367996106},{"_id":"public/tags/Andrew-NG/index.html","hash":"c6e1b930f3928905a6eb8447fdd44d0cebe84adc","modified":1516367996106},{"_id":"public/index.html","hash":"2f287032b574ac343474009f5ea3810840241f0d","modified":1517146736775},{"_id":"public/page/2/index.html","hash":"7a9f6b4da7ea3b18fc5eecf6cd0abbd4dfb031b3","modified":1516367996101},{"_id":"public/CNAME","hash":"014beaa2b2313177e6b23b36f31252660427c565","modified":1516367039730},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1516367039730},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1516367039731},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1516367039731},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1516367039731},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1516367039731},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1516367039731},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1516367039731},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1516367039731},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1516367039731},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1516367039731},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1516367039731},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1516367039731},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1516367039732},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1516367039732},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1516367039732},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1516367039732},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1516367039732},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1516367039732},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1516367039732},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1516367039732},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1516367039732},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1516367039732},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1516367039732},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1516367039732},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1516367039732},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1516367039733},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1516367039733},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1516367039733},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1516367039733},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1516367039733},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1516367039733},{"_id":"public/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1516367039733},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1516367039733},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1516367040744},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1516367040752},{"_id":"public/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1516367040773},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1516367040773},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1516367040773},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1516367040773},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1516367040774},{"_id":"public/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1516367040774},{"_id":"public/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1516367040774},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1516367040774},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1516367040774},{"_id":"public/js/src/utils.js","hash":"dbdc3d1300eec7da9632608ebc0e5b697779dad7","modified":1516367040774},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1516367040774},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1516367040774},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1516367040774},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1516367040774},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1516367040774},{"_id":"public/lib/fastclick/README.html","hash":"d6e90449a2c09f3033f7e43d68b0cc8208e22e09","modified":1516367040774},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1516367040774},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"06811ca2f722dead021493457f27cdc264ef928d","modified":1516367040775},{"_id":"public/lib/jquery_lazyload/README.html","hash":"a08fccd381c8fdb70ba8974b208254c5ba23a95f","modified":1516367040775},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1516367040775},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1516367040775},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1516367040775},{"_id":"public/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1516367040775},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1516367040775},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1516367040775},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1516367040775},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1516367040775},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1516367040775},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1516367040775},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1516367040775},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1516367040775},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1516367040775},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1516367040775},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1516367040775},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1516367040776},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1516367040776},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1516367040776},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1516367040776},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1516367040776},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1516367040776},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1516367040776},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1516367040776},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1516367040776},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1516367040776},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1516367040776},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1516367040776},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1516367040776},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1516367040776},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1516367040776},{"_id":"public/css/main.css","hash":"0dd3ad739c23917c0177c93d7d0977e9f629bd6e","modified":1516367040776},{"_id":"public/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1516367040777},{"_id":"public/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1516367040777},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1516367040777},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1516367040777},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1516367040777},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1516367040777},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1516367040777},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1516367040777},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1516367040777},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1516367040777},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1516367040777},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1516367040777},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1516367040777},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1516367040777},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1516367040778},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1516367040778},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1516367040778},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1516367040778},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1516367040778},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1516367040778},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1516367040778},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1516367040779},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1516367040779},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1516367040846}],"Category":[{"name":"深耕码农","_id":"cjclxs5h60004ajslm15bu3oe"},{"name":"AI梦","_id":"cjclxs5hf000aajslq2jdguop"}],"Data":[],"Page":[{"title":"about","date":"2017-12-06T09:20:41.000Z","_content":"\n\n## 关于我\n\n\nQQ：563593589\nEmail: zw_kprs@126.com","source":"about/index.md","raw":"---\ntitle: about\ndate: 2017-12-06 17:20:41\n---\n\n\n## 关于我\n\n\nQQ：563593589\nEmail: zw_kprs@126.com","updated":"2017-12-09T13:06:01.000Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjclxs5gv0000ajslrb8o23ng","content":"<h2 id=\"关于我\"><a href=\"#关于我\" class=\"headerlink\" title=\"关于我\"></a>关于我</h2><p>QQ：563593589<br>Email: zw_kprs@126.com</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"关于我\"><a href=\"#关于我\" class=\"headerlink\" title=\"关于我\"></a>关于我</h2><p>QQ：563593589<br>Email: zw_kprs@126.com</p>\n"},{"title":"tags","date":"2017-12-06T09:18:31.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2017-12-06 17:18:31\ntype: \"tags\"\n---\n","updated":"2017-12-09T13:06:01.000Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjclxs5h10002ajsl4vvrl6om","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2017-12-06T09:19:28.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2017-12-06 17:19:28\ntype: \"categories\"\n---\n","updated":"2017-12-09T13:06:01.000Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjclxs5ha0006ajslxkrr1n44","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Computer System, A Programmer's Perspective","date":"2017-08-21T06:22:46.000Z","mathjax":true,"_content":"\n\n# 导言\n1. 信息就是位+上下文，如文本文件、二进制文件；\n2. 程序：文本——预处理器——编译器——汇编器——链接器——可执行文件\n  ![CS_compile](http://p15i7i801.bkt.clouddn.com/d8403afcfd487487262309d0137d406f.png)\n\n从JAVA的角度上看，JAVACODE在JVM上亦符合上述过程。\nJVM这个代码级机器可参考百度百科，包括对JVM指令系统、寄存器、栈结构及子系统（GC、类加载器）等\n![JVM](http://p15i7i801.bkt.clouddn.com/8c5d39071eb21c9f569e9e46119377d3.png)\n![cs_jvm_stack](http://p15i7i801.bkt.clouddn.com/32f2e3db6434701a8206f6ce128bf139.png)\n\n**基本上，一个线程启动后分配一个栈（-XSSS），保存局部变量、操作数、指向常量池的引用及返回地址，这些称为一个桢，进入新方法后，再压入一个帧；同时分配一个私有PC寄存器用于任何分支，循环，方法调用，判断，异常处理，线程等待以及恢复线程，递归等等； 所有的线程共享方法区（PERM)及堆**\n","source":"_posts/Computer-System-A-Programmer-s-Perspective.md","raw":"---\ntitle: 'Computer System, A Programmer''s Perspective'\ndate: 2017-08-21 14:22:46\ntags:\n      - 系统结构\n      - 经典著作\ncategories: 深耕码农\nmathjax: true\n---\n\n\n# 导言\n1. 信息就是位+上下文，如文本文件、二进制文件；\n2. 程序：文本——预处理器——编译器——汇编器——链接器——可执行文件\n  ![CS_compile](http://p15i7i801.bkt.clouddn.com/d8403afcfd487487262309d0137d406f.png)\n\n从JAVA的角度上看，JAVACODE在JVM上亦符合上述过程。\nJVM这个代码级机器可参考百度百科，包括对JVM指令系统、寄存器、栈结构及子系统（GC、类加载器）等\n![JVM](http://p15i7i801.bkt.clouddn.com/8c5d39071eb21c9f569e9e46119377d3.png)\n![cs_jvm_stack](http://p15i7i801.bkt.clouddn.com/32f2e3db6434701a8206f6ce128bf139.png)\n\n**基本上，一个线程启动后分配一个栈（-XSSS），保存局部变量、操作数、指向常量池的引用及返回地址，这些称为一个桢，进入新方法后，再压入一个帧；同时分配一个私有PC寄存器用于任何分支，循环，方法调用，判断，异常处理，线程等待以及恢复线程，递归等等； 所有的线程共享方法区（PERM)及堆**\n","slug":"Computer-System-A-Programmer-s-Perspective","published":1,"updated":"2017-12-27T12:36:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjclxs5gx0001ajslt1hr2gn2","content":"<h1 id=\"导言\"><a href=\"#导言\" class=\"headerlink\" title=\"导言\"></a>导言</h1><ol>\n<li>信息就是位+上下文，如文本文件、二进制文件；</li>\n<li>程序：文本——预处理器——编译器——汇编器——链接器——可执行文件<br><img src=\"http://p15i7i801.bkt.clouddn.com/d8403afcfd487487262309d0137d406f.png\" alt=\"CS_compile\"></li>\n</ol>\n<p>从JAVA的角度上看，JAVACODE在JVM上亦符合上述过程。<br>JVM这个代码级机器可参考百度百科，包括对JVM指令系统、寄存器、栈结构及子系统（GC、类加载器）等<br><img src=\"http://p15i7i801.bkt.clouddn.com/8c5d39071eb21c9f569e9e46119377d3.png\" alt=\"JVM\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/32f2e3db6434701a8206f6ce128bf139.png\" alt=\"cs_jvm_stack\"></p>\n<p><strong>基本上，一个线程启动后分配一个栈（-XSSS），保存局部变量、操作数、指向常量池的引用及返回地址，这些称为一个桢，进入新方法后，再压入一个帧；同时分配一个私有PC寄存器用于任何分支，循环，方法调用，判断，异常处理，线程等待以及恢复线程，递归等等； 所有的线程共享方法区（PERM)及堆</strong></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"导言\"><a href=\"#导言\" class=\"headerlink\" title=\"导言\"></a>导言</h1><ol>\n<li>信息就是位+上下文，如文本文件、二进制文件；</li>\n<li>程序：文本——预处理器——编译器——汇编器——链接器——可执行文件<br><img src=\"http://p15i7i801.bkt.clouddn.com/d8403afcfd487487262309d0137d406f.png\" alt=\"CS_compile\"></li>\n</ol>\n<p>从JAVA的角度上看，JAVACODE在JVM上亦符合上述过程。<br>JVM这个代码级机器可参考百度百科，包括对JVM指令系统、寄存器、栈结构及子系统（GC、类加载器）等<br><img src=\"http://p15i7i801.bkt.clouddn.com/8c5d39071eb21c9f569e9e46119377d3.png\" alt=\"JVM\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/32f2e3db6434701a8206f6ce128bf139.png\" alt=\"cs_jvm_stack\"></p>\n<p><strong>基本上，一个线程启动后分配一个栈（-XSSS），保存局部变量、操作数、指向常量池的引用及返回地址，这些称为一个桢，进入新方法后，再压入一个帧；同时分配一个私有PC寄存器用于任何分支，循环，方法调用，判断，异常处理，线程等待以及恢复线程，递归等等； 所有的线程共享方法区（PERM)及堆</strong></p>\n"},{"title":"Neural Networks for Machine Learning","date":"2016-06-07T05:26:48.000Z","mathjax":true,"_content":"\n\n\n# 机器学习：不必为特定任务编写程序\n## 场景落地\n- Recognizing pattern     \n  - Objects    \n  - facial    \n  - Spoken word\n- Recognizing anomalies    \n  - credit card transaction    \n  - Sensor reality\n- Prediction    \n  - stock price,\n  - exchange rate   \n  - movie like\n\n##几种简单模型\n- Linear neurons\n$ y=b+\\sum x_iw_i $\n- Binary threshold neurons\n$$\nz=\\Sigma x_i w_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt 0\\\\\n0&otherwise\n\\end{cases} \\quad or \\qquad\n\\theta=-b \\quad\nz=b+\\Sigma x_i y_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt \\theta\\\\\n0&otherwise\n\\end{cases}\n$$\n- Rectified Linear Neurons 2\n$$z=\\Sigma x_i w_i, y=\n\\begin{cases}\nz,\t&if\\;z \\gt 0 \\\\\n0,\t&\\mbox{otherwise}\n\\end{cases}$$\n- sigmoid neurons\n$$ z=b+\\Sigma x_i w_i  \\quad y=\\frac {1} {1+e^{-z}} $$\n![nnml-sigmoid](http://p15i7i801.bkt.clouddn.com/86069ad24cfd3541f4b33070b67fd749.png)\n- Stochastic binary neurons\n$$z=b+\\Sigma x_i w_i  \\quad p(s=1)=\\frac {1} {1+e^{-z}}$$\n\n## 学习的类型\n- supervised learning\n  * Regression\n    * model class: $y=f(x;w)$\n    * 1/2这个系数在求导时被抵消\n![nnml-regression](http://p15i7i801.bkt.clouddn.com/93512d5be2aafca06ff3d058aff511b7.png)\n  * classification\n- reinforced learning\n\n  the output is an action or sequence of actions and the only supervisory is an occasional scalar reward._This is difficult_, The rewads are typically delayed so its had to know where we went wrong( or right)\n- unsupervised learning\n\n  ![nnml-unsupervised](http://p15i7i801.bkt.clouddn.com/ada9c5e7fe1e06957b0797be4022d724.png)\n## 神经网络的类型\n- Feed-Forward neural network\n\n  ![nnml-feedforward](http://p15i7i801.bkt.clouddn.com/3053d06760561e6169f20829df15a358.png)\n\n  if there is more than one hidden layer, we call them \"deep\" neural networks.\n\n- Recurrent network\n\n  It is hard to train but natural way for modeling sequence, recent algorithm can be able to do this, this also can be used to predict stock price.\n\n  ![nnml-recurrent](http://p15i7i801.bkt.clouddn.com/83abfe6f730b3ae61d9099ff0d5ff300.png)\n![nnml-rnn](http://p15i7i801.bkt.clouddn.com/3a688df70ed6bf1e46d62a03402e2a39.png)\n> 20171218 LSTM 现在貌似更流行\n\n- Symmetrically connected networks\n\n## 感知器\n![nnml-perceptron-arch](http://p15i7i801.bkt.clouddn.com/a1ba9669ccb220d05a8140bc8e0ca797.png)\n> how to learn biases using the same rule as we use for learning weights\n\n  ![nnml-handlebias](http://p15i7i801.bkt.clouddn.com/d73047067e05f31a249dc755851602aa.png)\n\n> 学习的过程：if the output unit is correct, leave its weights alone, if the output unit incorrectly output\n\n![nnml-per-train](http://p15i7i801.bkt.clouddn.com/6b72ab6f556908380fcd6d5561d2e650.png)\n  - __错误的输出了零，则Z需要新增以致其逐渐大于零而产生y=1（往正面），反之则需要减__ ？？此偏移向量为负如何解释，或者必须保证 该向量是指向正确的分类超平面的（perpendicular，正交）。\n  - $(w,b) \\leftarrow (w,b)-(x,1)$\n    $(w,b) \\leftarrow (w,b)+(x,1)$\n![nnml-adpateside](http://p15i7i801.bkt.clouddn.com/b109e1d1e4424d180575bb0f74e75906.png)\n  - *just like penalty function*\n\n> 为什么学习有效: _非正式的收敛证明_\n\n  1. Each time the perceptron makes a mistake, the current weight vector moves to decrease its squared distance from every weight vector in the \"generously feasible\" region.\n  2. <strong> The squared distance decreases by at least the squared length of the input vector. </strong>（向量的运算，几何解释）\n  3. So after a finite number of mistakes, the weight vector must lie in the feasible region if the region exist.\n\n## 感知器的缺陷\n> **right feature**\n\n  Once the hand-coded features have been determined, there are very strong limitations on what a perceptron can learn.\n  _The main question is : This type of table look-up won't generalize, it need too many feature_\n  _此篇习题未理解_\n\n> Group Invariance Theorem---Minsky and Papert\n\ncan't learn to do this if the transformations from a group{欧氏变换之内}，so it needs to use multiple feature unit to recognize transformations of informative sub-patterns.\nSo,more  important was all about how you learn **feature detectors, that's hidden units**. After 20 years, We know,\nwe need multiple layer of adaptive, non-linear hidden units.\n","source":"_posts/Neural-Networks-for-Machine-Learning.md","raw":"---\ntitle: Neural Networks for Machine Learning\ndate: 2016-06-07 13:26:48\ntags:\n      - 神经网络\n      - Geoffrey hinton\n      - 公开课\ncategories: AI梦\nmathjax: true\n---\n\n\n\n# 机器学习：不必为特定任务编写程序\n## 场景落地\n- Recognizing pattern     \n  - Objects    \n  - facial    \n  - Spoken word\n- Recognizing anomalies    \n  - credit card transaction    \n  - Sensor reality\n- Prediction    \n  - stock price,\n  - exchange rate   \n  - movie like\n\n##几种简单模型\n- Linear neurons\n$ y=b+\\sum x_iw_i $\n- Binary threshold neurons\n$$\nz=\\Sigma x_i w_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt 0\\\\\n0&otherwise\n\\end{cases} \\quad or \\qquad\n\\theta=-b \\quad\nz=b+\\Sigma x_i y_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt \\theta\\\\\n0&otherwise\n\\end{cases}\n$$\n- Rectified Linear Neurons 2\n$$z=\\Sigma x_i w_i, y=\n\\begin{cases}\nz,\t&if\\;z \\gt 0 \\\\\n0,\t&\\mbox{otherwise}\n\\end{cases}$$\n- sigmoid neurons\n$$ z=b+\\Sigma x_i w_i  \\quad y=\\frac {1} {1+e^{-z}} $$\n![nnml-sigmoid](http://p15i7i801.bkt.clouddn.com/86069ad24cfd3541f4b33070b67fd749.png)\n- Stochastic binary neurons\n$$z=b+\\Sigma x_i w_i  \\quad p(s=1)=\\frac {1} {1+e^{-z}}$$\n\n## 学习的类型\n- supervised learning\n  * Regression\n    * model class: $y=f(x;w)$\n    * 1/2这个系数在求导时被抵消\n![nnml-regression](http://p15i7i801.bkt.clouddn.com/93512d5be2aafca06ff3d058aff511b7.png)\n  * classification\n- reinforced learning\n\n  the output is an action or sequence of actions and the only supervisory is an occasional scalar reward._This is difficult_, The rewads are typically delayed so its had to know where we went wrong( or right)\n- unsupervised learning\n\n  ![nnml-unsupervised](http://p15i7i801.bkt.clouddn.com/ada9c5e7fe1e06957b0797be4022d724.png)\n## 神经网络的类型\n- Feed-Forward neural network\n\n  ![nnml-feedforward](http://p15i7i801.bkt.clouddn.com/3053d06760561e6169f20829df15a358.png)\n\n  if there is more than one hidden layer, we call them \"deep\" neural networks.\n\n- Recurrent network\n\n  It is hard to train but natural way for modeling sequence, recent algorithm can be able to do this, this also can be used to predict stock price.\n\n  ![nnml-recurrent](http://p15i7i801.bkt.clouddn.com/83abfe6f730b3ae61d9099ff0d5ff300.png)\n![nnml-rnn](http://p15i7i801.bkt.clouddn.com/3a688df70ed6bf1e46d62a03402e2a39.png)\n> 20171218 LSTM 现在貌似更流行\n\n- Symmetrically connected networks\n\n## 感知器\n![nnml-perceptron-arch](http://p15i7i801.bkt.clouddn.com/a1ba9669ccb220d05a8140bc8e0ca797.png)\n> how to learn biases using the same rule as we use for learning weights\n\n  ![nnml-handlebias](http://p15i7i801.bkt.clouddn.com/d73047067e05f31a249dc755851602aa.png)\n\n> 学习的过程：if the output unit is correct, leave its weights alone, if the output unit incorrectly output\n\n![nnml-per-train](http://p15i7i801.bkt.clouddn.com/6b72ab6f556908380fcd6d5561d2e650.png)\n  - __错误的输出了零，则Z需要新增以致其逐渐大于零而产生y=1（往正面），反之则需要减__ ？？此偏移向量为负如何解释，或者必须保证 该向量是指向正确的分类超平面的（perpendicular，正交）。\n  - $(w,b) \\leftarrow (w,b)-(x,1)$\n    $(w,b) \\leftarrow (w,b)+(x,1)$\n![nnml-adpateside](http://p15i7i801.bkt.clouddn.com/b109e1d1e4424d180575bb0f74e75906.png)\n  - *just like penalty function*\n\n> 为什么学习有效: _非正式的收敛证明_\n\n  1. Each time the perceptron makes a mistake, the current weight vector moves to decrease its squared distance from every weight vector in the \"generously feasible\" region.\n  2. <strong> The squared distance decreases by at least the squared length of the input vector. </strong>（向量的运算，几何解释）\n  3. So after a finite number of mistakes, the weight vector must lie in the feasible region if the region exist.\n\n## 感知器的缺陷\n> **right feature**\n\n  Once the hand-coded features have been determined, there are very strong limitations on what a perceptron can learn.\n  _The main question is : This type of table look-up won't generalize, it need too many feature_\n  _此篇习题未理解_\n\n> Group Invariance Theorem---Minsky and Papert\n\ncan't learn to do this if the transformations from a group{欧氏变换之内}，so it needs to use multiple feature unit to recognize transformations of informative sub-patterns.\nSo,more  important was all about how you learn **feature detectors, that's hidden units**. After 20 years, We know,\nwe need multiple layer of adaptive, non-linear hidden units.\n","slug":"Neural-Networks-for-Machine-Learning","published":1,"updated":"2017-12-18T13:50:32.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjclxs5h20003ajslnytlim7t","content":"<h1 id=\"机器学习：不必为特定任务编写程序\"><a href=\"#机器学习：不必为特定任务编写程序\" class=\"headerlink\" title=\"机器学习：不必为特定任务编写程序\"></a>机器学习：不必为特定任务编写程序</h1><h2 id=\"场景落地\"><a href=\"#场景落地\" class=\"headerlink\" title=\"场景落地\"></a>场景落地</h2><ul>\n<li>Recognizing pattern     <ul>\n<li>Objects    </li>\n<li>facial    </li>\n<li>Spoken word</li>\n</ul>\n</li>\n<li>Recognizing anomalies    <ul>\n<li>credit card transaction    </li>\n<li>Sensor reality</li>\n</ul>\n</li>\n<li>Prediction    <ul>\n<li>stock price,</li>\n<li>exchange rate   </li>\n<li>movie like</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"几种简单模型\"><a href=\"#几种简单模型\" class=\"headerlink\" title=\"几种简单模型\"></a>几种简单模型</h2><ul>\n<li>Linear neurons<br>$ y=b+\\sum x_iw_i $</li>\n<li>Binary threshold neurons<script type=\"math/tex; mode=display\">\nz=\\Sigma x_i w_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt 0\\\\\n0&otherwise\n\\end{cases} \\quad or \\qquad\n\\theta=-b \\quad\nz=b+\\Sigma x_i y_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt \\theta\\\\\n0&otherwise\n\\end{cases}</script></li>\n<li>Rectified Linear Neurons 2<script type=\"math/tex; mode=display\">z=\\Sigma x_i w_i, y=\n\\begin{cases}\nz,    &if\\;z \\gt 0 \\\\\n0,    &\\mbox{otherwise}\n\\end{cases}</script></li>\n<li>sigmoid neurons<script type=\"math/tex; mode=display\">z=b+\\Sigma x_i w_i  \\quad y=\\frac {1} {1+e^{-z}}</script><img src=\"http://p15i7i801.bkt.clouddn.com/86069ad24cfd3541f4b33070b67fd749.png\" alt=\"nnml-sigmoid\"></li>\n<li>Stochastic binary neurons<script type=\"math/tex; mode=display\">z=b+\\Sigma x_i w_i  \\quad p(s=1)=\\frac {1} {1+e^{-z}}</script></li>\n</ul>\n<h2 id=\"学习的类型\"><a href=\"#学习的类型\" class=\"headerlink\" title=\"学习的类型\"></a>学习的类型</h2><ul>\n<li>supervised learning<ul>\n<li>Regression<ul>\n<li>model class: $y=f(x;w)$</li>\n<li>1/2这个系数在求导时被抵消<br><img src=\"http://p15i7i801.bkt.clouddn.com/93512d5be2aafca06ff3d058aff511b7.png\" alt=\"nnml-regression\"></li>\n</ul>\n</li>\n<li>classification</li>\n</ul>\n</li>\n<li><p>reinforced learning</p>\n<p>the output is an action or sequence of actions and the only supervisory is an occasional scalar reward._This is difficult_, The rewads are typically delayed so its had to know where we went wrong( or right)</p>\n</li>\n<li><p>unsupervised learning</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/ada9c5e7fe1e06957b0797be4022d724.png\" alt=\"nnml-unsupervised\"></p>\n<h2 id=\"神经网络的类型\"><a href=\"#神经网络的类型\" class=\"headerlink\" title=\"神经网络的类型\"></a>神经网络的类型</h2></li>\n<li><p>Feed-Forward neural network</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/3053d06760561e6169f20829df15a358.png\" alt=\"nnml-feedforward\"></p>\n<p>if there is more than one hidden layer, we call them “deep” neural networks.</p>\n</li>\n<li><p>Recurrent network</p>\n<p>It is hard to train but natural way for modeling sequence, recent algorithm can be able to do this, this also can be used to predict stock price.</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/83abfe6f730b3ae61d9099ff0d5ff300.png\" alt=\"nnml-recurrent\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/3a688df70ed6bf1e46d62a03402e2a39.png\" alt=\"nnml-rnn\"></p>\n<blockquote>\n<p>20171218 LSTM 现在貌似更流行</p>\n</blockquote>\n</li>\n<li><p>Symmetrically connected networks</p>\n</li>\n</ul>\n<h2 id=\"感知器\"><a href=\"#感知器\" class=\"headerlink\" title=\"感知器\"></a>感知器</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/a1ba9669ccb220d05a8140bc8e0ca797.png\" alt=\"nnml-perceptron-arch\"></p>\n<blockquote>\n<p>how to learn biases using the same rule as we use for learning weights</p>\n</blockquote>\n<p>  <img src=\"http://p15i7i801.bkt.clouddn.com/d73047067e05f31a249dc755851602aa.png\" alt=\"nnml-handlebias\"></p>\n<blockquote>\n<p>学习的过程：if the output unit is correct, leave its weights alone, if the output unit incorrectly output</p>\n</blockquote>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/6b72ab6f556908380fcd6d5561d2e650.png\" alt=\"nnml-per-train\"></p>\n<ul>\n<li><strong>错误的输出了零，则Z需要新增以致其逐渐大于零而产生y=1（往正面），反之则需要减</strong> ？？此偏移向量为负如何解释，或者必须保证 该向量是指向正确的分类超平面的（perpendicular，正交）。</li>\n<li>$(w,b) \\leftarrow (w,b)-(x,1)$<br>$(w,b) \\leftarrow (w,b)+(x,1)$<br><img src=\"http://p15i7i801.bkt.clouddn.com/b109e1d1e4424d180575bb0f74e75906.png\" alt=\"nnml-adpateside\"></li>\n<li><em>just like penalty function</em></li>\n</ul>\n<blockquote>\n<p>为什么学习有效: _非正式的收敛证明_</p>\n</blockquote>\n<ol>\n<li>Each time the perceptron makes a mistake, the current weight vector moves to decrease its squared distance from every weight vector in the “generously feasible” region.</li>\n<li><strong> The squared distance decreases by at least the squared length of the input vector. </strong>（向量的运算，几何解释）</li>\n<li>So after a finite number of mistakes, the weight vector must lie in the feasible region if the region exist.</li>\n</ol>\n<h2 id=\"感知器的缺陷\"><a href=\"#感知器的缺陷\" class=\"headerlink\" title=\"感知器的缺陷\"></a>感知器的缺陷</h2><blockquote>\n<p><strong>right feature</strong></p>\n</blockquote>\n<p>  Once the hand-coded features have been determined, there are very strong limitations on what a perceptron can learn.<br>  _The main question is : This type of table look-up won’t generalize, it need too many feature_<br>  _此篇习题未理解_</p>\n<blockquote>\n<p>Group Invariance Theorem—-Minsky and Papert</p>\n</blockquote>\n<p>can’t learn to do this if the transformations from a group{欧氏变换之内}，so it needs to use multiple feature unit to recognize transformations of informative sub-patterns.<br>So,more  important was all about how you learn <strong>feature detectors, that’s hidden units</strong>. After 20 years, We know,<br>we need multiple layer of adaptive, non-linear hidden units.</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"机器学习：不必为特定任务编写程序\"><a href=\"#机器学习：不必为特定任务编写程序\" class=\"headerlink\" title=\"机器学习：不必为特定任务编写程序\"></a>机器学习：不必为特定任务编写程序</h1><h2 id=\"场景落地\"><a href=\"#场景落地\" class=\"headerlink\" title=\"场景落地\"></a>场景落地</h2><ul>\n<li>Recognizing pattern     <ul>\n<li>Objects    </li>\n<li>facial    </li>\n<li>Spoken word</li>\n</ul>\n</li>\n<li>Recognizing anomalies    <ul>\n<li>credit card transaction    </li>\n<li>Sensor reality</li>\n</ul>\n</li>\n<li>Prediction    <ul>\n<li>stock price,</li>\n<li>exchange rate   </li>\n<li>movie like</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"几种简单模型\"><a href=\"#几种简单模型\" class=\"headerlink\" title=\"几种简单模型\"></a>几种简单模型</h2><ul>\n<li>Linear neurons<br>$ y=b+\\sum x_iw_i $</li>\n<li>Binary threshold neurons<script type=\"math/tex; mode=display\">\nz=\\Sigma x_i w_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt 0\\\\\n0&otherwise\n\\end{cases} \\quad or \\qquad\n\\theta=-b \\quad\nz=b+\\Sigma x_i y_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt \\theta\\\\\n0&otherwise\n\\end{cases}</script></li>\n<li>Rectified Linear Neurons 2<script type=\"math/tex; mode=display\">z=\\Sigma x_i w_i, y=\n\\begin{cases}\nz,    &if\\;z \\gt 0 \\\\\n0,    &\\mbox{otherwise}\n\\end{cases}</script></li>\n<li>sigmoid neurons<script type=\"math/tex; mode=display\">z=b+\\Sigma x_i w_i  \\quad y=\\frac {1} {1+e^{-z}}</script><img src=\"http://p15i7i801.bkt.clouddn.com/86069ad24cfd3541f4b33070b67fd749.png\" alt=\"nnml-sigmoid\"></li>\n<li>Stochastic binary neurons<script type=\"math/tex; mode=display\">z=b+\\Sigma x_i w_i  \\quad p(s=1)=\\frac {1} {1+e^{-z}}</script></li>\n</ul>\n<h2 id=\"学习的类型\"><a href=\"#学习的类型\" class=\"headerlink\" title=\"学习的类型\"></a>学习的类型</h2><ul>\n<li>supervised learning<ul>\n<li>Regression<ul>\n<li>model class: $y=f(x;w)$</li>\n<li>1/2这个系数在求导时被抵消<br><img src=\"http://p15i7i801.bkt.clouddn.com/93512d5be2aafca06ff3d058aff511b7.png\" alt=\"nnml-regression\"></li>\n</ul>\n</li>\n<li>classification</li>\n</ul>\n</li>\n<li><p>reinforced learning</p>\n<p>the output is an action or sequence of actions and the only supervisory is an occasional scalar reward._This is difficult_, The rewads are typically delayed so its had to know where we went wrong( or right)</p>\n</li>\n<li><p>unsupervised learning</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/ada9c5e7fe1e06957b0797be4022d724.png\" alt=\"nnml-unsupervised\"></p>\n<h2 id=\"神经网络的类型\"><a href=\"#神经网络的类型\" class=\"headerlink\" title=\"神经网络的类型\"></a>神经网络的类型</h2></li>\n<li><p>Feed-Forward neural network</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/3053d06760561e6169f20829df15a358.png\" alt=\"nnml-feedforward\"></p>\n<p>if there is more than one hidden layer, we call them “deep” neural networks.</p>\n</li>\n<li><p>Recurrent network</p>\n<p>It is hard to train but natural way for modeling sequence, recent algorithm can be able to do this, this also can be used to predict stock price.</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/83abfe6f730b3ae61d9099ff0d5ff300.png\" alt=\"nnml-recurrent\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/3a688df70ed6bf1e46d62a03402e2a39.png\" alt=\"nnml-rnn\"></p>\n<blockquote>\n<p>20171218 LSTM 现在貌似更流行</p>\n</blockquote>\n</li>\n<li><p>Symmetrically connected networks</p>\n</li>\n</ul>\n<h2 id=\"感知器\"><a href=\"#感知器\" class=\"headerlink\" title=\"感知器\"></a>感知器</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/a1ba9669ccb220d05a8140bc8e0ca797.png\" alt=\"nnml-perceptron-arch\"></p>\n<blockquote>\n<p>how to learn biases using the same rule as we use for learning weights</p>\n</blockquote>\n<p>  <img src=\"http://p15i7i801.bkt.clouddn.com/d73047067e05f31a249dc755851602aa.png\" alt=\"nnml-handlebias\"></p>\n<blockquote>\n<p>学习的过程：if the output unit is correct, leave its weights alone, if the output unit incorrectly output</p>\n</blockquote>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/6b72ab6f556908380fcd6d5561d2e650.png\" alt=\"nnml-per-train\"></p>\n<ul>\n<li><strong>错误的输出了零，则Z需要新增以致其逐渐大于零而产生y=1（往正面），反之则需要减</strong> ？？此偏移向量为负如何解释，或者必须保证 该向量是指向正确的分类超平面的（perpendicular，正交）。</li>\n<li>$(w,b) \\leftarrow (w,b)-(x,1)$<br>$(w,b) \\leftarrow (w,b)+(x,1)$<br><img src=\"http://p15i7i801.bkt.clouddn.com/b109e1d1e4424d180575bb0f74e75906.png\" alt=\"nnml-adpateside\"></li>\n<li><em>just like penalty function</em></li>\n</ul>\n<blockquote>\n<p>为什么学习有效: _非正式的收敛证明_</p>\n</blockquote>\n<ol>\n<li>Each time the perceptron makes a mistake, the current weight vector moves to decrease its squared distance from every weight vector in the “generously feasible” region.</li>\n<li><strong> The squared distance decreases by at least the squared length of the input vector. </strong>（向量的运算，几何解释）</li>\n<li>So after a finite number of mistakes, the weight vector must lie in the feasible region if the region exist.</li>\n</ol>\n<h2 id=\"感知器的缺陷\"><a href=\"#感知器的缺陷\" class=\"headerlink\" title=\"感知器的缺陷\"></a>感知器的缺陷</h2><blockquote>\n<p><strong>right feature</strong></p>\n</blockquote>\n<p>  Once the hand-coded features have been determined, there are very strong limitations on what a perceptron can learn.<br>  _The main question is : This type of table look-up won’t generalize, it need too many feature_<br>  _此篇习题未理解_</p>\n<blockquote>\n<p>Group Invariance Theorem—-Minsky and Papert</p>\n</blockquote>\n<p>can’t learn to do this if the transformations from a group{欧氏变换之内}，so it needs to use multiple feature unit to recognize transformations of informative sub-patterns.<br>So,more  important was all about how you learn <strong>feature detectors, that’s hidden units</strong>. After 20 years, We know,<br>we need multiple layer of adaptive, non-linear hidden units.</p>\n"},{"title":"Thinking in Java","date":"2017-04-11T06:29:14.000Z","mathjax":false,"_content":"\n\n# 一、导论\n* 对象都有一个接口\n* private, protected, public和无限定词——包访问权限\n* 组合与继承（覆盖方法、添加新方法）\n* 多态\n  * C++用Virtual表示，JAVA内部用一小段特殊代码表示，默认方法皆如此，自动进行upcasting\n* 单根继承——Object\n* 首先便于基本操作，heap new\n* 其次GC\n* 范型\n* 异常\n* WEB发展，前端JS APLLET 跨平台 后端  servlet JSP 跨平台消除异构浏览器的复杂性\n\n# 二、一切为对象\n* 引用\n* 对象存储（寄存器、堆栈、堆、常量、持久化存储）\n* 特例：基本类型\n* 基本数据类型的作用域及对象的自动GC\n* 类，成员默认值，局部变量依然是随机值，强制初始化\n* 命名空间(com.apaceh.)，import，static\n  * 静态变量提供与对象无关的存储空间分配；\n  * 静态方法提供，直接调用 的方法，如main\n* JAVADOC，帮你自动生成JAVA API文档\n* 编码风格：驼峰\n\n# 三四、操作符、控制流\n* 引用，包括赋值、传递\n* 一元加减操作符\n* 逻辑运算符的短路现象：（）&&（）&&（），最短前两个即能判断该表达式\n* 指数计数 、移位（注意-1的表示 方法）、三元操作符\n* e作为10的幂次\n* 字符串操作符+、+=，定义若表达式以一个字符串开头，后续所有操作数都必须是字符串，要不然强制转\n* round()\n* 没有sizeof\n* for中的逗号操作符，foreach\n\n# 五、初始化与清理\n* 构造、重载（参数、顺序皆可，返回值不可，对于基本类型，传递参数可上升不可下降，因此要人工强制窄化）\n* this返回本对象\n* finalize，非析构，在GC时调用，可以认为是验证对象的终结，在调用NATIVE CODE时尤其有用\n* GC\n\n# 六、继承\n* 类的加载：找到类文件，加载基类，加载主类，静态初始化、创建对象，成员初始化（基础类型为默认值，对象引用置空），构造器\n* final成员、参数 、方法与类\n\n# 七、多态（OO三种基本特征，抽象、继承、多态）\n* 原理：upcasting,自动后期binding，除非final\n20170709  只有普通方法的调用可以是多态的，如私有方法、域及静态方法\n* 若要编码一个基础类，除非必需使用方法和成员，那么第一选择应该是使用接口；\n* 基类——抽象方法——抽象类——接口，由具体到抽象，若一个基类是纯抽象的，而未告诉编译器它是抽象的或是个接口，那么，可能会因为误重载（本来是覆盖，因为参数不同，导致动态绑定失效）的原因，调用基类中的“伪”方法，造成非预知的问题。\n* 接口：类默认为public，成员默认为staic和final ，可多重继承，\n* 适配接口，策略设计模式\n被适配的类 继承和实现基策略 任何类都可以通过为多重继承的方式被适配\n* 接口中的任何域都是static和final的\n* 接口可以嵌套，在类中或接口中 ，在类中private只能交给有权使用它的对象，即使他被一个public接口给实现；而嵌套在接口中则必须是public。\n* 工厂模式，较通用，一般用于框架，代码可复用\n\n# 十、内部类\n# 十一、异常\n* finally 在此处有用，处理异常下清理 ，但注意在构造方法中，不适合使用，而适在catch中处理\n* 注意exception有丢失的可能可能性，比如被下一个exception覆盖（即catch内层有几层异常发生），又比如被finnally return .\n* 异常匹配就近原则\n* 异常及类型检查是必要的，但可以编译器和运行发生，只要它存在。反射及范型也是JAVA为编译期过多的检查所作的补偿。\n\n# 十二、字符串\n* string +  性能不如stringbuilder 使用javap可反编译分析，且JAVA5前是stringbuffer,而stringbuffer是线程安全的，效率不高。\n* toString 可能会造成无穷递归调用 tostring, 如   tostring {  return 'a' + this ;},防止的方法是调用super.this\n* system.out.printf/formatter 及formatter类、string.format()  用于格式化修饰%[argument_index$][flags][width][.precision]conversion， flags用于对齐， 如%05x,    右对齐，不足5位用0补齐\n* string.match()/split()/replace()/replaceall()\n# 十四：RTTI\n* RTTI及反射使得运行时识别对象和类信息，而对象实际执行什么代码，这是有它指向的引用决定的，即多态机制。\n* 通常希望大部分代码尽可能少了解对象具体类型，而只与对象家族的一个通用表示打交道。因此，多态是OOP的基本目标。\n# 十五：范型\n* 范型方法的 类型参数推断 ，基本类型的自动打包机制。类型参数推断的局限性在于只对赋值有效，这种局限可以通过显式的类型说明来弥补\n* 范型方法可与可变参数配合\n十六、数组\n* 相对容器高效，但功能更少\n* 初始化，对象为null，基本类型 char \\0000 即转整型后为0\n* 有个取不重复随机数的方法：\n  ```java\n  do\n  t = rand.nextInt(len);\n  while(picked[i]);\n  ```\n","source":"_posts/Thinking-in-Java.md","raw":"---\ntitle: Thinking in Java\ndate: 2017-04-11 14:29:14\ntags:\n      - 计算机语言\n      - 经典著作\ncategories: 深耕码农\nmathjax: false\n---\n\n\n# 一、导论\n* 对象都有一个接口\n* private, protected, public和无限定词——包访问权限\n* 组合与继承（覆盖方法、添加新方法）\n* 多态\n  * C++用Virtual表示，JAVA内部用一小段特殊代码表示，默认方法皆如此，自动进行upcasting\n* 单根继承——Object\n* 首先便于基本操作，heap new\n* 其次GC\n* 范型\n* 异常\n* WEB发展，前端JS APLLET 跨平台 后端  servlet JSP 跨平台消除异构浏览器的复杂性\n\n# 二、一切为对象\n* 引用\n* 对象存储（寄存器、堆栈、堆、常量、持久化存储）\n* 特例：基本类型\n* 基本数据类型的作用域及对象的自动GC\n* 类，成员默认值，局部变量依然是随机值，强制初始化\n* 命名空间(com.apaceh.)，import，static\n  * 静态变量提供与对象无关的存储空间分配；\n  * 静态方法提供，直接调用 的方法，如main\n* JAVADOC，帮你自动生成JAVA API文档\n* 编码风格：驼峰\n\n# 三四、操作符、控制流\n* 引用，包括赋值、传递\n* 一元加减操作符\n* 逻辑运算符的短路现象：（）&&（）&&（），最短前两个即能判断该表达式\n* 指数计数 、移位（注意-1的表示 方法）、三元操作符\n* e作为10的幂次\n* 字符串操作符+、+=，定义若表达式以一个字符串开头，后续所有操作数都必须是字符串，要不然强制转\n* round()\n* 没有sizeof\n* for中的逗号操作符，foreach\n\n# 五、初始化与清理\n* 构造、重载（参数、顺序皆可，返回值不可，对于基本类型，传递参数可上升不可下降，因此要人工强制窄化）\n* this返回本对象\n* finalize，非析构，在GC时调用，可以认为是验证对象的终结，在调用NATIVE CODE时尤其有用\n* GC\n\n# 六、继承\n* 类的加载：找到类文件，加载基类，加载主类，静态初始化、创建对象，成员初始化（基础类型为默认值，对象引用置空），构造器\n* final成员、参数 、方法与类\n\n# 七、多态（OO三种基本特征，抽象、继承、多态）\n* 原理：upcasting,自动后期binding，除非final\n20170709  只有普通方法的调用可以是多态的，如私有方法、域及静态方法\n* 若要编码一个基础类，除非必需使用方法和成员，那么第一选择应该是使用接口；\n* 基类——抽象方法——抽象类——接口，由具体到抽象，若一个基类是纯抽象的，而未告诉编译器它是抽象的或是个接口，那么，可能会因为误重载（本来是覆盖，因为参数不同，导致动态绑定失效）的原因，调用基类中的“伪”方法，造成非预知的问题。\n* 接口：类默认为public，成员默认为staic和final ，可多重继承，\n* 适配接口，策略设计模式\n被适配的类 继承和实现基策略 任何类都可以通过为多重继承的方式被适配\n* 接口中的任何域都是static和final的\n* 接口可以嵌套，在类中或接口中 ，在类中private只能交给有权使用它的对象，即使他被一个public接口给实现；而嵌套在接口中则必须是public。\n* 工厂模式，较通用，一般用于框架，代码可复用\n\n# 十、内部类\n# 十一、异常\n* finally 在此处有用，处理异常下清理 ，但注意在构造方法中，不适合使用，而适在catch中处理\n* 注意exception有丢失的可能可能性，比如被下一个exception覆盖（即catch内层有几层异常发生），又比如被finnally return .\n* 异常匹配就近原则\n* 异常及类型检查是必要的，但可以编译器和运行发生，只要它存在。反射及范型也是JAVA为编译期过多的检查所作的补偿。\n\n# 十二、字符串\n* string +  性能不如stringbuilder 使用javap可反编译分析，且JAVA5前是stringbuffer,而stringbuffer是线程安全的，效率不高。\n* toString 可能会造成无穷递归调用 tostring, 如   tostring {  return 'a' + this ;},防止的方法是调用super.this\n* system.out.printf/formatter 及formatter类、string.format()  用于格式化修饰%[argument_index$][flags][width][.precision]conversion， flags用于对齐， 如%05x,    右对齐，不足5位用0补齐\n* string.match()/split()/replace()/replaceall()\n# 十四：RTTI\n* RTTI及反射使得运行时识别对象和类信息，而对象实际执行什么代码，这是有它指向的引用决定的，即多态机制。\n* 通常希望大部分代码尽可能少了解对象具体类型，而只与对象家族的一个通用表示打交道。因此，多态是OOP的基本目标。\n# 十五：范型\n* 范型方法的 类型参数推断 ，基本类型的自动打包机制。类型参数推断的局限性在于只对赋值有效，这种局限可以通过显式的类型说明来弥补\n* 范型方法可与可变参数配合\n十六、数组\n* 相对容器高效，但功能更少\n* 初始化，对象为null，基本类型 char \\0000 即转整型后为0\n* 有个取不重复随机数的方法：\n  ```java\n  do\n  t = rand.nextInt(len);\n  while(picked[i]);\n  ```\n","slug":"Thinking-in-Java","published":1,"updated":"2017-12-27T12:36:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjclxs5hb0007ajsl6obb6ujy","content":"<h1 id=\"一、导论\"><a href=\"#一、导论\" class=\"headerlink\" title=\"一、导论\"></a>一、导论</h1><ul>\n<li>对象都有一个接口</li>\n<li>private, protected, public和无限定词——包访问权限</li>\n<li>组合与继承（覆盖方法、添加新方法）</li>\n<li>多态<ul>\n<li>C++用Virtual表示，JAVA内部用一小段特殊代码表示，默认方法皆如此，自动进行upcasting</li>\n</ul>\n</li>\n<li>单根继承——Object</li>\n<li>首先便于基本操作，heap new</li>\n<li>其次GC</li>\n<li>范型</li>\n<li>异常</li>\n<li>WEB发展，前端JS APLLET 跨平台 后端  servlet JSP 跨平台消除异构浏览器的复杂性</li>\n</ul>\n<h1 id=\"二、一切为对象\"><a href=\"#二、一切为对象\" class=\"headerlink\" title=\"二、一切为对象\"></a>二、一切为对象</h1><ul>\n<li>引用</li>\n<li>对象存储（寄存器、堆栈、堆、常量、持久化存储）</li>\n<li>特例：基本类型</li>\n<li>基本数据类型的作用域及对象的自动GC</li>\n<li>类，成员默认值，局部变量依然是随机值，强制初始化</li>\n<li>命名空间(com.apaceh.)，import，static<ul>\n<li>静态变量提供与对象无关的存储空间分配；</li>\n<li>静态方法提供，直接调用 的方法，如main</li>\n</ul>\n</li>\n<li>JAVADOC，帮你自动生成JAVA API文档</li>\n<li>编码风格：驼峰</li>\n</ul>\n<h1 id=\"三四、操作符、控制流\"><a href=\"#三四、操作符、控制流\" class=\"headerlink\" title=\"三四、操作符、控制流\"></a>三四、操作符、控制流</h1><ul>\n<li>引用，包括赋值、传递</li>\n<li>一元加减操作符</li>\n<li>逻辑运算符的短路现象：（）&amp;&amp;（）&amp;&amp;（），最短前两个即能判断该表达式</li>\n<li>指数计数 、移位（注意-1的表示 方法）、三元操作符</li>\n<li>e作为10的幂次</li>\n<li>字符串操作符+、+=，定义若表达式以一个字符串开头，后续所有操作数都必须是字符串，要不然强制转</li>\n<li>round()</li>\n<li>没有sizeof</li>\n<li>for中的逗号操作符，foreach</li>\n</ul>\n<h1 id=\"五、初始化与清理\"><a href=\"#五、初始化与清理\" class=\"headerlink\" title=\"五、初始化与清理\"></a>五、初始化与清理</h1><ul>\n<li>构造、重载（参数、顺序皆可，返回值不可，对于基本类型，传递参数可上升不可下降，因此要人工强制窄化）</li>\n<li>this返回本对象</li>\n<li>finalize，非析构，在GC时调用，可以认为是验证对象的终结，在调用NATIVE CODE时尤其有用</li>\n<li>GC</li>\n</ul>\n<h1 id=\"六、继承\"><a href=\"#六、继承\" class=\"headerlink\" title=\"六、继承\"></a>六、继承</h1><ul>\n<li>类的加载：找到类文件，加载基类，加载主类，静态初始化、创建对象，成员初始化（基础类型为默认值，对象引用置空），构造器</li>\n<li>final成员、参数 、方法与类</li>\n</ul>\n<h1 id=\"七、多态（OO三种基本特征，抽象、继承、多态）\"><a href=\"#七、多态（OO三种基本特征，抽象、继承、多态）\" class=\"headerlink\" title=\"七、多态（OO三种基本特征，抽象、继承、多态）\"></a>七、多态（OO三种基本特征，抽象、继承、多态）</h1><ul>\n<li>原理：upcasting,自动后期binding，除非final<br>20170709  只有普通方法的调用可以是多态的，如私有方法、域及静态方法</li>\n<li>若要编码一个基础类，除非必需使用方法和成员，那么第一选择应该是使用接口；</li>\n<li>基类——抽象方法——抽象类——接口，由具体到抽象，若一个基类是纯抽象的，而未告诉编译器它是抽象的或是个接口，那么，可能会因为误重载（本来是覆盖，因为参数不同，导致动态绑定失效）的原因，调用基类中的“伪”方法，造成非预知的问题。</li>\n<li>接口：类默认为public，成员默认为staic和final ，可多重继承，</li>\n<li>适配接口，策略设计模式<br>被适配的类 继承和实现基策略 任何类都可以通过为多重继承的方式被适配</li>\n<li>接口中的任何域都是static和final的</li>\n<li>接口可以嵌套，在类中或接口中 ，在类中private只能交给有权使用它的对象，即使他被一个public接口给实现；而嵌套在接口中则必须是public。</li>\n<li>工厂模式，较通用，一般用于框架，代码可复用</li>\n</ul>\n<h1 id=\"十、内部类\"><a href=\"#十、内部类\" class=\"headerlink\" title=\"十、内部类\"></a>十、内部类</h1><h1 id=\"十一、异常\"><a href=\"#十一、异常\" class=\"headerlink\" title=\"十一、异常\"></a>十一、异常</h1><ul>\n<li>finally 在此处有用，处理异常下清理 ，但注意在构造方法中，不适合使用，而适在catch中处理</li>\n<li>注意exception有丢失的可能可能性，比如被下一个exception覆盖（即catch内层有几层异常发生），又比如被finnally return .</li>\n<li>异常匹配就近原则</li>\n<li>异常及类型检查是必要的，但可以编译器和运行发生，只要它存在。反射及范型也是JAVA为编译期过多的检查所作的补偿。</li>\n</ul>\n<h1 id=\"十二、字符串\"><a href=\"#十二、字符串\" class=\"headerlink\" title=\"十二、字符串\"></a>十二、字符串</h1><ul>\n<li>string +  性能不如stringbuilder 使用javap可反编译分析，且JAVA5前是stringbuffer,而stringbuffer是线程安全的，效率不高。</li>\n<li>toString 可能会造成无穷递归调用 tostring, 如   tostring {  return ‘a’ + this ;},防止的方法是调用super.this</li>\n<li>system.out.printf/formatter 及formatter类、string.format()  用于格式化修饰%[argument_index$][flags][width][.precision]conversion， flags用于对齐， 如%05x,    右对齐，不足5位用0补齐</li>\n<li>string.match()/split()/replace()/replaceall()<h1 id=\"十四：RTTI\"><a href=\"#十四：RTTI\" class=\"headerlink\" title=\"十四：RTTI\"></a>十四：RTTI</h1></li>\n<li>RTTI及反射使得运行时识别对象和类信息，而对象实际执行什么代码，这是有它指向的引用决定的，即多态机制。</li>\n<li>通常希望大部分代码尽可能少了解对象具体类型，而只与对象家族的一个通用表示打交道。因此，多态是OOP的基本目标。<h1 id=\"十五：范型\"><a href=\"#十五：范型\" class=\"headerlink\" title=\"十五：范型\"></a>十五：范型</h1></li>\n<li>范型方法的 类型参数推断 ，基本类型的自动打包机制。类型参数推断的局限性在于只对赋值有效，这种局限可以通过显式的类型说明来弥补</li>\n<li>范型方法可与可变参数配合<br>十六、数组</li>\n<li>相对容器高效，但功能更少</li>\n<li>初始化，对象为null，基本类型 char \\0000 即转整型后为0</li>\n<li>有个取不重复随机数的方法：<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">do</span></span><br><span class=\"line\">t = rand.nextInt(len);</span><br><span class=\"line\"><span class=\"keyword\">while</span>(picked[i]);</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"一、导论\"><a href=\"#一、导论\" class=\"headerlink\" title=\"一、导论\"></a>一、导论</h1><ul>\n<li>对象都有一个接口</li>\n<li>private, protected, public和无限定词——包访问权限</li>\n<li>组合与继承（覆盖方法、添加新方法）</li>\n<li>多态<ul>\n<li>C++用Virtual表示，JAVA内部用一小段特殊代码表示，默认方法皆如此，自动进行upcasting</li>\n</ul>\n</li>\n<li>单根继承——Object</li>\n<li>首先便于基本操作，heap new</li>\n<li>其次GC</li>\n<li>范型</li>\n<li>异常</li>\n<li>WEB发展，前端JS APLLET 跨平台 后端  servlet JSP 跨平台消除异构浏览器的复杂性</li>\n</ul>\n<h1 id=\"二、一切为对象\"><a href=\"#二、一切为对象\" class=\"headerlink\" title=\"二、一切为对象\"></a>二、一切为对象</h1><ul>\n<li>引用</li>\n<li>对象存储（寄存器、堆栈、堆、常量、持久化存储）</li>\n<li>特例：基本类型</li>\n<li>基本数据类型的作用域及对象的自动GC</li>\n<li>类，成员默认值，局部变量依然是随机值，强制初始化</li>\n<li>命名空间(com.apaceh.)，import，static<ul>\n<li>静态变量提供与对象无关的存储空间分配；</li>\n<li>静态方法提供，直接调用 的方法，如main</li>\n</ul>\n</li>\n<li>JAVADOC，帮你自动生成JAVA API文档</li>\n<li>编码风格：驼峰</li>\n</ul>\n<h1 id=\"三四、操作符、控制流\"><a href=\"#三四、操作符、控制流\" class=\"headerlink\" title=\"三四、操作符、控制流\"></a>三四、操作符、控制流</h1><ul>\n<li>引用，包括赋值、传递</li>\n<li>一元加减操作符</li>\n<li>逻辑运算符的短路现象：（）&amp;&amp;（）&amp;&amp;（），最短前两个即能判断该表达式</li>\n<li>指数计数 、移位（注意-1的表示 方法）、三元操作符</li>\n<li>e作为10的幂次</li>\n<li>字符串操作符+、+=，定义若表达式以一个字符串开头，后续所有操作数都必须是字符串，要不然强制转</li>\n<li>round()</li>\n<li>没有sizeof</li>\n<li>for中的逗号操作符，foreach</li>\n</ul>\n<h1 id=\"五、初始化与清理\"><a href=\"#五、初始化与清理\" class=\"headerlink\" title=\"五、初始化与清理\"></a>五、初始化与清理</h1><ul>\n<li>构造、重载（参数、顺序皆可，返回值不可，对于基本类型，传递参数可上升不可下降，因此要人工强制窄化）</li>\n<li>this返回本对象</li>\n<li>finalize，非析构，在GC时调用，可以认为是验证对象的终结，在调用NATIVE CODE时尤其有用</li>\n<li>GC</li>\n</ul>\n<h1 id=\"六、继承\"><a href=\"#六、继承\" class=\"headerlink\" title=\"六、继承\"></a>六、继承</h1><ul>\n<li>类的加载：找到类文件，加载基类，加载主类，静态初始化、创建对象，成员初始化（基础类型为默认值，对象引用置空），构造器</li>\n<li>final成员、参数 、方法与类</li>\n</ul>\n<h1 id=\"七、多态（OO三种基本特征，抽象、继承、多态）\"><a href=\"#七、多态（OO三种基本特征，抽象、继承、多态）\" class=\"headerlink\" title=\"七、多态（OO三种基本特征，抽象、继承、多态）\"></a>七、多态（OO三种基本特征，抽象、继承、多态）</h1><ul>\n<li>原理：upcasting,自动后期binding，除非final<br>20170709  只有普通方法的调用可以是多态的，如私有方法、域及静态方法</li>\n<li>若要编码一个基础类，除非必需使用方法和成员，那么第一选择应该是使用接口；</li>\n<li>基类——抽象方法——抽象类——接口，由具体到抽象，若一个基类是纯抽象的，而未告诉编译器它是抽象的或是个接口，那么，可能会因为误重载（本来是覆盖，因为参数不同，导致动态绑定失效）的原因，调用基类中的“伪”方法，造成非预知的问题。</li>\n<li>接口：类默认为public，成员默认为staic和final ，可多重继承，</li>\n<li>适配接口，策略设计模式<br>被适配的类 继承和实现基策略 任何类都可以通过为多重继承的方式被适配</li>\n<li>接口中的任何域都是static和final的</li>\n<li>接口可以嵌套，在类中或接口中 ，在类中private只能交给有权使用它的对象，即使他被一个public接口给实现；而嵌套在接口中则必须是public。</li>\n<li>工厂模式，较通用，一般用于框架，代码可复用</li>\n</ul>\n<h1 id=\"十、内部类\"><a href=\"#十、内部类\" class=\"headerlink\" title=\"十、内部类\"></a>十、内部类</h1><h1 id=\"十一、异常\"><a href=\"#十一、异常\" class=\"headerlink\" title=\"十一、异常\"></a>十一、异常</h1><ul>\n<li>finally 在此处有用，处理异常下清理 ，但注意在构造方法中，不适合使用，而适在catch中处理</li>\n<li>注意exception有丢失的可能可能性，比如被下一个exception覆盖（即catch内层有几层异常发生），又比如被finnally return .</li>\n<li>异常匹配就近原则</li>\n<li>异常及类型检查是必要的，但可以编译器和运行发生，只要它存在。反射及范型也是JAVA为编译期过多的检查所作的补偿。</li>\n</ul>\n<h1 id=\"十二、字符串\"><a href=\"#十二、字符串\" class=\"headerlink\" title=\"十二、字符串\"></a>十二、字符串</h1><ul>\n<li>string +  性能不如stringbuilder 使用javap可反编译分析，且JAVA5前是stringbuffer,而stringbuffer是线程安全的，效率不高。</li>\n<li>toString 可能会造成无穷递归调用 tostring, 如   tostring {  return ‘a’ + this ;},防止的方法是调用super.this</li>\n<li>system.out.printf/formatter 及formatter类、string.format()  用于格式化修饰%[argument_index$][flags][width][.precision]conversion， flags用于对齐， 如%05x,    右对齐，不足5位用0补齐</li>\n<li>string.match()/split()/replace()/replaceall()<h1 id=\"十四：RTTI\"><a href=\"#十四：RTTI\" class=\"headerlink\" title=\"十四：RTTI\"></a>十四：RTTI</h1></li>\n<li>RTTI及反射使得运行时识别对象和类信息，而对象实际执行什么代码，这是有它指向的引用决定的，即多态机制。</li>\n<li>通常希望大部分代码尽可能少了解对象具体类型，而只与对象家族的一个通用表示打交道。因此，多态是OOP的基本目标。<h1 id=\"十五：范型\"><a href=\"#十五：范型\" class=\"headerlink\" title=\"十五：范型\"></a>十五：范型</h1></li>\n<li>范型方法的 类型参数推断 ，基本类型的自动打包机制。类型参数推断的局限性在于只对赋值有效，这种局限可以通过显式的类型说明来弥补</li>\n<li>范型方法可与可变参数配合<br>十六、数组</li>\n<li>相对容器高效，但功能更少</li>\n<li>初始化，对象为null，基本类型 char \\0000 即转整型后为0</li>\n<li>有个取不重复随机数的方法：<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">do</span></span><br><span class=\"line\">t = rand.nextInt(len);</span><br><span class=\"line\"><span class=\"keyword\">while</span>(picked[i]);</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n"},{"title":"draft","date":"1900-12-27T07:10:09.000Z","mathjax":true,"_content":"\n# 增强学习\n* 四个要素 $(A,S,R,P)$\n\n* 策略 $\\pi$ 决定选择哪个行动 $a$, 即：\n  $$\n  \\begin{cases}\n  \\pi(s) \\implies a \\\\\n  \\pi(a|s)\n  \\end{cases}\n  $$\n* 根据要素抽象行为成序列，通常要满足[马尔可夫条件](http://www.cnblogs.com/jinxulin/p/3517377.html),这些序列构成训练模型样本\n  $$ \\{s_1,a_1,r_1,s_2,a_2,r_2,...,s_t,a_t,r_t\\} $$\n* 定义目标 $G_t$，作为长期回报，其值用收益折现来表达，有 $G_t=\\sum_{k=0}^N\\gamma^kr_{k+t}$, 因此可推导目标期望：\n$$\nV_\\pi(s)=E_\\pi(G_t|S_t=s)  \\\\\nQ_\\pi(s,a)=E_\\pi(G_t|S_t=s,A_t=a)\n$$\n* [Bellman等式](https://en.wikipedia.org/wiki/Bellman_equation)，也称动态规划等式，在RL里非常重要，其在此场景下用来表示长期回报——值函数：\n$$\n\\mbox{ 在某策略下：}V_\\pi(s)=\\sum\\pi(a|s)E[R_{t+1}+\\gamma V(s_{t+1})|S_t=s]  \\\\\n V_*(s)=E[R_{t+1}+\\gamma max_\\pi V(s_{t+1})|S_t=s]  \\\\\n Q_*(s,a)=E[R_{t+1}+\\gamma max_{a^\\prime} Q(s_{t+1},a^\\prime)|S_t=s,A_t=a]\n$$\n* 有了值函数 $V_\\pi$的概念后,则问题抽象为在马尔可夫决策过程中，任意初始条件 $s$下，求能够最大化值函数的策略 $\\pi$，其可表达为 $\\pi^*=arg_\\pi \\{maxV_\\pi(s)\\}$，具体求解过程可参看[计算实例](http://www.cnblogs.com/jinxulin/p/3517377.html)\n","source":"_posts/draft.md","raw":"---\ntitle: draft\ndate: 0000-12-27 15:10:09\ntags:\n  - draft\nmathjax: true\n---\n\n# 增强学习\n* 四个要素 $(A,S,R,P)$\n\n* 策略 $\\pi$ 决定选择哪个行动 $a$, 即：\n  $$\n  \\begin{cases}\n  \\pi(s) \\implies a \\\\\n  \\pi(a|s)\n  \\end{cases}\n  $$\n* 根据要素抽象行为成序列，通常要满足[马尔可夫条件](http://www.cnblogs.com/jinxulin/p/3517377.html),这些序列构成训练模型样本\n  $$ \\{s_1,a_1,r_1,s_2,a_2,r_2,...,s_t,a_t,r_t\\} $$\n* 定义目标 $G_t$，作为长期回报，其值用收益折现来表达，有 $G_t=\\sum_{k=0}^N\\gamma^kr_{k+t}$, 因此可推导目标期望：\n$$\nV_\\pi(s)=E_\\pi(G_t|S_t=s)  \\\\\nQ_\\pi(s,a)=E_\\pi(G_t|S_t=s,A_t=a)\n$$\n* [Bellman等式](https://en.wikipedia.org/wiki/Bellman_equation)，也称动态规划等式，在RL里非常重要，其在此场景下用来表示长期回报——值函数：\n$$\n\\mbox{ 在某策略下：}V_\\pi(s)=\\sum\\pi(a|s)E[R_{t+1}+\\gamma V(s_{t+1})|S_t=s]  \\\\\n V_*(s)=E[R_{t+1}+\\gamma max_\\pi V(s_{t+1})|S_t=s]  \\\\\n Q_*(s,a)=E[R_{t+1}+\\gamma max_{a^\\prime} Q(s_{t+1},a^\\prime)|S_t=s,A_t=a]\n$$\n* 有了值函数 $V_\\pi$的概念后,则问题抽象为在马尔可夫决策过程中，任意初始条件 $s$下，求能够最大化值函数的策略 $\\pi$，其可表达为 $\\pi^*=arg_\\pi \\{maxV_\\pi(s)\\}$，具体求解过程可参看[计算实例](http://www.cnblogs.com/jinxulin/p/3517377.html)\n","slug":"draft","published":1,"updated":"2017-12-27T12:48:41.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjclxs5hd0008ajslx5lun51t","content":"<h1 id=\"增强学习\"><a href=\"#增强学习\" class=\"headerlink\" title=\"增强学习\"></a>增强学习</h1><ul>\n<li><p>四个要素 $(A,S,R,P)$</p>\n</li>\n<li><p>策略 $\\pi$ 决定选择哪个行动 $a$, 即：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\n\\pi(s) \\implies a \\\\\n\\pi(a|s)\n\\end{cases}</script></li>\n<li>根据要素抽象行为成序列，通常要满足<a href=\"http://www.cnblogs.com/jinxulin/p/3517377.html\" target=\"_blank\" rel=\"noopener\">马尔可夫条件</a>,这些序列构成训练模型样本<script type=\"math/tex; mode=display\">\\{s_1,a_1,r_1,s_2,a_2,r_2,...,s_t,a_t,r_t\\}</script></li>\n<li>定义目标 $G_t$，作为长期回报，其值用收益折现来表达，有 $G_t=\\sum_{k=0}^N\\gamma^kr_{k+t}$, 因此可推导目标期望：<script type=\"math/tex; mode=display\">\nV_\\pi(s)=E_\\pi(G_t|S_t=s)  \\\\\nQ_\\pi(s,a)=E_\\pi(G_t|S_t=s,A_t=a)</script></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Bellman_equation\" target=\"_blank\" rel=\"noopener\">Bellman等式</a>，也称动态规划等式，在RL里非常重要，其在此场景下用来表示长期回报——值函数：<script type=\"math/tex; mode=display\">\n\\mbox{ 在某策略下：}V_\\pi(s)=\\sum\\pi(a|s)E[R_{t+1}+\\gamma V(s_{t+1})|S_t=s]  \\\\\nV_*(s)=E[R_{t+1}+\\gamma max_\\pi V(s_{t+1})|S_t=s]  \\\\\nQ_*(s,a)=E[R_{t+1}+\\gamma max_{a^\\prime} Q(s_{t+1},a^\\prime)|S_t=s,A_t=a]</script></li>\n<li>有了值函数 $V_\\pi$的概念后,则问题抽象为在马尔可夫决策过程中，任意初始条件 $s$下，求能够最大化值函数的策略 $\\pi$，其可表达为 $\\pi^*=arg_\\pi \\{maxV_\\pi(s)\\}$，具体求解过程可参看<a href=\"http://www.cnblogs.com/jinxulin/p/3517377.html\" target=\"_blank\" rel=\"noopener\">计算实例</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"增强学习\"><a href=\"#增强学习\" class=\"headerlink\" title=\"增强学习\"></a>增强学习</h1><ul>\n<li><p>四个要素 $(A,S,R,P)$</p>\n</li>\n<li><p>策略 $\\pi$ 决定选择哪个行动 $a$, 即：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\n\\pi(s) \\implies a \\\\\n\\pi(a|s)\n\\end{cases}</script></li>\n<li>根据要素抽象行为成序列，通常要满足<a href=\"http://www.cnblogs.com/jinxulin/p/3517377.html\" target=\"_blank\" rel=\"noopener\">马尔可夫条件</a>,这些序列构成训练模型样本<script type=\"math/tex; mode=display\">\\{s_1,a_1,r_1,s_2,a_2,r_2,...,s_t,a_t,r_t\\}</script></li>\n<li>定义目标 $G_t$，作为长期回报，其值用收益折现来表达，有 $G_t=\\sum_{k=0}^N\\gamma^kr_{k+t}$, 因此可推导目标期望：<script type=\"math/tex; mode=display\">\nV_\\pi(s)=E_\\pi(G_t|S_t=s)  \\\\\nQ_\\pi(s,a)=E_\\pi(G_t|S_t=s,A_t=a)</script></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Bellman_equation\" target=\"_blank\" rel=\"noopener\">Bellman等式</a>，也称动态规划等式，在RL里非常重要，其在此场景下用来表示长期回报——值函数：<script type=\"math/tex; mode=display\">\n\\mbox{ 在某策略下：}V_\\pi(s)=\\sum\\pi(a|s)E[R_{t+1}+\\gamma V(s_{t+1})|S_t=s]  \\\\\nV_*(s)=E[R_{t+1}+\\gamma max_\\pi V(s_{t+1})|S_t=s]  \\\\\nQ_*(s,a)=E[R_{t+1}+\\gamma max_{a^\\prime} Q(s_{t+1},a^\\prime)|S_t=s,A_t=a]</script></li>\n<li>有了值函数 $V_\\pi$的概念后,则问题抽象为在马尔可夫决策过程中，任意初始条件 $s$下，求能够最大化值函数的策略 $\\pi$，其可表达为 $\\pi^*=arg_\\pi \\{maxV_\\pi(s)\\}$，具体求解过程可参看<a href=\"http://www.cnblogs.com/jinxulin/p/3517377.html\" target=\"_blank\" rel=\"noopener\">计算实例</a></li>\n</ul>\n"},{"title":"Practical introduction to data structures and algorithm analysis","date":"2017-09-28T03:12:18.000Z","mathjax":true,"_content":"\n# 概率论及一些基础思考\n## 代价与效益\n在有效的资源下将问题解决，称该算法是有效的。一个好的数据结构（解决方案）设计首先要考虑到业务场景，在操作上（增删改查），它有什么需求。比如查询上要求并发高、但在插入速度上并无太高的要求，或者反之。\n## 计算复杂度\n* 通常我们考虑算法的在一定数据规模下的平均复杂度，但平均复杂度分析并不是一定可靠的，这建立在输入数据概率分布已知的情况下考虑的，通常这种假设每个元素出现的概率均等。但是不正确的假设也可能带来灾难的性的后果，特别是在搜索类算法上，如散列和搜索树，具体事例可参考9.4、5.4节。有时间考虑特殊的数据分布，也会带来好处，如9.2节（自组织线性表）\n* 在实时系统中不考虑平均情况，而考虑最差的情况。\n* 复杂度分析，通常采用渐近分析的方法，使用化简的方法计算上界，下界和平均情况。\n\n## 最完美算法\n联机算法——举例而言，最大子序列求和的问题\n## 运行时间中的对数\n* 如二叉树，contains操作，较明显，在常数据O(1)时间问题的大小缩减为其一部分（通过是一半或小于一半），则该算法是O(logN)\n* 最大公因(约）数，即使用欧几里德/辗转相除法计算gcd(M,N)。由定理，若M>N，则有M mod N < M/2，可得该算法迭代次数至多是O(logN)\n* 幂次运算，使用递归分解，将乘法降到O(logN)\n\n# 基本数据结构\n## 表、栈、队列\n## 树\n\n### 二叉树\n\n可以使用栈来实现表达式树，也可用递归，这样会方便些。基本操作包括增(insert)、删(remove,两种可能性)、查(contains, findmin,findmax)，平均计算复杂度：\n\n得O(nlogN)，但这无法控制上界，**为控制任意次连续M次操作在最坏的情况下花费时间为O(MlogN)，引入AVL平衡树，当然它无法保证单次的操作的时间界。最后根据二八法则，将引入伸展树**\n### AVL平衡树\n\n  使用单旋和双旋调整高度，结构上每个节点信息加入了高度。核心插入代码：\n```C++\n    void insert( const Comparable & x, AvlNode * & t )\n    {\n        if( t == NULL )\n            t = new AvlNode( x, NULL, NULL );\n        else if( x < t->element )\n        {\n            insert( x, t->left );\n            if( height( t->left ) - height( t->right ) == 2 )\n                if( x < t->left->element )\n                    rotateWithLeftChild( t );\n                else\n                    doubleWithLeftChild( t );\n        }\n        else if( t->element < x )\n        {\n            insert( x, t->right );\n            if( height( t->right ) - height( t->left ) == 2 )\n                if( t->right->element < x )\n                    rotateWithRightChild( t );\n                else\n                    doubleWithRightChild( t );\n        }\n        else\n            ;  // Duplicate; do nothing\n        t->height = max( height( t->left ), height( t->right ) ) + 1;\n    }\n```\n### 伸展树\n之形伸展与一字伸展，目标是沿访问路径旋转  \n### 树的遍历\n### B树——数据库的索引树，用于解决大量数据磁盘检索的问题\n注意B树的定义，5个特性，M\\L参数的确定。如一个区块8K，每个键32B，在一颗M阶的B树中有M-1个键，则可算出M。对于叶子节点若每个记录是256字节，区块是8K，则可算出L=32。再回来看定义：\n1. 数据在叶子节点，并在相同深度上，且数据项介于L/2到L之间；\n2. 非叶节点，存储M-1个键，指示查找方向，其儿子数在M/2到M之间;\n注意B树插入时的分裂操作和删除的认领操作。\n\n### STL中set和map的树形实现\n### 散列\n1. 散列函数:利用horner法则，计算多项式函数$h_k=k_0+37k_1+37k_2=(37+37k_2)k_1+k_0$。通常建议桶大小为素数\n2. 冲突解决方法：分离链接、线性/平方探测、双散列。分离链接法因子可以到1，性能下降不明显，探测方法最好不超过0.5，双散列宜用质数。\n3. 再散列：一般使用途中策略，到达装载因子再散列\n4. hashmap, hashset\n5. 散列比起二叉树而言速度虽有所提升但缺乏排序，通常用在比如编译器的符号表，拼写检查等等  \n\n### 优先队列（堆）\n\n##  排序与检索\n## 高级话题\n","source":"_posts/Practical-introduction-to-data-structures-and-algorithm-analysis.md","raw":"---\ntitle: Practical introduction to data structures and algorithm analysis\ndate: 2017-09-28 11:12:18\ntags:\n      - 数据结构\n      - 经典著作\ncategories: 深耕码农\nmathjax: true\n---\n\n# 概率论及一些基础思考\n## 代价与效益\n在有效的资源下将问题解决，称该算法是有效的。一个好的数据结构（解决方案）设计首先要考虑到业务场景，在操作上（增删改查），它有什么需求。比如查询上要求并发高、但在插入速度上并无太高的要求，或者反之。\n## 计算复杂度\n* 通常我们考虑算法的在一定数据规模下的平均复杂度，但平均复杂度分析并不是一定可靠的，这建立在输入数据概率分布已知的情况下考虑的，通常这种假设每个元素出现的概率均等。但是不正确的假设也可能带来灾难的性的后果，特别是在搜索类算法上，如散列和搜索树，具体事例可参考9.4、5.4节。有时间考虑特殊的数据分布，也会带来好处，如9.2节（自组织线性表）\n* 在实时系统中不考虑平均情况，而考虑最差的情况。\n* 复杂度分析，通常采用渐近分析的方法，使用化简的方法计算上界，下界和平均情况。\n\n## 最完美算法\n联机算法——举例而言，最大子序列求和的问题\n## 运行时间中的对数\n* 如二叉树，contains操作，较明显，在常数据O(1)时间问题的大小缩减为其一部分（通过是一半或小于一半），则该算法是O(logN)\n* 最大公因(约）数，即使用欧几里德/辗转相除法计算gcd(M,N)。由定理，若M>N，则有M mod N < M/2，可得该算法迭代次数至多是O(logN)\n* 幂次运算，使用递归分解，将乘法降到O(logN)\n\n# 基本数据结构\n## 表、栈、队列\n## 树\n\n### 二叉树\n\n可以使用栈来实现表达式树，也可用递归，这样会方便些。基本操作包括增(insert)、删(remove,两种可能性)、查(contains, findmin,findmax)，平均计算复杂度：\n\n得O(nlogN)，但这无法控制上界，**为控制任意次连续M次操作在最坏的情况下花费时间为O(MlogN)，引入AVL平衡树，当然它无法保证单次的操作的时间界。最后根据二八法则，将引入伸展树**\n### AVL平衡树\n\n  使用单旋和双旋调整高度，结构上每个节点信息加入了高度。核心插入代码：\n```C++\n    void insert( const Comparable & x, AvlNode * & t )\n    {\n        if( t == NULL )\n            t = new AvlNode( x, NULL, NULL );\n        else if( x < t->element )\n        {\n            insert( x, t->left );\n            if( height( t->left ) - height( t->right ) == 2 )\n                if( x < t->left->element )\n                    rotateWithLeftChild( t );\n                else\n                    doubleWithLeftChild( t );\n        }\n        else if( t->element < x )\n        {\n            insert( x, t->right );\n            if( height( t->right ) - height( t->left ) == 2 )\n                if( t->right->element < x )\n                    rotateWithRightChild( t );\n                else\n                    doubleWithRightChild( t );\n        }\n        else\n            ;  // Duplicate; do nothing\n        t->height = max( height( t->left ), height( t->right ) ) + 1;\n    }\n```\n### 伸展树\n之形伸展与一字伸展，目标是沿访问路径旋转  \n### 树的遍历\n### B树——数据库的索引树，用于解决大量数据磁盘检索的问题\n注意B树的定义，5个特性，M\\L参数的确定。如一个区块8K，每个键32B，在一颗M阶的B树中有M-1个键，则可算出M。对于叶子节点若每个记录是256字节，区块是8K，则可算出L=32。再回来看定义：\n1. 数据在叶子节点，并在相同深度上，且数据项介于L/2到L之间；\n2. 非叶节点，存储M-1个键，指示查找方向，其儿子数在M/2到M之间;\n注意B树插入时的分裂操作和删除的认领操作。\n\n### STL中set和map的树形实现\n### 散列\n1. 散列函数:利用horner法则，计算多项式函数$h_k=k_0+37k_1+37k_2=(37+37k_2)k_1+k_0$。通常建议桶大小为素数\n2. 冲突解决方法：分离链接、线性/平方探测、双散列。分离链接法因子可以到1，性能下降不明显，探测方法最好不超过0.5，双散列宜用质数。\n3. 再散列：一般使用途中策略，到达装载因子再散列\n4. hashmap, hashset\n5. 散列比起二叉树而言速度虽有所提升但缺乏排序，通常用在比如编译器的符号表，拼写检查等等  \n\n### 优先队列（堆）\n\n##  排序与检索\n## 高级话题\n","slug":"Practical-introduction-to-data-structures-and-algorithm-analysis","published":1,"updated":"2017-12-16T10:13:53.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjclxs5he0009ajsl89ve49sj","content":"<h1 id=\"概率论及一些基础思考\"><a href=\"#概率论及一些基础思考\" class=\"headerlink\" title=\"概率论及一些基础思考\"></a>概率论及一些基础思考</h1><h2 id=\"代价与效益\"><a href=\"#代价与效益\" class=\"headerlink\" title=\"代价与效益\"></a>代价与效益</h2><p>在有效的资源下将问题解决，称该算法是有效的。一个好的数据结构（解决方案）设计首先要考虑到业务场景，在操作上（增删改查），它有什么需求。比如查询上要求并发高、但在插入速度上并无太高的要求，或者反之。</p>\n<h2 id=\"计算复杂度\"><a href=\"#计算复杂度\" class=\"headerlink\" title=\"计算复杂度\"></a>计算复杂度</h2><ul>\n<li>通常我们考虑算法的在一定数据规模下的平均复杂度，但平均复杂度分析并不是一定可靠的，这建立在输入数据概率分布已知的情况下考虑的，通常这种假设每个元素出现的概率均等。但是不正确的假设也可能带来灾难的性的后果，特别是在搜索类算法上，如散列和搜索树，具体事例可参考9.4、5.4节。有时间考虑特殊的数据分布，也会带来好处，如9.2节（自组织线性表）</li>\n<li>在实时系统中不考虑平均情况，而考虑最差的情况。</li>\n<li>复杂度分析，通常采用渐近分析的方法，使用化简的方法计算上界，下界和平均情况。</li>\n</ul>\n<h2 id=\"最完美算法\"><a href=\"#最完美算法\" class=\"headerlink\" title=\"最完美算法\"></a>最完美算法</h2><p>联机算法——举例而言，最大子序列求和的问题</p>\n<h2 id=\"运行时间中的对数\"><a href=\"#运行时间中的对数\" class=\"headerlink\" title=\"运行时间中的对数\"></a>运行时间中的对数</h2><ul>\n<li>如二叉树，contains操作，较明显，在常数据O(1)时间问题的大小缩减为其一部分（通过是一半或小于一半），则该算法是O(logN)</li>\n<li>最大公因(约）数，即使用欧几里德/辗转相除法计算gcd(M,N)。由定理，若M&gt;N，则有M mod N &lt; M/2，可得该算法迭代次数至多是O(logN)</li>\n<li>幂次运算，使用递归分解，将乘法降到O(logN)</li>\n</ul>\n<h1 id=\"基本数据结构\"><a href=\"#基本数据结构\" class=\"headerlink\" title=\"基本数据结构\"></a>基本数据结构</h1><h2 id=\"表、栈、队列\"><a href=\"#表、栈、队列\" class=\"headerlink\" title=\"表、栈、队列\"></a>表、栈、队列</h2><h2 id=\"树\"><a href=\"#树\" class=\"headerlink\" title=\"树\"></a>树</h2><h3 id=\"二叉树\"><a href=\"#二叉树\" class=\"headerlink\" title=\"二叉树\"></a>二叉树</h3><p>可以使用栈来实现表达式树，也可用递归，这样会方便些。基本操作包括增(insert)、删(remove,两种可能性)、查(contains, findmin,findmax)，平均计算复杂度：</p>\n<p>得O(nlogN)，但这无法控制上界，<strong>为控制任意次连续M次操作在最坏的情况下花费时间为O(MlogN)，引入AVL平衡树，当然它无法保证单次的操作的时间界。最后根据二八法则，将引入伸展树</strong></p>\n<h3 id=\"AVL平衡树\"><a href=\"#AVL平衡树\" class=\"headerlink\" title=\"AVL平衡树\"></a>AVL平衡树</h3><p>  使用单旋和双旋调整高度，结构上每个节点信息加入了高度。核心插入代码：<br><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">insert</span><span class=\"params\">( <span class=\"keyword\">const</span> Comparable &amp; x, AvlNode * &amp; t )</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>( t == <span class=\"literal\">NULL</span> )</span><br><span class=\"line\">        t = <span class=\"keyword\">new</span> AvlNode( x, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span> );</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>( x &lt; t-&gt;element )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        insert( x, t-&gt;left );</span><br><span class=\"line\">        <span class=\"keyword\">if</span>( height( t-&gt;left ) - height( t-&gt;right ) == <span class=\"number\">2</span> )</span><br><span class=\"line\">            <span class=\"keyword\">if</span>( x &lt; t-&gt;left-&gt;element )</span><br><span class=\"line\">                rotateWithLeftChild( t );</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                doubleWithLeftChild( t );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>( t-&gt;element &lt; x )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        insert( x, t-&gt;right );</span><br><span class=\"line\">        <span class=\"keyword\">if</span>( height( t-&gt;right ) - height( t-&gt;left ) == <span class=\"number\">2</span> )</span><br><span class=\"line\">            <span class=\"keyword\">if</span>( t-&gt;right-&gt;element &lt; x )</span><br><span class=\"line\">                rotateWithRightChild( t );</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                doubleWithRightChild( t );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        ;  <span class=\"comment\">// Duplicate; do nothing</span></span><br><span class=\"line\">    t-&gt;height = max( height( t-&gt;left ), height( t-&gt;right ) ) + <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"伸展树\"><a href=\"#伸展树\" class=\"headerlink\" title=\"伸展树\"></a>伸展树</h3><p>之形伸展与一字伸展，目标是沿访问路径旋转  </p>\n<h3 id=\"树的遍历\"><a href=\"#树的遍历\" class=\"headerlink\" title=\"树的遍历\"></a>树的遍历</h3><h3 id=\"B树——数据库的索引树，用于解决大量数据磁盘检索的问题\"><a href=\"#B树——数据库的索引树，用于解决大量数据磁盘检索的问题\" class=\"headerlink\" title=\"B树——数据库的索引树，用于解决大量数据磁盘检索的问题\"></a>B树——数据库的索引树，用于解决大量数据磁盘检索的问题</h3><p>注意B树的定义，5个特性，M\\L参数的确定。如一个区块8K，每个键32B，在一颗M阶的B树中有M-1个键，则可算出M。对于叶子节点若每个记录是256字节，区块是8K，则可算出L=32。再回来看定义：</p>\n<ol>\n<li>数据在叶子节点，并在相同深度上，且数据项介于L/2到L之间；</li>\n<li>非叶节点，存储M-1个键，指示查找方向，其儿子数在M/2到M之间;<br>注意B树插入时的分裂操作和删除的认领操作。</li>\n</ol>\n<h3 id=\"STL中set和map的树形实现\"><a href=\"#STL中set和map的树形实现\" class=\"headerlink\" title=\"STL中set和map的树形实现\"></a>STL中set和map的树形实现</h3><h3 id=\"散列\"><a href=\"#散列\" class=\"headerlink\" title=\"散列\"></a>散列</h3><ol>\n<li>散列函数:利用horner法则，计算多项式函数$h_k=k_0+37k_1+37k_2=(37+37k_2)k_1+k_0$。通常建议桶大小为素数</li>\n<li>冲突解决方法：分离链接、线性/平方探测、双散列。分离链接法因子可以到1，性能下降不明显，探测方法最好不超过0.5，双散列宜用质数。</li>\n<li>再散列：一般使用途中策略，到达装载因子再散列</li>\n<li>hashmap, hashset</li>\n<li>散列比起二叉树而言速度虽有所提升但缺乏排序，通常用在比如编译器的符号表，拼写检查等等  </li>\n</ol>\n<h3 id=\"优先队列（堆）\"><a href=\"#优先队列（堆）\" class=\"headerlink\" title=\"优先队列（堆）\"></a>优先队列（堆）</h3><h2 id=\"排序与检索\"><a href=\"#排序与检索\" class=\"headerlink\" title=\"排序与检索\"></a>排序与检索</h2><h2 id=\"高级话题\"><a href=\"#高级话题\" class=\"headerlink\" title=\"高级话题\"></a>高级话题</h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"概率论及一些基础思考\"><a href=\"#概率论及一些基础思考\" class=\"headerlink\" title=\"概率论及一些基础思考\"></a>概率论及一些基础思考</h1><h2 id=\"代价与效益\"><a href=\"#代价与效益\" class=\"headerlink\" title=\"代价与效益\"></a>代价与效益</h2><p>在有效的资源下将问题解决，称该算法是有效的。一个好的数据结构（解决方案）设计首先要考虑到业务场景，在操作上（增删改查），它有什么需求。比如查询上要求并发高、但在插入速度上并无太高的要求，或者反之。</p>\n<h2 id=\"计算复杂度\"><a href=\"#计算复杂度\" class=\"headerlink\" title=\"计算复杂度\"></a>计算复杂度</h2><ul>\n<li>通常我们考虑算法的在一定数据规模下的平均复杂度，但平均复杂度分析并不是一定可靠的，这建立在输入数据概率分布已知的情况下考虑的，通常这种假设每个元素出现的概率均等。但是不正确的假设也可能带来灾难的性的后果，特别是在搜索类算法上，如散列和搜索树，具体事例可参考9.4、5.4节。有时间考虑特殊的数据分布，也会带来好处，如9.2节（自组织线性表）</li>\n<li>在实时系统中不考虑平均情况，而考虑最差的情况。</li>\n<li>复杂度分析，通常采用渐近分析的方法，使用化简的方法计算上界，下界和平均情况。</li>\n</ul>\n<h2 id=\"最完美算法\"><a href=\"#最完美算法\" class=\"headerlink\" title=\"最完美算法\"></a>最完美算法</h2><p>联机算法——举例而言，最大子序列求和的问题</p>\n<h2 id=\"运行时间中的对数\"><a href=\"#运行时间中的对数\" class=\"headerlink\" title=\"运行时间中的对数\"></a>运行时间中的对数</h2><ul>\n<li>如二叉树，contains操作，较明显，在常数据O(1)时间问题的大小缩减为其一部分（通过是一半或小于一半），则该算法是O(logN)</li>\n<li>最大公因(约）数，即使用欧几里德/辗转相除法计算gcd(M,N)。由定理，若M&gt;N，则有M mod N &lt; M/2，可得该算法迭代次数至多是O(logN)</li>\n<li>幂次运算，使用递归分解，将乘法降到O(logN)</li>\n</ul>\n<h1 id=\"基本数据结构\"><a href=\"#基本数据结构\" class=\"headerlink\" title=\"基本数据结构\"></a>基本数据结构</h1><h2 id=\"表、栈、队列\"><a href=\"#表、栈、队列\" class=\"headerlink\" title=\"表、栈、队列\"></a>表、栈、队列</h2><h2 id=\"树\"><a href=\"#树\" class=\"headerlink\" title=\"树\"></a>树</h2><h3 id=\"二叉树\"><a href=\"#二叉树\" class=\"headerlink\" title=\"二叉树\"></a>二叉树</h3><p>可以使用栈来实现表达式树，也可用递归，这样会方便些。基本操作包括增(insert)、删(remove,两种可能性)、查(contains, findmin,findmax)，平均计算复杂度：</p>\n<p>得O(nlogN)，但这无法控制上界，<strong>为控制任意次连续M次操作在最坏的情况下花费时间为O(MlogN)，引入AVL平衡树，当然它无法保证单次的操作的时间界。最后根据二八法则，将引入伸展树</strong></p>\n<h3 id=\"AVL平衡树\"><a href=\"#AVL平衡树\" class=\"headerlink\" title=\"AVL平衡树\"></a>AVL平衡树</h3><p>  使用单旋和双旋调整高度，结构上每个节点信息加入了高度。核心插入代码：<br><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">insert</span><span class=\"params\">( <span class=\"keyword\">const</span> Comparable &amp; x, AvlNode * &amp; t )</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>( t == <span class=\"literal\">NULL</span> )</span><br><span class=\"line\">        t = <span class=\"keyword\">new</span> AvlNode( x, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span> );</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>( x &lt; t-&gt;element )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        insert( x, t-&gt;left );</span><br><span class=\"line\">        <span class=\"keyword\">if</span>( height( t-&gt;left ) - height( t-&gt;right ) == <span class=\"number\">2</span> )</span><br><span class=\"line\">            <span class=\"keyword\">if</span>( x &lt; t-&gt;left-&gt;element )</span><br><span class=\"line\">                rotateWithLeftChild( t );</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                doubleWithLeftChild( t );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>( t-&gt;element &lt; x )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        insert( x, t-&gt;right );</span><br><span class=\"line\">        <span class=\"keyword\">if</span>( height( t-&gt;right ) - height( t-&gt;left ) == <span class=\"number\">2</span> )</span><br><span class=\"line\">            <span class=\"keyword\">if</span>( t-&gt;right-&gt;element &lt; x )</span><br><span class=\"line\">                rotateWithRightChild( t );</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                doubleWithRightChild( t );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        ;  <span class=\"comment\">// Duplicate; do nothing</span></span><br><span class=\"line\">    t-&gt;height = max( height( t-&gt;left ), height( t-&gt;right ) ) + <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"伸展树\"><a href=\"#伸展树\" class=\"headerlink\" title=\"伸展树\"></a>伸展树</h3><p>之形伸展与一字伸展，目标是沿访问路径旋转  </p>\n<h3 id=\"树的遍历\"><a href=\"#树的遍历\" class=\"headerlink\" title=\"树的遍历\"></a>树的遍历</h3><h3 id=\"B树——数据库的索引树，用于解决大量数据磁盘检索的问题\"><a href=\"#B树——数据库的索引树，用于解决大量数据磁盘检索的问题\" class=\"headerlink\" title=\"B树——数据库的索引树，用于解决大量数据磁盘检索的问题\"></a>B树——数据库的索引树，用于解决大量数据磁盘检索的问题</h3><p>注意B树的定义，5个特性，M\\L参数的确定。如一个区块8K，每个键32B，在一颗M阶的B树中有M-1个键，则可算出M。对于叶子节点若每个记录是256字节，区块是8K，则可算出L=32。再回来看定义：</p>\n<ol>\n<li>数据在叶子节点，并在相同深度上，且数据项介于L/2到L之间；</li>\n<li>非叶节点，存储M-1个键，指示查找方向，其儿子数在M/2到M之间;<br>注意B树插入时的分裂操作和删除的认领操作。</li>\n</ol>\n<h3 id=\"STL中set和map的树形实现\"><a href=\"#STL中set和map的树形实现\" class=\"headerlink\" title=\"STL中set和map的树形实现\"></a>STL中set和map的树形实现</h3><h3 id=\"散列\"><a href=\"#散列\" class=\"headerlink\" title=\"散列\"></a>散列</h3><ol>\n<li>散列函数:利用horner法则，计算多项式函数$h_k=k_0+37k_1+37k_2=(37+37k_2)k_1+k_0$。通常建议桶大小为素数</li>\n<li>冲突解决方法：分离链接、线性/平方探测、双散列。分离链接法因子可以到1，性能下降不明显，探测方法最好不超过0.5，双散列宜用质数。</li>\n<li>再散列：一般使用途中策略，到达装载因子再散列</li>\n<li>hashmap, hashset</li>\n<li>散列比起二叉树而言速度虽有所提升但缺乏排序，通常用在比如编译器的符号表，拼写检查等等  </li>\n</ol>\n<h3 id=\"优先队列（堆）\"><a href=\"#优先队列（堆）\" class=\"headerlink\" title=\"优先队列（堆）\"></a>优先队列（堆）</h3><h2 id=\"排序与检索\"><a href=\"#排序与检索\" class=\"headerlink\" title=\"排序与检索\"></a>排序与检索</h2><h2 id=\"高级话题\"><a href=\"#高级话题\" class=\"headerlink\" title=\"高级话题\"></a>高级话题</h2>"},{"title":"action in machine learning","date":"2017-08-29T10:21:54.000Z","mathjax":true,"_content":"\n### 树回归\n* 连续型数值的混乱度：总平方误差来表示，即方差*m\n* 离散型的标称的混乱度可以用香农熵表示\n* 若叶节使用的模型是分段常数则称为回归树，若叶节点使用的模型是线性回归方程则称为模型树\n* 使用CART构建出的树倾向于过拟合，分别使用预剪枝和后剪枝解决这个问题。预剪枝设置停止条件来调整拟合问题，这需要不断调整，见P168。而后剪枝首先构造足够复杂的树，然后使用测试集来判断叶节点合并能否降低误差\n* 决策树通过迭代实现分类，离散情况下，叶节点的类别即预测的类别，连续情况下，若是回归树则是该子树叶子节点的平均值，若是模型树，则是该（线性）模型的值\n\n### Random Forest/GBDT\n[参考介绍一：二分类](http://blog.csdn.net/google19890102/article/details/51746402)\n[参考介绍一:多分类](http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html)\n*  随机建立树的方式，随机采样包括行采样（样本），列采样（特征）\n* 即本质是每颗树是窄领域内的专家，众专家一起投票\n\n###预测数值型数据:回归\n#### 线性回归\n  使用最小二乘法，得$若X非奇异，则有w=(X^tX)^{-1}X^ty$\n#### 局部加权线性回归（Local Weighted Leaner Regression）\n  线性回归的问题是容易欠拟合，因此引入局部加权\n  $$\\hat{w}=(X^twX)^{-1}X^twy$$\n  通常使用高斯核用来赋于权重$w(i,i)=e^{\\frac {x^{(i)}-x}{-2k^2}}$，即有离预测点越近的点权重越大，同时参数K控制了参与运算的数据，这样就有如下图:\n![aiml-localweightedLR](http://p15i7i801.bkt.clouddn.com/d1676ee6aefd1a476cc7f40036c87f5c.png)\n\n  此方法缺点在于**计算量增加，每次预测必须在全集上运行**，好处在于可以控制拟合程度\n![aiml-LR](http://p15i7i801.bkt.clouddn.com/e9cc4d793fd511a4e7749fa6660005c5.png)\n\n#### 缩减系数——用于更好的理解数据并处理特征比样本多的问题\n  对于$X^TX$为奇异矩阵，在$X^TX$基础加上 $\\lambda I$使得矩阵非奇异，这种方法称为*岭回归*。此方法有三个用途：\n  * 用于处理特征比样本多的情况\n  * 用于在估计中加入偏差，从而得到更好的估计\n  * 通过引入   $\\lambda$ 来限制 $w$ 之和，通过引入惩罚，能够减少不重要的参数，这类技术在统计学称为衰减。（迹不变）\n  * 此公式$$等价于最小二乘加入约束\n\n### 附加篇：各类方法的一个归纳\n* 树模型：DT, GBDT, RF\n* 概率模型：Bayes\n* 最优化：LR, SVM\n    * LR：简单实用，互联网CTR预估被广泛使用\n    * SVM：简单、多分类问题比SVM好，不好解释；\n* 距离划分，判别模型\n    * KNN：简单、多分类问题比SVM好，不好解释；\n* 集成方法：Adaboost\n* 方法比较\n    * NB与LR\n      * 相同点：都是特征的线性表述，解释性对较好；\n      * 不同点：NB要求严格独立，NB有利于并行化，但随着数据量和维度增多，LR更合适\n* 可持久化的模型：决策树\n\n### 决策树|随机森林|adaboost|GBDT\n* [决策树模型组合之（在线）随机森林与GBDT](http://www.cnblogs.com/wentingtu/archive/2011/12/13/2286212.html)\n* [Bagging（Bootstrap aggregating）、随机森林（random forests）、AdaBoost](http://blog.csdn.net/xlinsist/article/details/51475345 )\n* [决策树ID3、C4.5、C5.0以及CART算法之间的比较-并用scikit-learn决策树拟合Iris数据集](http://blog.csdn.net/xlinsist/article/details/51468741)\n* ID3缺点:\n    *  对于多值的属性非常之敏感\n    *  无法处理连续值\n    *  容易产生过拟合，即训练数据训练太彻底，但大一点数据集错误率上升，关注了些局部信息\n* C4.5---C5.0\n  * 解决上述1、2的问题，核心思想是加入了增益率\n* CART\n  * 能预测出值（回归），同时能全用训练和交叉验证不断评估，修剪决策树\n\n### K-nearest\n* 思路：基于现有已知标签的样本数据集，寻找被预测样本的最近的K个样本，返回K个里频率最多的那个标签。\n* 优点：最简单有效的分类、对异常值不敏感\n* 缺点：解释性差、类别评分无规格化，不平衡问题\n* 适应：[brute force\\kd_tree\\balltree的选择](http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors )\n\n### PCA与SVD\nhttp://blog.csdn.net/jacke121/article/details/59057192\npca是单方向上的降维，SVD是两个方向上的降维。\nSVD表示是$A=U\\Sigma V^T$，其中U,V是正交的。\n若要降维：使用这个公式，其具体数学推导，暂不明。现实含义可以这样理解：\n在SVD的三个矩阵（假设叫1，2，3）中，1-2表示用户降维转换，2-3表示物品降维转换，在用户推荐那个场景，由于是使用基于物品的推荐，要计算两物品的相似度，则可理解为只需要用户角度降维，即1-2的转换。即有\n$$U_k \\Sigma V=> A^T U_k \\Sigma$$\n推导点：**如如此在用户上降维，那是否可以直接通过PCA在用户角度降维？**\n\n### 关联分析\n支持度，可信度\n* Apriori\n\n### 附加篇一\n> 张同学的问题：SVM中，如下代码中权重 $w$ 为何不能做特征的权重值。\n\n```python\ndef calcWs(alphas, dataArr, classLabels):\n    \"\"\"\n    输入：alphas, 数据集, 类别标签\n    输出：目标w\n    \"\"\"\n    X = mat(dataArr)\n    labelMat = mat(classLabels).transpose()\n    m, n = shape(X)\n    w = zeros((n, 1))\n    for i in range(m):\n        w += multiply(alphas[i] * labelMat[i], X[i, :].T)\n    return w\n```\n对比线性情况而言,有 $w= \\Sigma \\alpha y_i x_i$ ，书中有下代码\n```python\nw += multiply(alphas[i] * labelMat[i], X[i, :].T)\n```\n目标函数为：$y=(\\Sigma \\alpha y_i x_i) x + b$ 而对于映射核空间的情况，书中有描述\n![aiml-weightbook](http://p15i7i801.bkt.clouddn.com/c9fb298cf712884a5ad1c6d2407c4b95.png)\n此时目标函数则为 $y=\\Sigma \\alpha y_i k(\\phi(x_i),\\phi(x_j))+b$。显然，特征向量 $w$ 是无法拆出来的。\n\n> 对weights = weights + alpha * dataMatrix.transpose() * error 这一行的理解\n\n* 角度一：http://blog.csdn.net/lu597203933/article/details/38468303  AndrewNg\n![aiml-weight](http://p15i7i801.bkt.clouddn.com/79f4eb72472afa25f82690b0823b603d.png)\n* 角度二：矩阵微分的角度理解 http://blog.csdn.net/aichipmunk/article/details/9382503\n\n### 附加篇二：机器学习算法路径图\n![aiml-algorithm chea-sheet](http://p15i7i801.bkt.clouddn.com/e161673fc2063c42f4d407fd7057fc1f.png)\n![aiml-td](http://p15i7i801.bkt.clouddn.com/daa070eafedebe29f976a12a0b65b08c.png)\n","source":"_posts/action-in-machine-learning.md","raw":"---\ntitle: action in machine learning\ndate: 2017-08-29 18:21:54\ntags:\n      - 机器学习\n      - 经典著作\ncategories: AI梦\nmathjax: true\n---\n\n### 树回归\n* 连续型数值的混乱度：总平方误差来表示，即方差*m\n* 离散型的标称的混乱度可以用香农熵表示\n* 若叶节使用的模型是分段常数则称为回归树，若叶节点使用的模型是线性回归方程则称为模型树\n* 使用CART构建出的树倾向于过拟合，分别使用预剪枝和后剪枝解决这个问题。预剪枝设置停止条件来调整拟合问题，这需要不断调整，见P168。而后剪枝首先构造足够复杂的树，然后使用测试集来判断叶节点合并能否降低误差\n* 决策树通过迭代实现分类，离散情况下，叶节点的类别即预测的类别，连续情况下，若是回归树则是该子树叶子节点的平均值，若是模型树，则是该（线性）模型的值\n\n### Random Forest/GBDT\n[参考介绍一：二分类](http://blog.csdn.net/google19890102/article/details/51746402)\n[参考介绍一:多分类](http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html)\n*  随机建立树的方式，随机采样包括行采样（样本），列采样（特征）\n* 即本质是每颗树是窄领域内的专家，众专家一起投票\n\n###预测数值型数据:回归\n#### 线性回归\n  使用最小二乘法，得$若X非奇异，则有w=(X^tX)^{-1}X^ty$\n#### 局部加权线性回归（Local Weighted Leaner Regression）\n  线性回归的问题是容易欠拟合，因此引入局部加权\n  $$\\hat{w}=(X^twX)^{-1}X^twy$$\n  通常使用高斯核用来赋于权重$w(i,i)=e^{\\frac {x^{(i)}-x}{-2k^2}}$，即有离预测点越近的点权重越大，同时参数K控制了参与运算的数据，这样就有如下图:\n![aiml-localweightedLR](http://p15i7i801.bkt.clouddn.com/d1676ee6aefd1a476cc7f40036c87f5c.png)\n\n  此方法缺点在于**计算量增加，每次预测必须在全集上运行**，好处在于可以控制拟合程度\n![aiml-LR](http://p15i7i801.bkt.clouddn.com/e9cc4d793fd511a4e7749fa6660005c5.png)\n\n#### 缩减系数——用于更好的理解数据并处理特征比样本多的问题\n  对于$X^TX$为奇异矩阵，在$X^TX$基础加上 $\\lambda I$使得矩阵非奇异，这种方法称为*岭回归*。此方法有三个用途：\n  * 用于处理特征比样本多的情况\n  * 用于在估计中加入偏差，从而得到更好的估计\n  * 通过引入   $\\lambda$ 来限制 $w$ 之和，通过引入惩罚，能够减少不重要的参数，这类技术在统计学称为衰减。（迹不变）\n  * 此公式$$等价于最小二乘加入约束\n\n### 附加篇：各类方法的一个归纳\n* 树模型：DT, GBDT, RF\n* 概率模型：Bayes\n* 最优化：LR, SVM\n    * LR：简单实用，互联网CTR预估被广泛使用\n    * SVM：简单、多分类问题比SVM好，不好解释；\n* 距离划分，判别模型\n    * KNN：简单、多分类问题比SVM好，不好解释；\n* 集成方法：Adaboost\n* 方法比较\n    * NB与LR\n      * 相同点：都是特征的线性表述，解释性对较好；\n      * 不同点：NB要求严格独立，NB有利于并行化，但随着数据量和维度增多，LR更合适\n* 可持久化的模型：决策树\n\n### 决策树|随机森林|adaboost|GBDT\n* [决策树模型组合之（在线）随机森林与GBDT](http://www.cnblogs.com/wentingtu/archive/2011/12/13/2286212.html)\n* [Bagging（Bootstrap aggregating）、随机森林（random forests）、AdaBoost](http://blog.csdn.net/xlinsist/article/details/51475345 )\n* [决策树ID3、C4.5、C5.0以及CART算法之间的比较-并用scikit-learn决策树拟合Iris数据集](http://blog.csdn.net/xlinsist/article/details/51468741)\n* ID3缺点:\n    *  对于多值的属性非常之敏感\n    *  无法处理连续值\n    *  容易产生过拟合，即训练数据训练太彻底，但大一点数据集错误率上升，关注了些局部信息\n* C4.5---C5.0\n  * 解决上述1、2的问题，核心思想是加入了增益率\n* CART\n  * 能预测出值（回归），同时能全用训练和交叉验证不断评估，修剪决策树\n\n### K-nearest\n* 思路：基于现有已知标签的样本数据集，寻找被预测样本的最近的K个样本，返回K个里频率最多的那个标签。\n* 优点：最简单有效的分类、对异常值不敏感\n* 缺点：解释性差、类别评分无规格化，不平衡问题\n* 适应：[brute force\\kd_tree\\balltree的选择](http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors )\n\n### PCA与SVD\nhttp://blog.csdn.net/jacke121/article/details/59057192\npca是单方向上的降维，SVD是两个方向上的降维。\nSVD表示是$A=U\\Sigma V^T$，其中U,V是正交的。\n若要降维：使用这个公式，其具体数学推导，暂不明。现实含义可以这样理解：\n在SVD的三个矩阵（假设叫1，2，3）中，1-2表示用户降维转换，2-3表示物品降维转换，在用户推荐那个场景，由于是使用基于物品的推荐，要计算两物品的相似度，则可理解为只需要用户角度降维，即1-2的转换。即有\n$$U_k \\Sigma V=> A^T U_k \\Sigma$$\n推导点：**如如此在用户上降维，那是否可以直接通过PCA在用户角度降维？**\n\n### 关联分析\n支持度，可信度\n* Apriori\n\n### 附加篇一\n> 张同学的问题：SVM中，如下代码中权重 $w$ 为何不能做特征的权重值。\n\n```python\ndef calcWs(alphas, dataArr, classLabels):\n    \"\"\"\n    输入：alphas, 数据集, 类别标签\n    输出：目标w\n    \"\"\"\n    X = mat(dataArr)\n    labelMat = mat(classLabels).transpose()\n    m, n = shape(X)\n    w = zeros((n, 1))\n    for i in range(m):\n        w += multiply(alphas[i] * labelMat[i], X[i, :].T)\n    return w\n```\n对比线性情况而言,有 $w= \\Sigma \\alpha y_i x_i$ ，书中有下代码\n```python\nw += multiply(alphas[i] * labelMat[i], X[i, :].T)\n```\n目标函数为：$y=(\\Sigma \\alpha y_i x_i) x + b$ 而对于映射核空间的情况，书中有描述\n![aiml-weightbook](http://p15i7i801.bkt.clouddn.com/c9fb298cf712884a5ad1c6d2407c4b95.png)\n此时目标函数则为 $y=\\Sigma \\alpha y_i k(\\phi(x_i),\\phi(x_j))+b$。显然，特征向量 $w$ 是无法拆出来的。\n\n> 对weights = weights + alpha * dataMatrix.transpose() * error 这一行的理解\n\n* 角度一：http://blog.csdn.net/lu597203933/article/details/38468303  AndrewNg\n![aiml-weight](http://p15i7i801.bkt.clouddn.com/79f4eb72472afa25f82690b0823b603d.png)\n* 角度二：矩阵微分的角度理解 http://blog.csdn.net/aichipmunk/article/details/9382503\n\n### 附加篇二：机器学习算法路径图\n![aiml-algorithm chea-sheet](http://p15i7i801.bkt.clouddn.com/e161673fc2063c42f4d407fd7057fc1f.png)\n![aiml-td](http://p15i7i801.bkt.clouddn.com/daa070eafedebe29f976a12a0b65b08c.png)\n","slug":"action-in-machine-learning","published":1,"updated":"2017-12-19T11:12:31.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjclxs5hh000cajslvjgm1flm","content":"<h3 id=\"树回归\"><a href=\"#树回归\" class=\"headerlink\" title=\"树回归\"></a>树回归</h3><ul>\n<li>连续型数值的混乱度：总平方误差来表示，即方差*m</li>\n<li>离散型的标称的混乱度可以用香农熵表示</li>\n<li>若叶节使用的模型是分段常数则称为回归树，若叶节点使用的模型是线性回归方程则称为模型树</li>\n<li>使用CART构建出的树倾向于过拟合，分别使用预剪枝和后剪枝解决这个问题。预剪枝设置停止条件来调整拟合问题，这需要不断调整，见P168。而后剪枝首先构造足够复杂的树，然后使用测试集来判断叶节点合并能否降低误差</li>\n<li>决策树通过迭代实现分类，离散情况下，叶节点的类别即预测的类别，连续情况下，若是回归树则是该子树叶子节点的平均值，若是模型树，则是该（线性）模型的值</li>\n</ul>\n<h3 id=\"Random-Forest-GBDT\"><a href=\"#Random-Forest-GBDT\" class=\"headerlink\" title=\"Random Forest/GBDT\"></a>Random Forest/GBDT</h3><p><a href=\"http://blog.csdn.net/google19890102/article/details/51746402\" target=\"_blank\" rel=\"noopener\">参考介绍一：二分类</a><br><a href=\"http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html\" target=\"_blank\" rel=\"noopener\">参考介绍一:多分类</a></p>\n<ul>\n<li>随机建立树的方式，随机采样包括行采样（样本），列采样（特征）</li>\n<li>即本质是每颗树是窄领域内的专家，众专家一起投票</li>\n</ul>\n<h3 id=\"预测数值型数据-回归\"><a href=\"#预测数值型数据-回归\" class=\"headerlink\" title=\"预测数值型数据:回归\"></a>预测数值型数据:回归</h3><h4 id=\"线性回归\"><a href=\"#线性回归\" class=\"headerlink\" title=\"线性回归\"></a>线性回归</h4><p>  使用最小二乘法，得$若X非奇异，则有w=(X^tX)^{-1}X^ty$</p>\n<h4 id=\"局部加权线性回归（Local-Weighted-Leaner-Regression）\"><a href=\"#局部加权线性回归（Local-Weighted-Leaner-Regression）\" class=\"headerlink\" title=\"局部加权线性回归（Local Weighted Leaner Regression）\"></a>局部加权线性回归（Local Weighted Leaner Regression）</h4><p>  线性回归的问题是容易欠拟合，因此引入局部加权</p>\n<script type=\"math/tex; mode=display\">\\hat{w}=(X^twX)^{-1}X^twy</script><p>  通常使用高斯核用来赋于权重$w(i,i)=e^{\\frac {x^{(i)}-x}{-2k^2}}$，即有离预测点越近的点权重越大，同时参数K控制了参与运算的数据，这样就有如下图:<br><img src=\"http://p15i7i801.bkt.clouddn.com/d1676ee6aefd1a476cc7f40036c87f5c.png\" alt=\"aiml-localweightedLR\"></p>\n<p>  此方法缺点在于<strong>计算量增加，每次预测必须在全集上运行</strong>，好处在于可以控制拟合程度<br><img src=\"http://p15i7i801.bkt.clouddn.com/e9cc4d793fd511a4e7749fa6660005c5.png\" alt=\"aiml-LR\"></p>\n<h4 id=\"缩减系数——用于更好的理解数据并处理特征比样本多的问题\"><a href=\"#缩减系数——用于更好的理解数据并处理特征比样本多的问题\" class=\"headerlink\" title=\"缩减系数——用于更好的理解数据并处理特征比样本多的问题\"></a>缩减系数——用于更好的理解数据并处理特征比样本多的问题</h4><p>  对于$X^TX$为奇异矩阵，在$X^TX$基础加上 $\\lambda I$使得矩阵非奇异，这种方法称为<em>岭回归</em>。此方法有三个用途：</p>\n<ul>\n<li>用于处理特征比样本多的情况</li>\n<li>用于在估计中加入偏差，从而得到更好的估计</li>\n<li>通过引入   $\\lambda$ 来限制 $w$ 之和，通过引入惩罚，能够减少不重要的参数，这类技术在统计学称为衰减。（迹不变）</li>\n<li>此公式$$等价于最小二乘加入约束</li>\n</ul>\n<h3 id=\"附加篇：各类方法的一个归纳\"><a href=\"#附加篇：各类方法的一个归纳\" class=\"headerlink\" title=\"附加篇：各类方法的一个归纳\"></a>附加篇：各类方法的一个归纳</h3><ul>\n<li>树模型：DT, GBDT, RF</li>\n<li>概率模型：Bayes</li>\n<li>最优化：LR, SVM<ul>\n<li>LR：简单实用，互联网CTR预估被广泛使用</li>\n<li>SVM：简单、多分类问题比SVM好，不好解释；</li>\n</ul>\n</li>\n<li>距离划分，判别模型<ul>\n<li>KNN：简单、多分类问题比SVM好，不好解释；</li>\n</ul>\n</li>\n<li>集成方法：Adaboost</li>\n<li>方法比较<ul>\n<li>NB与LR<ul>\n<li>相同点：都是特征的线性表述，解释性对较好；</li>\n<li>不同点：NB要求严格独立，NB有利于并行化，但随着数据量和维度增多，LR更合适</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>可持久化的模型：决策树</li>\n</ul>\n<h3 id=\"决策树-随机森林-adaboost-GBDT\"><a href=\"#决策树-随机森林-adaboost-GBDT\" class=\"headerlink\" title=\"决策树|随机森林|adaboost|GBDT\"></a>决策树|随机森林|adaboost|GBDT</h3><ul>\n<li><a href=\"http://www.cnblogs.com/wentingtu/archive/2011/12/13/2286212.html\" target=\"_blank\" rel=\"noopener\">决策树模型组合之（在线）随机森林与GBDT</a></li>\n<li><a href=\"http://blog.csdn.net/xlinsist/article/details/51475345\" target=\"_blank\" rel=\"noopener\">Bagging（Bootstrap aggregating）、随机森林（random forests）、AdaBoost</a></li>\n<li><a href=\"http://blog.csdn.net/xlinsist/article/details/51468741\" target=\"_blank\" rel=\"noopener\">决策树ID3、C4.5、C5.0以及CART算法之间的比较-并用scikit-learn决策树拟合Iris数据集</a></li>\n<li>ID3缺点:<ul>\n<li>对于多值的属性非常之敏感</li>\n<li>无法处理连续值</li>\n<li>容易产生过拟合，即训练数据训练太彻底，但大一点数据集错误率上升，关注了些局部信息</li>\n</ul>\n</li>\n<li>C4.5—-C5.0<ul>\n<li>解决上述1、2的问题，核心思想是加入了增益率</li>\n</ul>\n</li>\n<li>CART<ul>\n<li>能预测出值（回归），同时能全用训练和交叉验证不断评估，修剪决策树</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"K-nearest\"><a href=\"#K-nearest\" class=\"headerlink\" title=\"K-nearest\"></a>K-nearest</h3><ul>\n<li>思路：基于现有已知标签的样本数据集，寻找被预测样本的最近的K个样本，返回K个里频率最多的那个标签。</li>\n<li>优点：最简单有效的分类、对异常值不敏感</li>\n<li>缺点：解释性差、类别评分无规格化，不平衡问题</li>\n<li>适应：<a href=\"http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors\" target=\"_blank\" rel=\"noopener\">brute force\\kd_tree\\balltree的选择</a></li>\n</ul>\n<h3 id=\"PCA与SVD\"><a href=\"#PCA与SVD\" class=\"headerlink\" title=\"PCA与SVD\"></a>PCA与SVD</h3><p><a href=\"http://blog.csdn.net/jacke121/article/details/59057192\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/jacke121/article/details/59057192</a><br>pca是单方向上的降维，SVD是两个方向上的降维。<br>SVD表示是$A=U\\Sigma V^T$，其中U,V是正交的。<br>若要降维：使用这个公式，其具体数学推导，暂不明。现实含义可以这样理解：<br>在SVD的三个矩阵（假设叫1，2，3）中，1-2表示用户降维转换，2-3表示物品降维转换，在用户推荐那个场景，由于是使用基于物品的推荐，要计算两物品的相似度，则可理解为只需要用户角度降维，即1-2的转换。即有</p>\n<script type=\"math/tex; mode=display\">U_k \\Sigma V=> A^T U_k \\Sigma</script><p>推导点：<strong>如如此在用户上降维，那是否可以直接通过PCA在用户角度降维？</strong></p>\n<h3 id=\"关联分析\"><a href=\"#关联分析\" class=\"headerlink\" title=\"关联分析\"></a>关联分析</h3><p>支持度，可信度</p>\n<ul>\n<li>Apriori</li>\n</ul>\n<h3 id=\"附加篇一\"><a href=\"#附加篇一\" class=\"headerlink\" title=\"附加篇一\"></a>附加篇一</h3><blockquote>\n<p>张同学的问题：SVM中，如下代码中权重 $w$ 为何不能做特征的权重值。</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calcWs</span><span class=\"params\">(alphas, dataArr, classLabels)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    输入：alphas, 数据集, 类别标签</span></span><br><span class=\"line\"><span class=\"string\">    输出：目标w</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    X = mat(dataArr)</span><br><span class=\"line\">    labelMat = mat(classLabels).transpose()</span><br><span class=\"line\">    m, n = shape(X)</span><br><span class=\"line\">    w = zeros((n, <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(m):</span><br><span class=\"line\">        w += multiply(alphas[i] * labelMat[i], X[i, :].T)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> w</span><br></pre></td></tr></table></figure>\n<p>对比线性情况而言,有 $w= \\Sigma \\alpha y_i x_i$ ，书中有下代码<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w += multiply(alphas[i] * labelMat[i], X[i, :].T)</span><br></pre></td></tr></table></figure></p>\n<p>目标函数为：$y=(\\Sigma \\alpha y_i x_i) x + b$ 而对于映射核空间的情况，书中有描述<br><img src=\"http://p15i7i801.bkt.clouddn.com/c9fb298cf712884a5ad1c6d2407c4b95.png\" alt=\"aiml-weightbook\"><br>此时目标函数则为 $y=\\Sigma \\alpha y_i k(\\phi(x_i),\\phi(x_j))+b$。显然，特征向量 $w$ 是无法拆出来的。</p>\n<blockquote>\n<p>对weights = weights + alpha <em> dataMatrix.transpose() </em> error 这一行的理解</p>\n</blockquote>\n<ul>\n<li>角度一：<a href=\"http://blog.csdn.net/lu597203933/article/details/38468303\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/lu597203933/article/details/38468303</a>  AndrewNg<br><img src=\"http://p15i7i801.bkt.clouddn.com/79f4eb72472afa25f82690b0823b603d.png\" alt=\"aiml-weight\"></li>\n<li>角度二：矩阵微分的角度理解 <a href=\"http://blog.csdn.net/aichipmunk/article/details/9382503\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/aichipmunk/article/details/9382503</a></li>\n</ul>\n<h3 id=\"附加篇二：机器学习算法路径图\"><a href=\"#附加篇二：机器学习算法路径图\" class=\"headerlink\" title=\"附加篇二：机器学习算法路径图\"></a>附加篇二：机器学习算法路径图</h3><p><img src=\"http://p15i7i801.bkt.clouddn.com/e161673fc2063c42f4d407fd7057fc1f.png\" alt=\"aiml-algorithm chea-sheet\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/daa070eafedebe29f976a12a0b65b08c.png\" alt=\"aiml-td\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"树回归\"><a href=\"#树回归\" class=\"headerlink\" title=\"树回归\"></a>树回归</h3><ul>\n<li>连续型数值的混乱度：总平方误差来表示，即方差*m</li>\n<li>离散型的标称的混乱度可以用香农熵表示</li>\n<li>若叶节使用的模型是分段常数则称为回归树，若叶节点使用的模型是线性回归方程则称为模型树</li>\n<li>使用CART构建出的树倾向于过拟合，分别使用预剪枝和后剪枝解决这个问题。预剪枝设置停止条件来调整拟合问题，这需要不断调整，见P168。而后剪枝首先构造足够复杂的树，然后使用测试集来判断叶节点合并能否降低误差</li>\n<li>决策树通过迭代实现分类，离散情况下，叶节点的类别即预测的类别，连续情况下，若是回归树则是该子树叶子节点的平均值，若是模型树，则是该（线性）模型的值</li>\n</ul>\n<h3 id=\"Random-Forest-GBDT\"><a href=\"#Random-Forest-GBDT\" class=\"headerlink\" title=\"Random Forest/GBDT\"></a>Random Forest/GBDT</h3><p><a href=\"http://blog.csdn.net/google19890102/article/details/51746402\" target=\"_blank\" rel=\"noopener\">参考介绍一：二分类</a><br><a href=\"http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html\" target=\"_blank\" rel=\"noopener\">参考介绍一:多分类</a></p>\n<ul>\n<li>随机建立树的方式，随机采样包括行采样（样本），列采样（特征）</li>\n<li>即本质是每颗树是窄领域内的专家，众专家一起投票</li>\n</ul>\n<h3 id=\"预测数值型数据-回归\"><a href=\"#预测数值型数据-回归\" class=\"headerlink\" title=\"预测数值型数据:回归\"></a>预测数值型数据:回归</h3><h4 id=\"线性回归\"><a href=\"#线性回归\" class=\"headerlink\" title=\"线性回归\"></a>线性回归</h4><p>  使用最小二乘法，得$若X非奇异，则有w=(X^tX)^{-1}X^ty$</p>\n<h4 id=\"局部加权线性回归（Local-Weighted-Leaner-Regression）\"><a href=\"#局部加权线性回归（Local-Weighted-Leaner-Regression）\" class=\"headerlink\" title=\"局部加权线性回归（Local Weighted Leaner Regression）\"></a>局部加权线性回归（Local Weighted Leaner Regression）</h4><p>  线性回归的问题是容易欠拟合，因此引入局部加权</p>\n<script type=\"math/tex; mode=display\">\\hat{w}=(X^twX)^{-1}X^twy</script><p>  通常使用高斯核用来赋于权重$w(i,i)=e^{\\frac {x^{(i)}-x}{-2k^2}}$，即有离预测点越近的点权重越大，同时参数K控制了参与运算的数据，这样就有如下图:<br><img src=\"http://p15i7i801.bkt.clouddn.com/d1676ee6aefd1a476cc7f40036c87f5c.png\" alt=\"aiml-localweightedLR\"></p>\n<p>  此方法缺点在于<strong>计算量增加，每次预测必须在全集上运行</strong>，好处在于可以控制拟合程度<br><img src=\"http://p15i7i801.bkt.clouddn.com/e9cc4d793fd511a4e7749fa6660005c5.png\" alt=\"aiml-LR\"></p>\n<h4 id=\"缩减系数——用于更好的理解数据并处理特征比样本多的问题\"><a href=\"#缩减系数——用于更好的理解数据并处理特征比样本多的问题\" class=\"headerlink\" title=\"缩减系数——用于更好的理解数据并处理特征比样本多的问题\"></a>缩减系数——用于更好的理解数据并处理特征比样本多的问题</h4><p>  对于$X^TX$为奇异矩阵，在$X^TX$基础加上 $\\lambda I$使得矩阵非奇异，这种方法称为<em>岭回归</em>。此方法有三个用途：</p>\n<ul>\n<li>用于处理特征比样本多的情况</li>\n<li>用于在估计中加入偏差，从而得到更好的估计</li>\n<li>通过引入   $\\lambda$ 来限制 $w$ 之和，通过引入惩罚，能够减少不重要的参数，这类技术在统计学称为衰减。（迹不变）</li>\n<li>此公式$$等价于最小二乘加入约束</li>\n</ul>\n<h3 id=\"附加篇：各类方法的一个归纳\"><a href=\"#附加篇：各类方法的一个归纳\" class=\"headerlink\" title=\"附加篇：各类方法的一个归纳\"></a>附加篇：各类方法的一个归纳</h3><ul>\n<li>树模型：DT, GBDT, RF</li>\n<li>概率模型：Bayes</li>\n<li>最优化：LR, SVM<ul>\n<li>LR：简单实用，互联网CTR预估被广泛使用</li>\n<li>SVM：简单、多分类问题比SVM好，不好解释；</li>\n</ul>\n</li>\n<li>距离划分，判别模型<ul>\n<li>KNN：简单、多分类问题比SVM好，不好解释；</li>\n</ul>\n</li>\n<li>集成方法：Adaboost</li>\n<li>方法比较<ul>\n<li>NB与LR<ul>\n<li>相同点：都是特征的线性表述，解释性对较好；</li>\n<li>不同点：NB要求严格独立，NB有利于并行化，但随着数据量和维度增多，LR更合适</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>可持久化的模型：决策树</li>\n</ul>\n<h3 id=\"决策树-随机森林-adaboost-GBDT\"><a href=\"#决策树-随机森林-adaboost-GBDT\" class=\"headerlink\" title=\"决策树|随机森林|adaboost|GBDT\"></a>决策树|随机森林|adaboost|GBDT</h3><ul>\n<li><a href=\"http://www.cnblogs.com/wentingtu/archive/2011/12/13/2286212.html\" target=\"_blank\" rel=\"noopener\">决策树模型组合之（在线）随机森林与GBDT</a></li>\n<li><a href=\"http://blog.csdn.net/xlinsist/article/details/51475345\" target=\"_blank\" rel=\"noopener\">Bagging（Bootstrap aggregating）、随机森林（random forests）、AdaBoost</a></li>\n<li><a href=\"http://blog.csdn.net/xlinsist/article/details/51468741\" target=\"_blank\" rel=\"noopener\">决策树ID3、C4.5、C5.0以及CART算法之间的比较-并用scikit-learn决策树拟合Iris数据集</a></li>\n<li>ID3缺点:<ul>\n<li>对于多值的属性非常之敏感</li>\n<li>无法处理连续值</li>\n<li>容易产生过拟合，即训练数据训练太彻底，但大一点数据集错误率上升，关注了些局部信息</li>\n</ul>\n</li>\n<li>C4.5—-C5.0<ul>\n<li>解决上述1、2的问题，核心思想是加入了增益率</li>\n</ul>\n</li>\n<li>CART<ul>\n<li>能预测出值（回归），同时能全用训练和交叉验证不断评估，修剪决策树</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"K-nearest\"><a href=\"#K-nearest\" class=\"headerlink\" title=\"K-nearest\"></a>K-nearest</h3><ul>\n<li>思路：基于现有已知标签的样本数据集，寻找被预测样本的最近的K个样本，返回K个里频率最多的那个标签。</li>\n<li>优点：最简单有效的分类、对异常值不敏感</li>\n<li>缺点：解释性差、类别评分无规格化，不平衡问题</li>\n<li>适应：<a href=\"http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors\" target=\"_blank\" rel=\"noopener\">brute force\\kd_tree\\balltree的选择</a></li>\n</ul>\n<h3 id=\"PCA与SVD\"><a href=\"#PCA与SVD\" class=\"headerlink\" title=\"PCA与SVD\"></a>PCA与SVD</h3><p><a href=\"http://blog.csdn.net/jacke121/article/details/59057192\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/jacke121/article/details/59057192</a><br>pca是单方向上的降维，SVD是两个方向上的降维。<br>SVD表示是$A=U\\Sigma V^T$，其中U,V是正交的。<br>若要降维：使用这个公式，其具体数学推导，暂不明。现实含义可以这样理解：<br>在SVD的三个矩阵（假设叫1，2，3）中，1-2表示用户降维转换，2-3表示物品降维转换，在用户推荐那个场景，由于是使用基于物品的推荐，要计算两物品的相似度，则可理解为只需要用户角度降维，即1-2的转换。即有</p>\n<script type=\"math/tex; mode=display\">U_k \\Sigma V=> A^T U_k \\Sigma</script><p>推导点：<strong>如如此在用户上降维，那是否可以直接通过PCA在用户角度降维？</strong></p>\n<h3 id=\"关联分析\"><a href=\"#关联分析\" class=\"headerlink\" title=\"关联分析\"></a>关联分析</h3><p>支持度，可信度</p>\n<ul>\n<li>Apriori</li>\n</ul>\n<h3 id=\"附加篇一\"><a href=\"#附加篇一\" class=\"headerlink\" title=\"附加篇一\"></a>附加篇一</h3><blockquote>\n<p>张同学的问题：SVM中，如下代码中权重 $w$ 为何不能做特征的权重值。</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calcWs</span><span class=\"params\">(alphas, dataArr, classLabels)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    输入：alphas, 数据集, 类别标签</span></span><br><span class=\"line\"><span class=\"string\">    输出：目标w</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    X = mat(dataArr)</span><br><span class=\"line\">    labelMat = mat(classLabels).transpose()</span><br><span class=\"line\">    m, n = shape(X)</span><br><span class=\"line\">    w = zeros((n, <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(m):</span><br><span class=\"line\">        w += multiply(alphas[i] * labelMat[i], X[i, :].T)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> w</span><br></pre></td></tr></table></figure>\n<p>对比线性情况而言,有 $w= \\Sigma \\alpha y_i x_i$ ，书中有下代码<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w += multiply(alphas[i] * labelMat[i], X[i, :].T)</span><br></pre></td></tr></table></figure></p>\n<p>目标函数为：$y=(\\Sigma \\alpha y_i x_i) x + b$ 而对于映射核空间的情况，书中有描述<br><img src=\"http://p15i7i801.bkt.clouddn.com/c9fb298cf712884a5ad1c6d2407c4b95.png\" alt=\"aiml-weightbook\"><br>此时目标函数则为 $y=\\Sigma \\alpha y_i k(\\phi(x_i),\\phi(x_j))+b$。显然，特征向量 $w$ 是无法拆出来的。</p>\n<blockquote>\n<p>对weights = weights + alpha <em> dataMatrix.transpose() </em> error 这一行的理解</p>\n</blockquote>\n<ul>\n<li>角度一：<a href=\"http://blog.csdn.net/lu597203933/article/details/38468303\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/lu597203933/article/details/38468303</a>  AndrewNg<br><img src=\"http://p15i7i801.bkt.clouddn.com/79f4eb72472afa25f82690b0823b603d.png\" alt=\"aiml-weight\"></li>\n<li>角度二：矩阵微分的角度理解 <a href=\"http://blog.csdn.net/aichipmunk/article/details/9382503\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/aichipmunk/article/details/9382503</a></li>\n</ul>\n<h3 id=\"附加篇二：机器学习算法路径图\"><a href=\"#附加篇二：机器学习算法路径图\" class=\"headerlink\" title=\"附加篇二：机器学习算法路径图\"></a>附加篇二：机器学习算法路径图</h3><p><img src=\"http://p15i7i801.bkt.clouddn.com/e161673fc2063c42f4d407fd7057fc1f.png\" alt=\"aiml-algorithm chea-sheet\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/daa070eafedebe29f976a12a0b65b08c.png\" alt=\"aiml-td\"></p>\n"},{"title":"卷积神经网络","date":"2018-01-15T13:09:47.000Z","mathjax":true,"_content":"\n# 卷积神经网络\n\n## 第一周 卷积神经网络\n### 计算机视觉\n* 图像分类\n* 目标检测\n* 衍生的新的艺术形式：风格迁移\n* 以上处理参数巨大，难以有足够的数据防止过拟合，所以需要引入卷积\n\n### 边缘检测\n* 使用卷积进行边缘检测的示例\n![ng_edgedetect](http://p15i7i801.bkt.clouddn.com/d62af4f28a74993ca8203e2bc49724f7.png)\n![ng_edetect](http://p15i7i801.bkt.clouddn.com/a5e08bd2c256d7f8869b1529f39f067b.png)\n* 卷积核或者说过滤器的选择很多文献讨论过，除了常见的过滤器，如下图的边缘检测，过滤器也可以做为参数学习出来\n![ng_filter](http://p15i7i801.bkt.clouddn.com/f1ed787d1a20b5960ed419dd5d60c69e.png)\n\n### 卷积神经网络\n以下$n$为图像大小，$f$为filter大小，$p$为padding大小\n* valid: $n-f+1$为过滤后的大小, 这样输出**图像会越来越小,而且会丢掉边缘位置**，因此引入padding使用输出图像大小\n* same: $n+2p-f+1=n$, $p=(f-1)/2$，因此$f$为奇数时，便可以选择相应大小的padding填充尺寸，所以在计算机视觉中很少看到偶数尺寸的过滤器，除非使用不自然的不对称的填充，而且用奇数过滤器有个中心便于指出它的位置.因此，建议使用*奇数过滤器*。\n* 若加入步伐stride,则输出尺寸为\n![ng_stride](http://p15i7i801.bkt.clouddn.com/6ae57951570e44ca6b74003fe54cde53.png)\n* 多个过滤器，一个过滤器通常一个二维输出（即使有多个通道，但过滤规则就一个，比如通常用红的横向边缘检测），但可以用多个过滤器去卷之，这样同样能得到一个输出立方体。注意过滤器的长宽可以不一致。\n![ng_multifilter](http://p15i7i801.bkt.clouddn.com/9ad4f66cd2080d4086a0acec0852eda9.png)\n* 单层卷积，由超参过滤器大小f，步伐s,Padding p决定下层的大小和过滤数的个数filters决定下一层的深度，即通道数，最后一层所有参数作为输入经激活函数得出结果。\n![ng_conv](http://p15i7i801.bkt.clouddn.com/0cb72ff62d4c89ad9b8136f2e0db0331.png)\n* 池化，一般有最大池化和平均池化，前者后普遍使用，经常使用超参f=2,s=2，输出大小 正好减半，也有f=3,s=2或f=3,s=1。至少为什么用池化，总之在实验室效果很好，直观上在某个区域只要获取特征是否存在，存在取最大值。同时池化基本不用padding，后面会讲。\n* 参考LeNet5的一个完整的CNN示例，关注以下几点\n  * 在这里把CONV-POOL作为一层，POOL层无参数，但有的文献算为两层\n  * 超参的选择最好是参考论文，比如S=F=2，大小正好减半\n  * 这里的全连接层每一层400，第二层120，百度原因全连接在于对细化特征的重新分类，参考知乎上的说法，\n  * 这里的全连接层每一层400，第二层120，百度原因全连接在于对细化特征的重新分类，[参考知乎](https://www.zhihu.com/question/41037974)上的说法，近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP，即卷积核为1*1）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。需要指出的是，用GAP替代FC的网络通常有较好的预测性能，它减少了参数和过拟合。\n  * 参数表下图，有二特点，一输出维度越来越小；二参数，输入没有参数，第一层卷积，卷积核参数5*5*8+8（BIAS）=208个参数，池化层无参数，第\n![ng_cnn](http://p15i7i801.bkt.clouddn.com/f6c006b85ade925f134b42d60bb13cab.png)\n![ng_cnn_1](http://p15i7i801.bkt.clouddn.com/db0d8e0123bc420d34c9a6705ac60b13.png)\n* 为什么用卷积：参数共享+稀疏连接，前者是建立在对输入不同区域其特征检测方法仍然适用，正如很多人知道的，CNN擅于抓取具有平移不变性的特征\n* 在建立完成CNN后，定义好损失函数，我们就可以用梯度下降或RMSPRO等等方法优化神经网络的参数了\n\n## 第二周 深度卷积神经网络\n### 实例研究\n#### LeNet-5\n* LeCun et al., 1988. Gradient-based learning applied to document recognition\n* 由于两个全连接，参数在6万左右\n* 数据大小逐渐减少，通道数逐渐增多\n* 过程为：输入——卷积——池化——卷积——池化——全连接——全连接——softmax\n* 激活函数用的是sigmoid\\tanh\n* 如下图：\n![ng_LeNet-5](http://p15i7i801.bkt.clouddn.com/f1c276be3dd216d841203567c98c4b17.png)\n#### AlexNet\n* Krizhevsky et al., 2012. ImageNet classification with deep convolutional neural network\n* 结构和LeNet很像，只不过参数变成了6千万\n* 使用了Relu\n* 使用了多个GPU，并且特别研究了如何将计算量分布到多个GPU上面\n![ng_AlexNet](http://p15i7i801.bkt.clouddn.com/7b6475049b5a00feb38029552230eb9c.png)\n#### VGG\n#### ResNet,152层\n#### Inception\n","source":"_posts/卷积神经网络.md","raw":"---\ntitle: 卷积神经网络\ndate: 2018-01-15 21:09:47\ntags:\n      - 深度学习\n      - Andrew NG\n      - 公开课\ncategories: AI梦\nmathjax: true\n---\n\n# 卷积神经网络\n\n## 第一周 卷积神经网络\n### 计算机视觉\n* 图像分类\n* 目标检测\n* 衍生的新的艺术形式：风格迁移\n* 以上处理参数巨大，难以有足够的数据防止过拟合，所以需要引入卷积\n\n### 边缘检测\n* 使用卷积进行边缘检测的示例\n![ng_edgedetect](http://p15i7i801.bkt.clouddn.com/d62af4f28a74993ca8203e2bc49724f7.png)\n![ng_edetect](http://p15i7i801.bkt.clouddn.com/a5e08bd2c256d7f8869b1529f39f067b.png)\n* 卷积核或者说过滤器的选择很多文献讨论过，除了常见的过滤器，如下图的边缘检测，过滤器也可以做为参数学习出来\n![ng_filter](http://p15i7i801.bkt.clouddn.com/f1ed787d1a20b5960ed419dd5d60c69e.png)\n\n### 卷积神经网络\n以下$n$为图像大小，$f$为filter大小，$p$为padding大小\n* valid: $n-f+1$为过滤后的大小, 这样输出**图像会越来越小,而且会丢掉边缘位置**，因此引入padding使用输出图像大小\n* same: $n+2p-f+1=n$, $p=(f-1)/2$，因此$f$为奇数时，便可以选择相应大小的padding填充尺寸，所以在计算机视觉中很少看到偶数尺寸的过滤器，除非使用不自然的不对称的填充，而且用奇数过滤器有个中心便于指出它的位置.因此，建议使用*奇数过滤器*。\n* 若加入步伐stride,则输出尺寸为\n![ng_stride](http://p15i7i801.bkt.clouddn.com/6ae57951570e44ca6b74003fe54cde53.png)\n* 多个过滤器，一个过滤器通常一个二维输出（即使有多个通道，但过滤规则就一个，比如通常用红的横向边缘检测），但可以用多个过滤器去卷之，这样同样能得到一个输出立方体。注意过滤器的长宽可以不一致。\n![ng_multifilter](http://p15i7i801.bkt.clouddn.com/9ad4f66cd2080d4086a0acec0852eda9.png)\n* 单层卷积，由超参过滤器大小f，步伐s,Padding p决定下层的大小和过滤数的个数filters决定下一层的深度，即通道数，最后一层所有参数作为输入经激活函数得出结果。\n![ng_conv](http://p15i7i801.bkt.clouddn.com/0cb72ff62d4c89ad9b8136f2e0db0331.png)\n* 池化，一般有最大池化和平均池化，前者后普遍使用，经常使用超参f=2,s=2，输出大小 正好减半，也有f=3,s=2或f=3,s=1。至少为什么用池化，总之在实验室效果很好，直观上在某个区域只要获取特征是否存在，存在取最大值。同时池化基本不用padding，后面会讲。\n* 参考LeNet5的一个完整的CNN示例，关注以下几点\n  * 在这里把CONV-POOL作为一层，POOL层无参数，但有的文献算为两层\n  * 超参的选择最好是参考论文，比如S=F=2，大小正好减半\n  * 这里的全连接层每一层400，第二层120，百度原因全连接在于对细化特征的重新分类，参考知乎上的说法，\n  * 这里的全连接层每一层400，第二层120，百度原因全连接在于对细化特征的重新分类，[参考知乎](https://www.zhihu.com/question/41037974)上的说法，近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP，即卷积核为1*1）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。需要指出的是，用GAP替代FC的网络通常有较好的预测性能，它减少了参数和过拟合。\n  * 参数表下图，有二特点，一输出维度越来越小；二参数，输入没有参数，第一层卷积，卷积核参数5*5*8+8（BIAS）=208个参数，池化层无参数，第\n![ng_cnn](http://p15i7i801.bkt.clouddn.com/f6c006b85ade925f134b42d60bb13cab.png)\n![ng_cnn_1](http://p15i7i801.bkt.clouddn.com/db0d8e0123bc420d34c9a6705ac60b13.png)\n* 为什么用卷积：参数共享+稀疏连接，前者是建立在对输入不同区域其特征检测方法仍然适用，正如很多人知道的，CNN擅于抓取具有平移不变性的特征\n* 在建立完成CNN后，定义好损失函数，我们就可以用梯度下降或RMSPRO等等方法优化神经网络的参数了\n\n## 第二周 深度卷积神经网络\n### 实例研究\n#### LeNet-5\n* LeCun et al., 1988. Gradient-based learning applied to document recognition\n* 由于两个全连接，参数在6万左右\n* 数据大小逐渐减少，通道数逐渐增多\n* 过程为：输入——卷积——池化——卷积——池化——全连接——全连接——softmax\n* 激活函数用的是sigmoid\\tanh\n* 如下图：\n![ng_LeNet-5](http://p15i7i801.bkt.clouddn.com/f1c276be3dd216d841203567c98c4b17.png)\n#### AlexNet\n* Krizhevsky et al., 2012. ImageNet classification with deep convolutional neural network\n* 结构和LeNet很像，只不过参数变成了6千万\n* 使用了Relu\n* 使用了多个GPU，并且特别研究了如何将计算量分布到多个GPU上面\n![ng_AlexNet](http://p15i7i801.bkt.clouddn.com/7b6475049b5a00feb38029552230eb9c.png)\n#### VGG\n#### ResNet,152层\n#### Inception\n","slug":"卷积神经网络","published":1,"updated":"2018-01-28T13:36:54.000Z","_id":"cjclxs5hj000dajslb0bif253","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"卷积神经网络\"><a href=\"#卷积神经网络\" class=\"headerlink\" title=\"卷积神经网络\"></a>卷积神经网络</h1><h2 id=\"第一周-卷积神经网络\"><a href=\"#第一周-卷积神经网络\" class=\"headerlink\" title=\"第一周 卷积神经网络\"></a>第一周 卷积神经网络</h2><h3 id=\"计算机视觉\"><a href=\"#计算机视觉\" class=\"headerlink\" title=\"计算机视觉\"></a>计算机视觉</h3><ul>\n<li>图像分类</li>\n<li>目标检测</li>\n<li>衍生的新的艺术形式：风格迁移</li>\n<li>以上处理参数巨大，难以有足够的数据防止过拟合，所以需要引入卷积</li>\n</ul>\n<h3 id=\"边缘检测\"><a href=\"#边缘检测\" class=\"headerlink\" title=\"边缘检测\"></a>边缘检测</h3><ul>\n<li>使用卷积进行边缘检测的示例<br><img src=\"http://p15i7i801.bkt.clouddn.com/d62af4f28a74993ca8203e2bc49724f7.png\" alt=\"ng_edgedetect\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/a5e08bd2c256d7f8869b1529f39f067b.png\" alt=\"ng_edetect\"></li>\n<li>卷积核或者说过滤器的选择很多文献讨论过，除了常见的过滤器，如下图的边缘检测，过滤器也可以做为参数学习出来<br><img src=\"http://p15i7i801.bkt.clouddn.com/f1ed787d1a20b5960ed419dd5d60c69e.png\" alt=\"ng_filter\"></li>\n</ul>\n<h3 id=\"卷积神经网络-1\"><a href=\"#卷积神经网络-1\" class=\"headerlink\" title=\"卷积神经网络\"></a>卷积神经网络</h3><p>以下$n$为图像大小，$f$为filter大小，$p$为padding大小</p>\n<ul>\n<li>valid: $n-f+1$为过滤后的大小, 这样输出<strong>图像会越来越小,而且会丢掉边缘位置</strong>，因此引入padding使用输出图像大小</li>\n<li>same: $n+2p-f+1=n$, $p=(f-1)/2$，因此$f$为奇数时，便可以选择相应大小的padding填充尺寸，所以在计算机视觉中很少看到偶数尺寸的过滤器，除非使用不自然的不对称的填充，而且用奇数过滤器有个中心便于指出它的位置.因此，建议使用<em>奇数过滤器</em>。</li>\n<li>若加入步伐stride,则输出尺寸为<br><img src=\"http://p15i7i801.bkt.clouddn.com/6ae57951570e44ca6b74003fe54cde53.png\" alt=\"ng_stride\"></li>\n<li>多个过滤器，一个过滤器通常一个二维输出（即使有多个通道，但过滤规则就一个，比如通常用红的横向边缘检测），但可以用多个过滤器去卷之，这样同样能得到一个输出立方体。注意过滤器的长宽可以不一致。<br><img src=\"http://p15i7i801.bkt.clouddn.com/9ad4f66cd2080d4086a0acec0852eda9.png\" alt=\"ng_multifilter\"></li>\n<li>单层卷积，由超参过滤器大小f，步伐s,Padding p决定下层的大小和过滤数的个数filters决定下一层的深度，即通道数，最后一层所有参数作为输入经激活函数得出结果。<br><img src=\"http://p15i7i801.bkt.clouddn.com/0cb72ff62d4c89ad9b8136f2e0db0331.png\" alt=\"ng_conv\"></li>\n<li>池化，一般有最大池化和平均池化，前者后普遍使用，经常使用超参f=2,s=2，输出大小 正好减半，也有f=3,s=2或f=3,s=1。至少为什么用池化，总之在实验室效果很好，直观上在某个区域只要获取特征是否存在，存在取最大值。同时池化基本不用padding，后面会讲。</li>\n<li>参考LeNet5的一个完整的CNN示例，关注以下几点<ul>\n<li>在这里把CONV-POOL作为一层，POOL层无参数，但有的文献算为两层</li>\n<li>超参的选择最好是参考论文，比如S=F=2，大小正好减半</li>\n<li>这里的全连接层每一层400，第二层120，百度原因全连接在于对细化特征的重新分类，参考知乎上的说法，</li>\n<li>这里的全连接层每一层400，第二层120，百度原因全连接在于对细化特征的重新分类，<a href=\"https://www.zhihu.com/question/41037974\" target=\"_blank\" rel=\"noopener\">参考知乎</a>上的说法，近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP，即卷积核为1*1）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。需要指出的是，用GAP替代FC的网络通常有较好的预测性能，它减少了参数和过拟合。</li>\n<li>参数表下图，有二特点，一输出维度越来越小；二参数，输入没有参数，第一层卷积，卷积核参数5<em>5</em>8+8（BIAS）=208个参数，池化层无参数，第<br><img src=\"http://p15i7i801.bkt.clouddn.com/f6c006b85ade925f134b42d60bb13cab.png\" alt=\"ng_cnn\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/db0d8e0123bc420d34c9a6705ac60b13.png\" alt=\"ng_cnn_1\"></li>\n</ul>\n</li>\n<li>为什么用卷积：参数共享+稀疏连接，前者是建立在对输入不同区域其特征检测方法仍然适用，正如很多人知道的，CNN擅于抓取具有平移不变性的特征</li>\n<li>在建立完成CNN后，定义好损失函数，我们就可以用梯度下降或RMSPRO等等方法优化神经网络的参数了</li>\n</ul>\n<h2 id=\"第二周-深度卷积神经网络\"><a href=\"#第二周-深度卷积神经网络\" class=\"headerlink\" title=\"第二周 深度卷积神经网络\"></a>第二周 深度卷积神经网络</h2><h3 id=\"实例研究\"><a href=\"#实例研究\" class=\"headerlink\" title=\"实例研究\"></a>实例研究</h3><h4 id=\"LeNet-5\"><a href=\"#LeNet-5\" class=\"headerlink\" title=\"LeNet-5\"></a>LeNet-5</h4><ul>\n<li>LeCun et al., 1988. Gradient-based learning applied to document recognition</li>\n<li>由于两个全连接，参数在6万左右</li>\n<li>数据大小逐渐减少，通道数逐渐增多</li>\n<li>过程为：输入——卷积——池化——卷积——池化——全连接——全连接——softmax</li>\n<li>激活函数用的是sigmoid\\tanh</li>\n<li>如下图：<br><img src=\"http://p15i7i801.bkt.clouddn.com/f1c276be3dd216d841203567c98c4b17.png\" alt=\"ng_LeNet-5\"><h4 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h4></li>\n<li>Krizhevsky et al., 2012. ImageNet classification with deep convolutional neural network</li>\n<li>结构和LeNet很像，只不过参数变成了6千万</li>\n<li>使用了Relu</li>\n<li>使用了多个GPU，并且特别研究了如何将计算量分布到多个GPU上面<br><img src=\"http://p15i7i801.bkt.clouddn.com/7b6475049b5a00feb38029552230eb9c.png\" alt=\"ng_AlexNet\"><h4 id=\"VGG\"><a href=\"#VGG\" class=\"headerlink\" title=\"VGG\"></a>VGG</h4><h4 id=\"ResNet-152层\"><a href=\"#ResNet-152层\" class=\"headerlink\" title=\"ResNet,152层\"></a>ResNet,152层</h4><h4 id=\"Inception\"><a href=\"#Inception\" class=\"headerlink\" title=\"Inception\"></a>Inception</h4></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"卷积神经网络\"><a href=\"#卷积神经网络\" class=\"headerlink\" title=\"卷积神经网络\"></a>卷积神经网络</h1><h2 id=\"第一周-卷积神经网络\"><a href=\"#第一周-卷积神经网络\" class=\"headerlink\" title=\"第一周 卷积神经网络\"></a>第一周 卷积神经网络</h2><h3 id=\"计算机视觉\"><a href=\"#计算机视觉\" class=\"headerlink\" title=\"计算机视觉\"></a>计算机视觉</h3><ul>\n<li>图像分类</li>\n<li>目标检测</li>\n<li>衍生的新的艺术形式：风格迁移</li>\n<li>以上处理参数巨大，难以有足够的数据防止过拟合，所以需要引入卷积</li>\n</ul>\n<h3 id=\"边缘检测\"><a href=\"#边缘检测\" class=\"headerlink\" title=\"边缘检测\"></a>边缘检测</h3><ul>\n<li>使用卷积进行边缘检测的示例<br><img src=\"http://p15i7i801.bkt.clouddn.com/d62af4f28a74993ca8203e2bc49724f7.png\" alt=\"ng_edgedetect\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/a5e08bd2c256d7f8869b1529f39f067b.png\" alt=\"ng_edetect\"></li>\n<li>卷积核或者说过滤器的选择很多文献讨论过，除了常见的过滤器，如下图的边缘检测，过滤器也可以做为参数学习出来<br><img src=\"http://p15i7i801.bkt.clouddn.com/f1ed787d1a20b5960ed419dd5d60c69e.png\" alt=\"ng_filter\"></li>\n</ul>\n<h3 id=\"卷积神经网络-1\"><a href=\"#卷积神经网络-1\" class=\"headerlink\" title=\"卷积神经网络\"></a>卷积神经网络</h3><p>以下$n$为图像大小，$f$为filter大小，$p$为padding大小</p>\n<ul>\n<li>valid: $n-f+1$为过滤后的大小, 这样输出<strong>图像会越来越小,而且会丢掉边缘位置</strong>，因此引入padding使用输出图像大小</li>\n<li>same: $n+2p-f+1=n$, $p=(f-1)/2$，因此$f$为奇数时，便可以选择相应大小的padding填充尺寸，所以在计算机视觉中很少看到偶数尺寸的过滤器，除非使用不自然的不对称的填充，而且用奇数过滤器有个中心便于指出它的位置.因此，建议使用<em>奇数过滤器</em>。</li>\n<li>若加入步伐stride,则输出尺寸为<br><img src=\"http://p15i7i801.bkt.clouddn.com/6ae57951570e44ca6b74003fe54cde53.png\" alt=\"ng_stride\"></li>\n<li>多个过滤器，一个过滤器通常一个二维输出（即使有多个通道，但过滤规则就一个，比如通常用红的横向边缘检测），但可以用多个过滤器去卷之，这样同样能得到一个输出立方体。注意过滤器的长宽可以不一致。<br><img src=\"http://p15i7i801.bkt.clouddn.com/9ad4f66cd2080d4086a0acec0852eda9.png\" alt=\"ng_multifilter\"></li>\n<li>单层卷积，由超参过滤器大小f，步伐s,Padding p决定下层的大小和过滤数的个数filters决定下一层的深度，即通道数，最后一层所有参数作为输入经激活函数得出结果。<br><img src=\"http://p15i7i801.bkt.clouddn.com/0cb72ff62d4c89ad9b8136f2e0db0331.png\" alt=\"ng_conv\"></li>\n<li>池化，一般有最大池化和平均池化，前者后普遍使用，经常使用超参f=2,s=2，输出大小 正好减半，也有f=3,s=2或f=3,s=1。至少为什么用池化，总之在实验室效果很好，直观上在某个区域只要获取特征是否存在，存在取最大值。同时池化基本不用padding，后面会讲。</li>\n<li>参考LeNet5的一个完整的CNN示例，关注以下几点<ul>\n<li>在这里把CONV-POOL作为一层，POOL层无参数，但有的文献算为两层</li>\n<li>超参的选择最好是参考论文，比如S=F=2，大小正好减半</li>\n<li>这里的全连接层每一层400，第二层120，百度原因全连接在于对细化特征的重新分类，参考知乎上的说法，</li>\n<li>这里的全连接层每一层400，第二层120，百度原因全连接在于对细化特征的重新分类，<a href=\"https://www.zhihu.com/question/41037974\" target=\"_blank\" rel=\"noopener\">参考知乎</a>上的说法，近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP，即卷积核为1*1）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。需要指出的是，用GAP替代FC的网络通常有较好的预测性能，它减少了参数和过拟合。</li>\n<li>参数表下图，有二特点，一输出维度越来越小；二参数，输入没有参数，第一层卷积，卷积核参数5<em>5</em>8+8（BIAS）=208个参数，池化层无参数，第<br><img src=\"http://p15i7i801.bkt.clouddn.com/f6c006b85ade925f134b42d60bb13cab.png\" alt=\"ng_cnn\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/db0d8e0123bc420d34c9a6705ac60b13.png\" alt=\"ng_cnn_1\"></li>\n</ul>\n</li>\n<li>为什么用卷积：参数共享+稀疏连接，前者是建立在对输入不同区域其特征检测方法仍然适用，正如很多人知道的，CNN擅于抓取具有平移不变性的特征</li>\n<li>在建立完成CNN后，定义好损失函数，我们就可以用梯度下降或RMSPRO等等方法优化神经网络的参数了</li>\n</ul>\n<h2 id=\"第二周-深度卷积神经网络\"><a href=\"#第二周-深度卷积神经网络\" class=\"headerlink\" title=\"第二周 深度卷积神经网络\"></a>第二周 深度卷积神经网络</h2><h3 id=\"实例研究\"><a href=\"#实例研究\" class=\"headerlink\" title=\"实例研究\"></a>实例研究</h3><h4 id=\"LeNet-5\"><a href=\"#LeNet-5\" class=\"headerlink\" title=\"LeNet-5\"></a>LeNet-5</h4><ul>\n<li>LeCun et al., 1988. Gradient-based learning applied to document recognition</li>\n<li>由于两个全连接，参数在6万左右</li>\n<li>数据大小逐渐减少，通道数逐渐增多</li>\n<li>过程为：输入——卷积——池化——卷积——池化——全连接——全连接——softmax</li>\n<li>激活函数用的是sigmoid\\tanh</li>\n<li>如下图：<br><img src=\"http://p15i7i801.bkt.clouddn.com/f1c276be3dd216d841203567c98c4b17.png\" alt=\"ng_LeNet-5\"><h4 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h4></li>\n<li>Krizhevsky et al., 2012. ImageNet classification with deep convolutional neural network</li>\n<li>结构和LeNet很像，只不过参数变成了6千万</li>\n<li>使用了Relu</li>\n<li>使用了多个GPU，并且特别研究了如何将计算量分布到多个GPU上面<br><img src=\"http://p15i7i801.bkt.clouddn.com/7b6475049b5a00feb38029552230eb9c.png\" alt=\"ng_AlexNet\"><h4 id=\"VGG\"><a href=\"#VGG\" class=\"headerlink\" title=\"VGG\"></a>VGG</h4><h4 id=\"ResNet-152层\"><a href=\"#ResNet-152层\" class=\"headerlink\" title=\"ResNet,152层\"></a>ResNet,152层</h4><h4 id=\"Inception\"><a href=\"#Inception\" class=\"headerlink\" title=\"Inception\"></a>Inception</h4></li>\n</ul>\n"},{"title":"神经网络与深度学习","date":"2017-08-30T09:20:41.000Z","mathjax":true,"_content":"\n\n### 第一、二周：概论与基础\n> sigmoid在逻辑回归里为什么没用被ReLU替代\n\n  * sigmoid在感知器结果输出里可以将任意输入连接的映射到 $[0,1]$ 之间，且结果保持连续\n  * ReLu作为深度学习引入的激活函数，特性在于稀疏的激活性、单侧抑制与相对宽的兴奋边界\n\n> LR中损失函数不用平方误差，为什么它是非凸的。\n\n* 因为加入了非线性的Sigmoid函数(注意，若是纯线性的，使用最小二乘可直接估计 $w$)，它使得平方误差损失函数是非凸的，而以下损失函数定义（极大似然的思想）没有这个问题，而且它易于求导的特点决定了使用梯度下降来求极值更方便[**在无约束优化问题中，对目标函数直接微分的方法一般难以求解，故常用迭代的方法**]：\n$$ da/dz = z(1-z), dL/dz= a - y \\\\  \\mbox{if } L=-( y log a + (1-y)log(1-a) ) $$\n![ng_gradientdescent](http://p15i7i801.bkt.clouddn.com/e70536dc3cdac781213e6e86a904d5ac.png)\n\n> 矩阵运算Numpy的高效\n\n* 避免矩阵运算使用For循环，100m时差300倍效率，因为 numpy使用到了CPU\\GPU的SIMD指令，运行速度更快。同时，对于numpy有内置的函数也一样，尽量使用，如log,max  etc..\n* 在python中，z= np.dot(w.T, x) + b , b会自动扩展成一个n维向量，这在python称为broadcast\n![ng_broadcast](http://p15i7i801.bkt.clouddn.com/849e2ed8c823392209bd02a6e7397727.png)\n* 在使用的过程中，reshape是o(1)操作，多多益善，尽量不使用rank为1的数组。\n\n### 第三周 浅层神经网络\n* 二级神经网络，即只有一个隐藏层的神经网络。\n*  输出层通常使用sigmoid函数，而隐藏层不用，一般至少使用tanh来代替，因为它的输出介于-1与1之间，这样均值为0，便于下一层的计算。考虑到梯度下降，sigmoid与tanh在最大小值处导数变化小，reLU及leaky reLU更常用。当然这一切根据实际情况调整。\n*  隐藏层不应使用线性激活函数，若用则相当于没有隐藏层，输出层若要出回归值，则可使用。\n*  随机w,并乘以较小的系数，一般让随机参数比较小。\n\n### 第四周 深层神经网络\n![ng_forwardandbackward](http://p15i7i801.bkt.clouddn.com/44daf85883f94f8f64ccac3f651cfefd.png)\n\n####  hyper parameter——控制参数的参数\n  * 层数\n  * 迭代次数\n  * 下降速率\n  * 激活函数的选择\n  * batch size\n\n####  parameter\n  * 权重 $w$\n  * 偏移量 $b$\n","source":"_posts/神经网络与深度学习.md","raw":"---\ntitle: 神经网络与深度学习\ndate: 2017-08-30 17:20:41\ntags:\n      - 深度学习\n      - Andrew NG\n      - 公开课\ncategories: AI梦\nmathjax: true\n---\n\n\n### 第一、二周：概论与基础\n> sigmoid在逻辑回归里为什么没用被ReLU替代\n\n  * sigmoid在感知器结果输出里可以将任意输入连接的映射到 $[0,1]$ 之间，且结果保持连续\n  * ReLu作为深度学习引入的激活函数，特性在于稀疏的激活性、单侧抑制与相对宽的兴奋边界\n\n> LR中损失函数不用平方误差，为什么它是非凸的。\n\n* 因为加入了非线性的Sigmoid函数(注意，若是纯线性的，使用最小二乘可直接估计 $w$)，它使得平方误差损失函数是非凸的，而以下损失函数定义（极大似然的思想）没有这个问题，而且它易于求导的特点决定了使用梯度下降来求极值更方便[**在无约束优化问题中，对目标函数直接微分的方法一般难以求解，故常用迭代的方法**]：\n$$ da/dz = z(1-z), dL/dz= a - y \\\\  \\mbox{if } L=-( y log a + (1-y)log(1-a) ) $$\n![ng_gradientdescent](http://p15i7i801.bkt.clouddn.com/e70536dc3cdac781213e6e86a904d5ac.png)\n\n> 矩阵运算Numpy的高效\n\n* 避免矩阵运算使用For循环，100m时差300倍效率，因为 numpy使用到了CPU\\GPU的SIMD指令，运行速度更快。同时，对于numpy有内置的函数也一样，尽量使用，如log,max  etc..\n* 在python中，z= np.dot(w.T, x) + b , b会自动扩展成一个n维向量，这在python称为broadcast\n![ng_broadcast](http://p15i7i801.bkt.clouddn.com/849e2ed8c823392209bd02a6e7397727.png)\n* 在使用的过程中，reshape是o(1)操作，多多益善，尽量不使用rank为1的数组。\n\n### 第三周 浅层神经网络\n* 二级神经网络，即只有一个隐藏层的神经网络。\n*  输出层通常使用sigmoid函数，而隐藏层不用，一般至少使用tanh来代替，因为它的输出介于-1与1之间，这样均值为0，便于下一层的计算。考虑到梯度下降，sigmoid与tanh在最大小值处导数变化小，reLU及leaky reLU更常用。当然这一切根据实际情况调整。\n*  隐藏层不应使用线性激活函数，若用则相当于没有隐藏层，输出层若要出回归值，则可使用。\n*  随机w,并乘以较小的系数，一般让随机参数比较小。\n\n### 第四周 深层神经网络\n![ng_forwardandbackward](http://p15i7i801.bkt.clouddn.com/44daf85883f94f8f64ccac3f651cfefd.png)\n\n####  hyper parameter——控制参数的参数\n  * 层数\n  * 迭代次数\n  * 下降速率\n  * 激活函数的选择\n  * batch size\n\n####  parameter\n  * 权重 $w$\n  * 偏移量 $b$\n","slug":"神经网络与深度学习","published":1,"updated":"2018-01-20T13:59:10.000Z","_id":"cjclxs5hq000hajslfndxkf2b","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"第一、二周：概论与基础\"><a href=\"#第一、二周：概论与基础\" class=\"headerlink\" title=\"第一、二周：概论与基础\"></a>第一、二周：概论与基础</h3><blockquote>\n<p>sigmoid在逻辑回归里为什么没用被ReLU替代</p>\n</blockquote>\n<ul>\n<li>sigmoid在感知器结果输出里可以将任意输入连接的映射到 $[0,1]$ 之间，且结果保持连续</li>\n<li>ReLu作为深度学习引入的激活函数，特性在于稀疏的激活性、单侧抑制与相对宽的兴奋边界</li>\n</ul>\n<blockquote>\n<p>LR中损失函数不用平方误差，为什么它是非凸的。</p>\n</blockquote>\n<ul>\n<li>因为加入了非线性的Sigmoid函数(注意，若是纯线性的，使用最小二乘可直接估计 $w$)，它使得平方误差损失函数是非凸的，而以下损失函数定义（极大似然的思想）没有这个问题，而且它易于求导的特点决定了使用梯度下降来求极值更方便[<strong>在无约束优化问题中，对目标函数直接微分的方法一般难以求解，故常用迭代的方法</strong>]：<script type=\"math/tex; mode=display\">da/dz = z(1-z), dL/dz= a - y \\\\  \\mbox{if } L=-( y log a + (1-y)log(1-a) )</script><img src=\"http://p15i7i801.bkt.clouddn.com/e70536dc3cdac781213e6e86a904d5ac.png\" alt=\"ng_gradientdescent\"></li>\n</ul>\n<blockquote>\n<p>矩阵运算Numpy的高效</p>\n</blockquote>\n<ul>\n<li>避免矩阵运算使用For循环，100m时差300倍效率，因为 numpy使用到了CPU\\GPU的SIMD指令，运行速度更快。同时，对于numpy有内置的函数也一样，尽量使用，如log,max  etc..</li>\n<li>在python中，z= np.dot(w.T, x) + b , b会自动扩展成一个n维向量，这在python称为broadcast<br><img src=\"http://p15i7i801.bkt.clouddn.com/849e2ed8c823392209bd02a6e7397727.png\" alt=\"ng_broadcast\"></li>\n<li>在使用的过程中，reshape是o(1)操作，多多益善，尽量不使用rank为1的数组。</li>\n</ul>\n<h3 id=\"第三周-浅层神经网络\"><a href=\"#第三周-浅层神经网络\" class=\"headerlink\" title=\"第三周 浅层神经网络\"></a>第三周 浅层神经网络</h3><ul>\n<li>二级神经网络，即只有一个隐藏层的神经网络。</li>\n<li>输出层通常使用sigmoid函数，而隐藏层不用，一般至少使用tanh来代替，因为它的输出介于-1与1之间，这样均值为0，便于下一层的计算。考虑到梯度下降，sigmoid与tanh在最大小值处导数变化小，reLU及leaky reLU更常用。当然这一切根据实际情况调整。</li>\n<li>隐藏层不应使用线性激活函数，若用则相当于没有隐藏层，输出层若要出回归值，则可使用。</li>\n<li>随机w,并乘以较小的系数，一般让随机参数比较小。</li>\n</ul>\n<h3 id=\"第四周-深层神经网络\"><a href=\"#第四周-深层神经网络\" class=\"headerlink\" title=\"第四周 深层神经网络\"></a>第四周 深层神经网络</h3><p><img src=\"http://p15i7i801.bkt.clouddn.com/44daf85883f94f8f64ccac3f651cfefd.png\" alt=\"ng_forwardandbackward\"></p>\n<h4 id=\"hyper-parameter——控制参数的参数\"><a href=\"#hyper-parameter——控制参数的参数\" class=\"headerlink\" title=\"hyper parameter——控制参数的参数\"></a>hyper parameter——控制参数的参数</h4><ul>\n<li>层数</li>\n<li>迭代次数</li>\n<li>下降速率</li>\n<li>激活函数的选择</li>\n<li>batch size</li>\n</ul>\n<h4 id=\"parameter\"><a href=\"#parameter\" class=\"headerlink\" title=\"parameter\"></a>parameter</h4><ul>\n<li>权重 $w$</li>\n<li>偏移量 $b$</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"第一、二周：概论与基础\"><a href=\"#第一、二周：概论与基础\" class=\"headerlink\" title=\"第一、二周：概论与基础\"></a>第一、二周：概论与基础</h3><blockquote>\n<p>sigmoid在逻辑回归里为什么没用被ReLU替代</p>\n</blockquote>\n<ul>\n<li>sigmoid在感知器结果输出里可以将任意输入连接的映射到 $[0,1]$ 之间，且结果保持连续</li>\n<li>ReLu作为深度学习引入的激活函数，特性在于稀疏的激活性、单侧抑制与相对宽的兴奋边界</li>\n</ul>\n<blockquote>\n<p>LR中损失函数不用平方误差，为什么它是非凸的。</p>\n</blockquote>\n<ul>\n<li>因为加入了非线性的Sigmoid函数(注意，若是纯线性的，使用最小二乘可直接估计 $w$)，它使得平方误差损失函数是非凸的，而以下损失函数定义（极大似然的思想）没有这个问题，而且它易于求导的特点决定了使用梯度下降来求极值更方便[<strong>在无约束优化问题中，对目标函数直接微分的方法一般难以求解，故常用迭代的方法</strong>]：<script type=\"math/tex; mode=display\">da/dz = z(1-z), dL/dz= a - y \\\\  \\mbox{if } L=-( y log a + (1-y)log(1-a) )</script><img src=\"http://p15i7i801.bkt.clouddn.com/e70536dc3cdac781213e6e86a904d5ac.png\" alt=\"ng_gradientdescent\"></li>\n</ul>\n<blockquote>\n<p>矩阵运算Numpy的高效</p>\n</blockquote>\n<ul>\n<li>避免矩阵运算使用For循环，100m时差300倍效率，因为 numpy使用到了CPU\\GPU的SIMD指令，运行速度更快。同时，对于numpy有内置的函数也一样，尽量使用，如log,max  etc..</li>\n<li>在python中，z= np.dot(w.T, x) + b , b会自动扩展成一个n维向量，这在python称为broadcast<br><img src=\"http://p15i7i801.bkt.clouddn.com/849e2ed8c823392209bd02a6e7397727.png\" alt=\"ng_broadcast\"></li>\n<li>在使用的过程中，reshape是o(1)操作，多多益善，尽量不使用rank为1的数组。</li>\n</ul>\n<h3 id=\"第三周-浅层神经网络\"><a href=\"#第三周-浅层神经网络\" class=\"headerlink\" title=\"第三周 浅层神经网络\"></a>第三周 浅层神经网络</h3><ul>\n<li>二级神经网络，即只有一个隐藏层的神经网络。</li>\n<li>输出层通常使用sigmoid函数，而隐藏层不用，一般至少使用tanh来代替，因为它的输出介于-1与1之间，这样均值为0，便于下一层的计算。考虑到梯度下降，sigmoid与tanh在最大小值处导数变化小，reLU及leaky reLU更常用。当然这一切根据实际情况调整。</li>\n<li>隐藏层不应使用线性激活函数，若用则相当于没有隐藏层，输出层若要出回归值，则可使用。</li>\n<li>随机w,并乘以较小的系数，一般让随机参数比较小。</li>\n</ul>\n<h3 id=\"第四周-深层神经网络\"><a href=\"#第四周-深层神经网络\" class=\"headerlink\" title=\"第四周 深层神经网络\"></a>第四周 深层神经网络</h3><p><img src=\"http://p15i7i801.bkt.clouddn.com/44daf85883f94f8f64ccac3f651cfefd.png\" alt=\"ng_forwardandbackward\"></p>\n<h4 id=\"hyper-parameter——控制参数的参数\"><a href=\"#hyper-parameter——控制参数的参数\" class=\"headerlink\" title=\"hyper parameter——控制参数的参数\"></a>hyper parameter——控制参数的参数</h4><ul>\n<li>层数</li>\n<li>迭代次数</li>\n<li>下降速率</li>\n<li>激活函数的选择</li>\n<li>batch size</li>\n</ul>\n<h4 id=\"parameter\"><a href=\"#parameter\" class=\"headerlink\" title=\"parameter\"></a>parameter</h4><ul>\n<li>权重 $w$</li>\n<li>偏移量 $b$</li>\n</ul>\n"},{"title":"改善深层神经网络：超参数调试、正则化以及优化","date":"2017-09-15T09:45:29.000Z","mathjax":true,"_content":"\n\n#  第一周 深度学习的实用层面\n## 训练、验证、测试集\n* 在小数时代，比例一般为6：2：2，大数据时代可适当调整，验证及测试集为10000左右即可。\n* train set 最好和后两者有相当的分布，比如猫的问题，在网上爬的猫有许多精良的照片，而用户实际上传的可能比较糙。\n* test set是为了拿到测试的无偏估计\n* ML是一个高度迭代的过程，即使最牛的专家也是如此。\n\n## 偏差与方差\n* 偏差代表训练集的错误率有多高，方差代表dev set 和train set的错误率的差别\n\n## 正则化\nregularizer，规则化，通过先验的知识来最大化贝叶斯最大后验估计，本质上是贝叶斯最大后验估计的**似然函数**\n* L1\n* L2：Frobenius Norm，直观上Lambda越大，权重矩阵被设为零\n![nn_reg_12181821](http://p15i7i801.bkt.clouddn.com/2d81fcb8cfe1cb048929564aa91f9c4f.png)\n\n* dropout\n* 其它减少过拟合的方法\n    * Data augmentation\n    * early stop: 此种方法相对L2来说减少lambda的尝试计算量\n\n## 正则化输入（normalize)\n使用零均值，主要是用来提升速度，范围尺度类似即可，也不是一定要0-1。如下图，若不同特征相差尺度很大，则会形成如左不规则超球体，这样w的迭代次数会大大增加，对应的迭代步长也应设置偏小。\n\n## 梯度消失及爆炸\n由于层数太多，梯度指数级变化，这容易导致步长会非常小，训练时间会变得很长。传统激活函数都存在这一缺点，正向时爆炸，反向时消失，为防止上述问题，权重的初始值的选择有：\n* 若用RELU作为激活函数（[种类及特点](http://blog.csdn.net/mzpmzk/article/details/77418030)), $w$ 初始值可以使用 $np.sqrt(\\frac{2}{n^{[l-1]}+n^{[l]}})$\n![ng_vanishing_exploding_grad](http://p15i7i801.bkt.clouddn.com/1f3a7fe0374fc7a514f6c69809520867.png)\n# 第二周 优化算法\n* Min-batch梯度下降：避免需要处理所有数据才能进行下一步\n* 指数加权平均\n* 动量（Momentum）梯度下降:类似移动平均线减缓摆动\n$$ v_{dw} = \\beta v_{dw} + (1-\\beta)dw $$\n$$ v_{db}=\\beta v_{db} + (1-\\beta)db $$\n$$ w=w-\\alpha v_{dw} $$\n$$b=b-\\alpha v_{db}$$\n* RMSprop：让幅度大的参数变缓，让幅度小的参数变大\n\n$$ S_{dw} = \\beta S_{dw} + (1-\\beta){dw}^2 $$\n$$ S_{db}=\\beta S_{db} + (1-\\beta){db}^2$$\n$$ w=w-\\alpha \\frac{dw}{\\sqrt{S_{dw}}}$$\n$$ b=b-\\alpha \\frac{db}{\\sqrt{S_{db}}}$$\n* Adam(Adaptive Moment Estimation): 各算法适应于不同的神经网络结构，该算法结合Momentum and RMSprop，非常常用。\n  * 公式\n  $$ v_{dw} = \\beta v_{dw} + (1-\\beta_1)dw $$\n  $$ v_{db}=\\beta v_{db} + (1-\\beta_1)db$$\n  $$ S_{dw} = \\beta S_{dw} + (1-\\beta_2){dw}^2 $$\n  $$ S_{db}=\\beta S_{db} + (1-\\beta_2){db}^2$$\n  $$ w=w-\\alpha \\frac{v_{dw}}{\\sqrt{S_{dw}}}$$\n  $$ b=b-\\alpha \\frac{v_{db}}{\\sqrt{S_{db}}}$$\n  * 超参数一般值：\n      * $\\alpha$ : needs to be tune\n      * $\\beta_1$ : (first momentum):0.9\n      * $\\beta_2$ : (second momnetum:RMSprop):0.99\n* 学习率衰减：多个方法，如倒数、指数等等\n* 关于局部最优：在大量参数情况下可能会出现saddle问题，这样问题容易到达plateau状态，较难走出。这种可能性较小，通常使用Adam之类的优化方法解决\n\n# 第三周 超参数调度、Batch正则与程序框架\n\n## 超参数调整\n![ng_hp](http://p15i7i801.bkt.clouddn.com/149d38d31a5fa4e0576ff617c0408f08.png)\n* 首先是红色的 $\\alpha$，再者是黄色的学习率，第一、二动量及min-batch，最后再考虑调整层数及学习率延时 $\\varepsilon$。\n* 使用随机数、从粗到细\n* 使用合适的分布尺寸，而不是使用均匀分布，比如对于 $\\alpha$可以使用a log scala，举例而言：\n  ```python\n  r = -4*np.random.rand()\n  alpha = 10^r\n  ```\n\n## 实践经验：两个流派\n* 使用各领域的常见参数（**这意味需要对各个智能领域有的了解**）\n* 直觉很有效，**定期更新参数，比如几个月**\n* babysitting（pandan）: 长期照看，评估调整——（**不断的持久化模型吗？，一直迭代下去，还是每次都重新训练，模型可以以直训练下去吗？**）\n* train in paraller: Caviar(鱼子酱)\n\n## 网络内激活函数的normalizaion\n![ng_batchnorm](http://p15i7i801.bkt.clouddn.com/eaa10f3214ad4cbd03b074755a13a8e0.png)\n请注意以下三点：\n* 通过 $\\alpha、\\beta$两个超参数，可以控制 $Z^i$ 的范围，如右上图所示\n* 单层网络中我们通过normalizaion来加快梯度下降的速度，那么也可以通过控制隐藏层的输入值来控制\n* andrew的推荐: Normalize $z$ rather than $a$\n\n## 将Batch Norm拟合进神经网络\n![ng_addBatchNorm](http://p15i7i801.bkt.clouddn.com/b398a029678304574bdf4e67d9ecc406.png)\n* 偏执项b从此被标准化抵消，即式子可简化为 $z^l=w^l a^{l-1}$\n\n## Batch Norm奏效的原因\n* 加速梯度下降\n* It makes weights, later or deeper than your network, more robost to changes to weights in earlier layers of neural netowork. 举例而言，对于不带颜色的猫可能训练得很好，当带上颜色后，对于同样判断是否为猫的模型而言，输入数据的分布变化了，导致模型需要重新训练。这也称为**covariate shift**，为解决这个问题，可以使用Batch Norm。\n* 顺便的作用，如dropout，引入了噪声，相当于加入了regularization.\n\n## 测试模型时BatchNorm的计算\n* 需要估算平均值及标准差\n* 简单靠谱的处理方式是用指数加权平均去**估算**\n* 跑全量数据batchnorm也可以\n\n## Softmax分类\n逻辑回归的一般形式,对比hard max而言，比如分类结果值为 $[1,0,0,0]$ 而非 $[0.8,0.2,0,0]$\n![ng_softmax](http://p15i7i801.bkt.clouddn.com/6d6b110e436fb245d51cbf1179d231d2.png)\n\n## 理解及训练Softmax分类器\n同逻辑回归的二分类器的损失函数，这里同样使用最大似然的思想来定义损失函数，使用[常见最优化方法](https://www.cnblogs.com/maybe2030/p/4751804.html#_label1)——梯度下降法来求解模型参数 $w,b$。\n![ng_softmax_loss](http://p15i7i801.bkt.clouddn.com/4e804d7c109e9149bf95fc2df0603b14.png)\n\n## 深度学习框架\n![ng_frameworks](http://p15i7i801.bkt.clouddn.com/de6e08b5039630e827e6a07ddb979868.png)\n#### 易用\n#### 性能\n  长时间的开源，真正的开放，而不是一段时间后转为自已的服务，停止开源。\n#### 其它\n当然也取决于语言、用途（视觉、NLP或者广告）\n## Tensflow\n* 它核心在于计算损失函数并自动求导，它重载了普通的运算，所以以下是等价的：\n![ng_cost](http://p15i7i801.bkt.clouddn.com/8d8b480a53b18ff52346c3ab73387c63.png)\n* 它通过简单几句初始化建立计算图，通过placeholder喂以数据，通过自动求导计算最优cost\n![ng_computergraph](http://p15i7i801.bkt.clouddn.com/43e55f2e9f72d5d917becb7c40778281.png)\n* Python 使用with可自动清理类的显式资源，tensoflow通过tf.variable来定义求取的变量\n* 总而言之，**它的框架是计算图，核心是损失函数及对应的求解方法，辅以定义自变量、喂养数据的手段，最后运行该图**\n![ng_feed](http://p15i7i801.bkt.clouddn.com/449c3a1a770e93d143aade68d90f049f.png)\n","source":"_posts/改善深层神经网络：超参数调试、正则化以及优化.md","raw":"---\ntitle: 改善深层神经网络：超参数调试、正则化以及优化\ndate: 2017-09-15 17:45:29\ntags:\n      - 深度学习\n      - Andrew NG\n      - 公开课\ncategories: AI梦\nmathjax: true\n---\n\n\n#  第一周 深度学习的实用层面\n## 训练、验证、测试集\n* 在小数时代，比例一般为6：2：2，大数据时代可适当调整，验证及测试集为10000左右即可。\n* train set 最好和后两者有相当的分布，比如猫的问题，在网上爬的猫有许多精良的照片，而用户实际上传的可能比较糙。\n* test set是为了拿到测试的无偏估计\n* ML是一个高度迭代的过程，即使最牛的专家也是如此。\n\n## 偏差与方差\n* 偏差代表训练集的错误率有多高，方差代表dev set 和train set的错误率的差别\n\n## 正则化\nregularizer，规则化，通过先验的知识来最大化贝叶斯最大后验估计，本质上是贝叶斯最大后验估计的**似然函数**\n* L1\n* L2：Frobenius Norm，直观上Lambda越大，权重矩阵被设为零\n![nn_reg_12181821](http://p15i7i801.bkt.clouddn.com/2d81fcb8cfe1cb048929564aa91f9c4f.png)\n\n* dropout\n* 其它减少过拟合的方法\n    * Data augmentation\n    * early stop: 此种方法相对L2来说减少lambda的尝试计算量\n\n## 正则化输入（normalize)\n使用零均值，主要是用来提升速度，范围尺度类似即可，也不是一定要0-1。如下图，若不同特征相差尺度很大，则会形成如左不规则超球体，这样w的迭代次数会大大增加，对应的迭代步长也应设置偏小。\n\n## 梯度消失及爆炸\n由于层数太多，梯度指数级变化，这容易导致步长会非常小，训练时间会变得很长。传统激活函数都存在这一缺点，正向时爆炸，反向时消失，为防止上述问题，权重的初始值的选择有：\n* 若用RELU作为激活函数（[种类及特点](http://blog.csdn.net/mzpmzk/article/details/77418030)), $w$ 初始值可以使用 $np.sqrt(\\frac{2}{n^{[l-1]}+n^{[l]}})$\n![ng_vanishing_exploding_grad](http://p15i7i801.bkt.clouddn.com/1f3a7fe0374fc7a514f6c69809520867.png)\n# 第二周 优化算法\n* Min-batch梯度下降：避免需要处理所有数据才能进行下一步\n* 指数加权平均\n* 动量（Momentum）梯度下降:类似移动平均线减缓摆动\n$$ v_{dw} = \\beta v_{dw} + (1-\\beta)dw $$\n$$ v_{db}=\\beta v_{db} + (1-\\beta)db $$\n$$ w=w-\\alpha v_{dw} $$\n$$b=b-\\alpha v_{db}$$\n* RMSprop：让幅度大的参数变缓，让幅度小的参数变大\n\n$$ S_{dw} = \\beta S_{dw} + (1-\\beta){dw}^2 $$\n$$ S_{db}=\\beta S_{db} + (1-\\beta){db}^2$$\n$$ w=w-\\alpha \\frac{dw}{\\sqrt{S_{dw}}}$$\n$$ b=b-\\alpha \\frac{db}{\\sqrt{S_{db}}}$$\n* Adam(Adaptive Moment Estimation): 各算法适应于不同的神经网络结构，该算法结合Momentum and RMSprop，非常常用。\n  * 公式\n  $$ v_{dw} = \\beta v_{dw} + (1-\\beta_1)dw $$\n  $$ v_{db}=\\beta v_{db} + (1-\\beta_1)db$$\n  $$ S_{dw} = \\beta S_{dw} + (1-\\beta_2){dw}^2 $$\n  $$ S_{db}=\\beta S_{db} + (1-\\beta_2){db}^2$$\n  $$ w=w-\\alpha \\frac{v_{dw}}{\\sqrt{S_{dw}}}$$\n  $$ b=b-\\alpha \\frac{v_{db}}{\\sqrt{S_{db}}}$$\n  * 超参数一般值：\n      * $\\alpha$ : needs to be tune\n      * $\\beta_1$ : (first momentum):0.9\n      * $\\beta_2$ : (second momnetum:RMSprop):0.99\n* 学习率衰减：多个方法，如倒数、指数等等\n* 关于局部最优：在大量参数情况下可能会出现saddle问题，这样问题容易到达plateau状态，较难走出。这种可能性较小，通常使用Adam之类的优化方法解决\n\n# 第三周 超参数调度、Batch正则与程序框架\n\n## 超参数调整\n![ng_hp](http://p15i7i801.bkt.clouddn.com/149d38d31a5fa4e0576ff617c0408f08.png)\n* 首先是红色的 $\\alpha$，再者是黄色的学习率，第一、二动量及min-batch，最后再考虑调整层数及学习率延时 $\\varepsilon$。\n* 使用随机数、从粗到细\n* 使用合适的分布尺寸，而不是使用均匀分布，比如对于 $\\alpha$可以使用a log scala，举例而言：\n  ```python\n  r = -4*np.random.rand()\n  alpha = 10^r\n  ```\n\n## 实践经验：两个流派\n* 使用各领域的常见参数（**这意味需要对各个智能领域有的了解**）\n* 直觉很有效，**定期更新参数，比如几个月**\n* babysitting（pandan）: 长期照看，评估调整——（**不断的持久化模型吗？，一直迭代下去，还是每次都重新训练，模型可以以直训练下去吗？**）\n* train in paraller: Caviar(鱼子酱)\n\n## 网络内激活函数的normalizaion\n![ng_batchnorm](http://p15i7i801.bkt.clouddn.com/eaa10f3214ad4cbd03b074755a13a8e0.png)\n请注意以下三点：\n* 通过 $\\alpha、\\beta$两个超参数，可以控制 $Z^i$ 的范围，如右上图所示\n* 单层网络中我们通过normalizaion来加快梯度下降的速度，那么也可以通过控制隐藏层的输入值来控制\n* andrew的推荐: Normalize $z$ rather than $a$\n\n## 将Batch Norm拟合进神经网络\n![ng_addBatchNorm](http://p15i7i801.bkt.clouddn.com/b398a029678304574bdf4e67d9ecc406.png)\n* 偏执项b从此被标准化抵消，即式子可简化为 $z^l=w^l a^{l-1}$\n\n## Batch Norm奏效的原因\n* 加速梯度下降\n* It makes weights, later or deeper than your network, more robost to changes to weights in earlier layers of neural netowork. 举例而言，对于不带颜色的猫可能训练得很好，当带上颜色后，对于同样判断是否为猫的模型而言，输入数据的分布变化了，导致模型需要重新训练。这也称为**covariate shift**，为解决这个问题，可以使用Batch Norm。\n* 顺便的作用，如dropout，引入了噪声，相当于加入了regularization.\n\n## 测试模型时BatchNorm的计算\n* 需要估算平均值及标准差\n* 简单靠谱的处理方式是用指数加权平均去**估算**\n* 跑全量数据batchnorm也可以\n\n## Softmax分类\n逻辑回归的一般形式,对比hard max而言，比如分类结果值为 $[1,0,0,0]$ 而非 $[0.8,0.2,0,0]$\n![ng_softmax](http://p15i7i801.bkt.clouddn.com/6d6b110e436fb245d51cbf1179d231d2.png)\n\n## 理解及训练Softmax分类器\n同逻辑回归的二分类器的损失函数，这里同样使用最大似然的思想来定义损失函数，使用[常见最优化方法](https://www.cnblogs.com/maybe2030/p/4751804.html#_label1)——梯度下降法来求解模型参数 $w,b$。\n![ng_softmax_loss](http://p15i7i801.bkt.clouddn.com/4e804d7c109e9149bf95fc2df0603b14.png)\n\n## 深度学习框架\n![ng_frameworks](http://p15i7i801.bkt.clouddn.com/de6e08b5039630e827e6a07ddb979868.png)\n#### 易用\n#### 性能\n  长时间的开源，真正的开放，而不是一段时间后转为自已的服务，停止开源。\n#### 其它\n当然也取决于语言、用途（视觉、NLP或者广告）\n## Tensflow\n* 它核心在于计算损失函数并自动求导，它重载了普通的运算，所以以下是等价的：\n![ng_cost](http://p15i7i801.bkt.clouddn.com/8d8b480a53b18ff52346c3ab73387c63.png)\n* 它通过简单几句初始化建立计算图，通过placeholder喂以数据，通过自动求导计算最优cost\n![ng_computergraph](http://p15i7i801.bkt.clouddn.com/43e55f2e9f72d5d917becb7c40778281.png)\n* Python 使用with可自动清理类的显式资源，tensoflow通过tf.variable来定义求取的变量\n* 总而言之，**它的框架是计算图，核心是损失函数及对应的求解方法，辅以定义自变量、喂养数据的手段，最后运行该图**\n![ng_feed](http://p15i7i801.bkt.clouddn.com/449c3a1a770e93d143aade68d90f049f.png)\n","slug":"改善深层神经网络：超参数调试、正则化以及优化","published":1,"updated":"2018-01-08T10:35:41.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjclxs5hs000kajslkvpdc1qr","content":"<h1 id=\"第一周-深度学习的实用层面\"><a href=\"#第一周-深度学习的实用层面\" class=\"headerlink\" title=\"第一周 深度学习的实用层面\"></a>第一周 深度学习的实用层面</h1><h2 id=\"训练、验证、测试集\"><a href=\"#训练、验证、测试集\" class=\"headerlink\" title=\"训练、验证、测试集\"></a>训练、验证、测试集</h2><ul>\n<li>在小数时代，比例一般为6：2：2，大数据时代可适当调整，验证及测试集为10000左右即可。</li>\n<li>train set 最好和后两者有相当的分布，比如猫的问题，在网上爬的猫有许多精良的照片，而用户实际上传的可能比较糙。</li>\n<li>test set是为了拿到测试的无偏估计</li>\n<li>ML是一个高度迭代的过程，即使最牛的专家也是如此。</li>\n</ul>\n<h2 id=\"偏差与方差\"><a href=\"#偏差与方差\" class=\"headerlink\" title=\"偏差与方差\"></a>偏差与方差</h2><ul>\n<li>偏差代表训练集的错误率有多高，方差代表dev set 和train set的错误率的差别</li>\n</ul>\n<h2 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h2><p>regularizer，规则化，通过先验的知识来最大化贝叶斯最大后验估计，本质上是贝叶斯最大后验估计的<strong>似然函数</strong></p>\n<ul>\n<li>L1</li>\n<li><p>L2：Frobenius Norm，直观上Lambda越大，权重矩阵被设为零<br><img src=\"http://p15i7i801.bkt.clouddn.com/2d81fcb8cfe1cb048929564aa91f9c4f.png\" alt=\"nn_reg_12181821\"></p>\n</li>\n<li><p>dropout</p>\n</li>\n<li>其它减少过拟合的方法<ul>\n<li>Data augmentation</li>\n<li>early stop: 此种方法相对L2来说减少lambda的尝试计算量</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"正则化输入（normalize\"><a href=\"#正则化输入（normalize\" class=\"headerlink\" title=\"正则化输入（normalize)\"></a>正则化输入（normalize)</h2><p>使用零均值，主要是用来提升速度，范围尺度类似即可，也不是一定要0-1。如下图，若不同特征相差尺度很大，则会形成如左不规则超球体，这样w的迭代次数会大大增加，对应的迭代步长也应设置偏小。</p>\n<h2 id=\"梯度消失及爆炸\"><a href=\"#梯度消失及爆炸\" class=\"headerlink\" title=\"梯度消失及爆炸\"></a>梯度消失及爆炸</h2><p>由于层数太多，梯度指数级变化，这容易导致步长会非常小，训练时间会变得很长。传统激活函数都存在这一缺点，正向时爆炸，反向时消失，为防止上述问题，权重的初始值的选择有：</p>\n<ul>\n<li>若用RELU作为激活函数（<a href=\"http://blog.csdn.net/mzpmzk/article/details/77418030\" target=\"_blank\" rel=\"noopener\">种类及特点</a>), $w$ 初始值可以使用 $np.sqrt(\\frac{2}{n^{[l-1]}+n^{[l]}})$<br><img src=\"http://p15i7i801.bkt.clouddn.com/1f3a7fe0374fc7a514f6c69809520867.png\" alt=\"ng_vanishing_exploding_grad\"><h1 id=\"第二周-优化算法\"><a href=\"#第二周-优化算法\" class=\"headerlink\" title=\"第二周 优化算法\"></a>第二周 优化算法</h1></li>\n<li>Min-batch梯度下降：避免需要处理所有数据才能进行下一步</li>\n<li>指数加权平均</li>\n<li>动量（Momentum）梯度下降:类似移动平均线减缓摆动<script type=\"math/tex; mode=display\">v_{dw} = \\beta v_{dw} + (1-\\beta)dw</script><script type=\"math/tex; mode=display\">v_{db}=\\beta v_{db} + (1-\\beta)db</script><script type=\"math/tex; mode=display\">w=w-\\alpha v_{dw}</script><script type=\"math/tex; mode=display\">b=b-\\alpha v_{db}</script></li>\n<li>RMSprop：让幅度大的参数变缓，让幅度小的参数变大</li>\n</ul>\n<script type=\"math/tex; mode=display\">S_{dw} = \\beta S_{dw} + (1-\\beta){dw}^2</script><script type=\"math/tex; mode=display\">S_{db}=\\beta S_{db} + (1-\\beta){db}^2</script><script type=\"math/tex; mode=display\">w=w-\\alpha \\frac{dw}{\\sqrt{S_{dw}}}</script><script type=\"math/tex; mode=display\">b=b-\\alpha \\frac{db}{\\sqrt{S_{db}}}</script><ul>\n<li>Adam(Adaptive Moment Estimation): 各算法适应于不同的神经网络结构，该算法结合Momentum and RMSprop，非常常用。<ul>\n<li>公式<script type=\"math/tex; mode=display\">v_{dw} = \\beta v_{dw} + (1-\\beta_1)dw</script><script type=\"math/tex; mode=display\">v_{db}=\\beta v_{db} + (1-\\beta_1)db</script><script type=\"math/tex; mode=display\">S_{dw} = \\beta S_{dw} + (1-\\beta_2){dw}^2</script><script type=\"math/tex; mode=display\">S_{db}=\\beta S_{db} + (1-\\beta_2){db}^2</script><script type=\"math/tex; mode=display\">w=w-\\alpha \\frac{v_{dw}}{\\sqrt{S_{dw}}}</script><script type=\"math/tex; mode=display\">b=b-\\alpha \\frac{v_{db}}{\\sqrt{S_{db}}}</script></li>\n<li>超参数一般值：<ul>\n<li>$\\alpha$ : needs to be tune</li>\n<li>$\\beta_1$ : (first momentum):0.9</li>\n<li>$\\beta_2$ : (second momnetum:RMSprop):0.99</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>学习率衰减：多个方法，如倒数、指数等等</li>\n<li>关于局部最优：在大量参数情况下可能会出现saddle问题，这样问题容易到达plateau状态，较难走出。这种可能性较小，通常使用Adam之类的优化方法解决</li>\n</ul>\n<h1 id=\"第三周-超参数调度、Batch正则与程序框架\"><a href=\"#第三周-超参数调度、Batch正则与程序框架\" class=\"headerlink\" title=\"第三周 超参数调度、Batch正则与程序框架\"></a>第三周 超参数调度、Batch正则与程序框架</h1><h2 id=\"超参数调整\"><a href=\"#超参数调整\" class=\"headerlink\" title=\"超参数调整\"></a>超参数调整</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/149d38d31a5fa4e0576ff617c0408f08.png\" alt=\"ng_hp\"></p>\n<ul>\n<li>首先是红色的 $\\alpha$，再者是黄色的学习率，第一、二动量及min-batch，最后再考虑调整层数及学习率延时 $\\varepsilon$。</li>\n<li>使用随机数、从粗到细</li>\n<li>使用合适的分布尺寸，而不是使用均匀分布，比如对于 $\\alpha$可以使用a log scala，举例而言：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">r = <span class=\"number\">-4</span>*np.random.rand()</span><br><span class=\"line\">alpha = <span class=\"number\">10</span>^r</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"实践经验：两个流派\"><a href=\"#实践经验：两个流派\" class=\"headerlink\" title=\"实践经验：两个流派\"></a>实践经验：两个流派</h2><ul>\n<li>使用各领域的常见参数（<strong>这意味需要对各个智能领域有的了解</strong>）</li>\n<li>直觉很有效，<strong>定期更新参数，比如几个月</strong></li>\n<li>babysitting（pandan）: 长期照看，评估调整——（<strong>不断的持久化模型吗？，一直迭代下去，还是每次都重新训练，模型可以以直训练下去吗？</strong>）</li>\n<li>train in paraller: Caviar(鱼子酱)</li>\n</ul>\n<h2 id=\"网络内激活函数的normalizaion\"><a href=\"#网络内激活函数的normalizaion\" class=\"headerlink\" title=\"网络内激活函数的normalizaion\"></a>网络内激活函数的normalizaion</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/eaa10f3214ad4cbd03b074755a13a8e0.png\" alt=\"ng_batchnorm\"><br>请注意以下三点：</p>\n<ul>\n<li>通过 $\\alpha、\\beta$两个超参数，可以控制 $Z^i$ 的范围，如右上图所示</li>\n<li>单层网络中我们通过normalizaion来加快梯度下降的速度，那么也可以通过控制隐藏层的输入值来控制</li>\n<li>andrew的推荐: Normalize $z$ rather than $a$</li>\n</ul>\n<h2 id=\"将Batch-Norm拟合进神经网络\"><a href=\"#将Batch-Norm拟合进神经网络\" class=\"headerlink\" title=\"将Batch Norm拟合进神经网络\"></a>将Batch Norm拟合进神经网络</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/b398a029678304574bdf4e67d9ecc406.png\" alt=\"ng_addBatchNorm\"></p>\n<ul>\n<li>偏执项b从此被标准化抵消，即式子可简化为 $z^l=w^l a^{l-1}$</li>\n</ul>\n<h2 id=\"Batch-Norm奏效的原因\"><a href=\"#Batch-Norm奏效的原因\" class=\"headerlink\" title=\"Batch Norm奏效的原因\"></a>Batch Norm奏效的原因</h2><ul>\n<li>加速梯度下降</li>\n<li>It makes weights, later or deeper than your network, more robost to changes to weights in earlier layers of neural netowork. 举例而言，对于不带颜色的猫可能训练得很好，当带上颜色后，对于同样判断是否为猫的模型而言，输入数据的分布变化了，导致模型需要重新训练。这也称为<strong>covariate shift</strong>，为解决这个问题，可以使用Batch Norm。</li>\n<li>顺便的作用，如dropout，引入了噪声，相当于加入了regularization.</li>\n</ul>\n<h2 id=\"测试模型时BatchNorm的计算\"><a href=\"#测试模型时BatchNorm的计算\" class=\"headerlink\" title=\"测试模型时BatchNorm的计算\"></a>测试模型时BatchNorm的计算</h2><ul>\n<li>需要估算平均值及标准差</li>\n<li>简单靠谱的处理方式是用指数加权平均去<strong>估算</strong></li>\n<li>跑全量数据batchnorm也可以</li>\n</ul>\n<h2 id=\"Softmax分类\"><a href=\"#Softmax分类\" class=\"headerlink\" title=\"Softmax分类\"></a>Softmax分类</h2><p>逻辑回归的一般形式,对比hard max而言，比如分类结果值为 $[1,0,0,0]$ 而非 $[0.8,0.2,0,0]$<br><img src=\"http://p15i7i801.bkt.clouddn.com/6d6b110e436fb245d51cbf1179d231d2.png\" alt=\"ng_softmax\"></p>\n<h2 id=\"理解及训练Softmax分类器\"><a href=\"#理解及训练Softmax分类器\" class=\"headerlink\" title=\"理解及训练Softmax分类器\"></a>理解及训练Softmax分类器</h2><p>同逻辑回归的二分类器的损失函数，这里同样使用最大似然的思想来定义损失函数，使用<a href=\"https://www.cnblogs.com/maybe2030/p/4751804.html#_label1\" target=\"_blank\" rel=\"noopener\">常见最优化方法</a>——梯度下降法来求解模型参数 $w,b$。<br><img src=\"http://p15i7i801.bkt.clouddn.com/4e804d7c109e9149bf95fc2df0603b14.png\" alt=\"ng_softmax_loss\"></p>\n<h2 id=\"深度学习框架\"><a href=\"#深度学习框架\" class=\"headerlink\" title=\"深度学习框架\"></a>深度学习框架</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/de6e08b5039630e827e6a07ddb979868.png\" alt=\"ng_frameworks\"></p>\n<h4 id=\"易用\"><a href=\"#易用\" class=\"headerlink\" title=\"易用\"></a>易用</h4><h4 id=\"性能\"><a href=\"#性能\" class=\"headerlink\" title=\"性能\"></a>性能</h4><p>  长时间的开源，真正的开放，而不是一段时间后转为自已的服务，停止开源。</p>\n<h4 id=\"其它\"><a href=\"#其它\" class=\"headerlink\" title=\"其它\"></a>其它</h4><p>当然也取决于语言、用途（视觉、NLP或者广告）</p>\n<h2 id=\"Tensflow\"><a href=\"#Tensflow\" class=\"headerlink\" title=\"Tensflow\"></a>Tensflow</h2><ul>\n<li>它核心在于计算损失函数并自动求导，它重载了普通的运算，所以以下是等价的：<br><img src=\"http://p15i7i801.bkt.clouddn.com/8d8b480a53b18ff52346c3ab73387c63.png\" alt=\"ng_cost\"></li>\n<li>它通过简单几句初始化建立计算图，通过placeholder喂以数据，通过自动求导计算最优cost<br><img src=\"http://p15i7i801.bkt.clouddn.com/43e55f2e9f72d5d917becb7c40778281.png\" alt=\"ng_computergraph\"></li>\n<li>Python 使用with可自动清理类的显式资源，tensoflow通过tf.variable来定义求取的变量</li>\n<li>总而言之，<strong>它的框架是计算图，核心是损失函数及对应的求解方法，辅以定义自变量、喂养数据的手段，最后运行该图</strong><br><img src=\"http://p15i7i801.bkt.clouddn.com/449c3a1a770e93d143aade68d90f049f.png\" alt=\"ng_feed\"></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"第一周-深度学习的实用层面\"><a href=\"#第一周-深度学习的实用层面\" class=\"headerlink\" title=\"第一周 深度学习的实用层面\"></a>第一周 深度学习的实用层面</h1><h2 id=\"训练、验证、测试集\"><a href=\"#训练、验证、测试集\" class=\"headerlink\" title=\"训练、验证、测试集\"></a>训练、验证、测试集</h2><ul>\n<li>在小数时代，比例一般为6：2：2，大数据时代可适当调整，验证及测试集为10000左右即可。</li>\n<li>train set 最好和后两者有相当的分布，比如猫的问题，在网上爬的猫有许多精良的照片，而用户实际上传的可能比较糙。</li>\n<li>test set是为了拿到测试的无偏估计</li>\n<li>ML是一个高度迭代的过程，即使最牛的专家也是如此。</li>\n</ul>\n<h2 id=\"偏差与方差\"><a href=\"#偏差与方差\" class=\"headerlink\" title=\"偏差与方差\"></a>偏差与方差</h2><ul>\n<li>偏差代表训练集的错误率有多高，方差代表dev set 和train set的错误率的差别</li>\n</ul>\n<h2 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h2><p>regularizer，规则化，通过先验的知识来最大化贝叶斯最大后验估计，本质上是贝叶斯最大后验估计的<strong>似然函数</strong></p>\n<ul>\n<li>L1</li>\n<li><p>L2：Frobenius Norm，直观上Lambda越大，权重矩阵被设为零<br><img src=\"http://p15i7i801.bkt.clouddn.com/2d81fcb8cfe1cb048929564aa91f9c4f.png\" alt=\"nn_reg_12181821\"></p>\n</li>\n<li><p>dropout</p>\n</li>\n<li>其它减少过拟合的方法<ul>\n<li>Data augmentation</li>\n<li>early stop: 此种方法相对L2来说减少lambda的尝试计算量</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"正则化输入（normalize\"><a href=\"#正则化输入（normalize\" class=\"headerlink\" title=\"正则化输入（normalize)\"></a>正则化输入（normalize)</h2><p>使用零均值，主要是用来提升速度，范围尺度类似即可，也不是一定要0-1。如下图，若不同特征相差尺度很大，则会形成如左不规则超球体，这样w的迭代次数会大大增加，对应的迭代步长也应设置偏小。</p>\n<h2 id=\"梯度消失及爆炸\"><a href=\"#梯度消失及爆炸\" class=\"headerlink\" title=\"梯度消失及爆炸\"></a>梯度消失及爆炸</h2><p>由于层数太多，梯度指数级变化，这容易导致步长会非常小，训练时间会变得很长。传统激活函数都存在这一缺点，正向时爆炸，反向时消失，为防止上述问题，权重的初始值的选择有：</p>\n<ul>\n<li>若用RELU作为激活函数（<a href=\"http://blog.csdn.net/mzpmzk/article/details/77418030\" target=\"_blank\" rel=\"noopener\">种类及特点</a>), $w$ 初始值可以使用 $np.sqrt(\\frac{2}{n^{[l-1]}+n^{[l]}})$<br><img src=\"http://p15i7i801.bkt.clouddn.com/1f3a7fe0374fc7a514f6c69809520867.png\" alt=\"ng_vanishing_exploding_grad\"><h1 id=\"第二周-优化算法\"><a href=\"#第二周-优化算法\" class=\"headerlink\" title=\"第二周 优化算法\"></a>第二周 优化算法</h1></li>\n<li>Min-batch梯度下降：避免需要处理所有数据才能进行下一步</li>\n<li>指数加权平均</li>\n<li>动量（Momentum）梯度下降:类似移动平均线减缓摆动<script type=\"math/tex; mode=display\">v_{dw} = \\beta v_{dw} + (1-\\beta)dw</script><script type=\"math/tex; mode=display\">v_{db}=\\beta v_{db} + (1-\\beta)db</script><script type=\"math/tex; mode=display\">w=w-\\alpha v_{dw}</script><script type=\"math/tex; mode=display\">b=b-\\alpha v_{db}</script></li>\n<li>RMSprop：让幅度大的参数变缓，让幅度小的参数变大</li>\n</ul>\n<script type=\"math/tex; mode=display\">S_{dw} = \\beta S_{dw} + (1-\\beta){dw}^2</script><script type=\"math/tex; mode=display\">S_{db}=\\beta S_{db} + (1-\\beta){db}^2</script><script type=\"math/tex; mode=display\">w=w-\\alpha \\frac{dw}{\\sqrt{S_{dw}}}</script><script type=\"math/tex; mode=display\">b=b-\\alpha \\frac{db}{\\sqrt{S_{db}}}</script><ul>\n<li>Adam(Adaptive Moment Estimation): 各算法适应于不同的神经网络结构，该算法结合Momentum and RMSprop，非常常用。<ul>\n<li>公式<script type=\"math/tex; mode=display\">v_{dw} = \\beta v_{dw} + (1-\\beta_1)dw</script><script type=\"math/tex; mode=display\">v_{db}=\\beta v_{db} + (1-\\beta_1)db</script><script type=\"math/tex; mode=display\">S_{dw} = \\beta S_{dw} + (1-\\beta_2){dw}^2</script><script type=\"math/tex; mode=display\">S_{db}=\\beta S_{db} + (1-\\beta_2){db}^2</script><script type=\"math/tex; mode=display\">w=w-\\alpha \\frac{v_{dw}}{\\sqrt{S_{dw}}}</script><script type=\"math/tex; mode=display\">b=b-\\alpha \\frac{v_{db}}{\\sqrt{S_{db}}}</script></li>\n<li>超参数一般值：<ul>\n<li>$\\alpha$ : needs to be tune</li>\n<li>$\\beta_1$ : (first momentum):0.9</li>\n<li>$\\beta_2$ : (second momnetum:RMSprop):0.99</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>学习率衰减：多个方法，如倒数、指数等等</li>\n<li>关于局部最优：在大量参数情况下可能会出现saddle问题，这样问题容易到达plateau状态，较难走出。这种可能性较小，通常使用Adam之类的优化方法解决</li>\n</ul>\n<h1 id=\"第三周-超参数调度、Batch正则与程序框架\"><a href=\"#第三周-超参数调度、Batch正则与程序框架\" class=\"headerlink\" title=\"第三周 超参数调度、Batch正则与程序框架\"></a>第三周 超参数调度、Batch正则与程序框架</h1><h2 id=\"超参数调整\"><a href=\"#超参数调整\" class=\"headerlink\" title=\"超参数调整\"></a>超参数调整</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/149d38d31a5fa4e0576ff617c0408f08.png\" alt=\"ng_hp\"></p>\n<ul>\n<li>首先是红色的 $\\alpha$，再者是黄色的学习率，第一、二动量及min-batch，最后再考虑调整层数及学习率延时 $\\varepsilon$。</li>\n<li>使用随机数、从粗到细</li>\n<li>使用合适的分布尺寸，而不是使用均匀分布，比如对于 $\\alpha$可以使用a log scala，举例而言：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">r = <span class=\"number\">-4</span>*np.random.rand()</span><br><span class=\"line\">alpha = <span class=\"number\">10</span>^r</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"实践经验：两个流派\"><a href=\"#实践经验：两个流派\" class=\"headerlink\" title=\"实践经验：两个流派\"></a>实践经验：两个流派</h2><ul>\n<li>使用各领域的常见参数（<strong>这意味需要对各个智能领域有的了解</strong>）</li>\n<li>直觉很有效，<strong>定期更新参数，比如几个月</strong></li>\n<li>babysitting（pandan）: 长期照看，评估调整——（<strong>不断的持久化模型吗？，一直迭代下去，还是每次都重新训练，模型可以以直训练下去吗？</strong>）</li>\n<li>train in paraller: Caviar(鱼子酱)</li>\n</ul>\n<h2 id=\"网络内激活函数的normalizaion\"><a href=\"#网络内激活函数的normalizaion\" class=\"headerlink\" title=\"网络内激活函数的normalizaion\"></a>网络内激活函数的normalizaion</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/eaa10f3214ad4cbd03b074755a13a8e0.png\" alt=\"ng_batchnorm\"><br>请注意以下三点：</p>\n<ul>\n<li>通过 $\\alpha、\\beta$两个超参数，可以控制 $Z^i$ 的范围，如右上图所示</li>\n<li>单层网络中我们通过normalizaion来加快梯度下降的速度，那么也可以通过控制隐藏层的输入值来控制</li>\n<li>andrew的推荐: Normalize $z$ rather than $a$</li>\n</ul>\n<h2 id=\"将Batch-Norm拟合进神经网络\"><a href=\"#将Batch-Norm拟合进神经网络\" class=\"headerlink\" title=\"将Batch Norm拟合进神经网络\"></a>将Batch Norm拟合进神经网络</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/b398a029678304574bdf4e67d9ecc406.png\" alt=\"ng_addBatchNorm\"></p>\n<ul>\n<li>偏执项b从此被标准化抵消，即式子可简化为 $z^l=w^l a^{l-1}$</li>\n</ul>\n<h2 id=\"Batch-Norm奏效的原因\"><a href=\"#Batch-Norm奏效的原因\" class=\"headerlink\" title=\"Batch Norm奏效的原因\"></a>Batch Norm奏效的原因</h2><ul>\n<li>加速梯度下降</li>\n<li>It makes weights, later or deeper than your network, more robost to changes to weights in earlier layers of neural netowork. 举例而言，对于不带颜色的猫可能训练得很好，当带上颜色后，对于同样判断是否为猫的模型而言，输入数据的分布变化了，导致模型需要重新训练。这也称为<strong>covariate shift</strong>，为解决这个问题，可以使用Batch Norm。</li>\n<li>顺便的作用，如dropout，引入了噪声，相当于加入了regularization.</li>\n</ul>\n<h2 id=\"测试模型时BatchNorm的计算\"><a href=\"#测试模型时BatchNorm的计算\" class=\"headerlink\" title=\"测试模型时BatchNorm的计算\"></a>测试模型时BatchNorm的计算</h2><ul>\n<li>需要估算平均值及标准差</li>\n<li>简单靠谱的处理方式是用指数加权平均去<strong>估算</strong></li>\n<li>跑全量数据batchnorm也可以</li>\n</ul>\n<h2 id=\"Softmax分类\"><a href=\"#Softmax分类\" class=\"headerlink\" title=\"Softmax分类\"></a>Softmax分类</h2><p>逻辑回归的一般形式,对比hard max而言，比如分类结果值为 $[1,0,0,0]$ 而非 $[0.8,0.2,0,0]$<br><img src=\"http://p15i7i801.bkt.clouddn.com/6d6b110e436fb245d51cbf1179d231d2.png\" alt=\"ng_softmax\"></p>\n<h2 id=\"理解及训练Softmax分类器\"><a href=\"#理解及训练Softmax分类器\" class=\"headerlink\" title=\"理解及训练Softmax分类器\"></a>理解及训练Softmax分类器</h2><p>同逻辑回归的二分类器的损失函数，这里同样使用最大似然的思想来定义损失函数，使用<a href=\"https://www.cnblogs.com/maybe2030/p/4751804.html#_label1\" target=\"_blank\" rel=\"noopener\">常见最优化方法</a>——梯度下降法来求解模型参数 $w,b$。<br><img src=\"http://p15i7i801.bkt.clouddn.com/4e804d7c109e9149bf95fc2df0603b14.png\" alt=\"ng_softmax_loss\"></p>\n<h2 id=\"深度学习框架\"><a href=\"#深度学习框架\" class=\"headerlink\" title=\"深度学习框架\"></a>深度学习框架</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/de6e08b5039630e827e6a07ddb979868.png\" alt=\"ng_frameworks\"></p>\n<h4 id=\"易用\"><a href=\"#易用\" class=\"headerlink\" title=\"易用\"></a>易用</h4><h4 id=\"性能\"><a href=\"#性能\" class=\"headerlink\" title=\"性能\"></a>性能</h4><p>  长时间的开源，真正的开放，而不是一段时间后转为自已的服务，停止开源。</p>\n<h4 id=\"其它\"><a href=\"#其它\" class=\"headerlink\" title=\"其它\"></a>其它</h4><p>当然也取决于语言、用途（视觉、NLP或者广告）</p>\n<h2 id=\"Tensflow\"><a href=\"#Tensflow\" class=\"headerlink\" title=\"Tensflow\"></a>Tensflow</h2><ul>\n<li>它核心在于计算损失函数并自动求导，它重载了普通的运算，所以以下是等价的：<br><img src=\"http://p15i7i801.bkt.clouddn.com/8d8b480a53b18ff52346c3ab73387c63.png\" alt=\"ng_cost\"></li>\n<li>它通过简单几句初始化建立计算图，通过placeholder喂以数据，通过自动求导计算最优cost<br><img src=\"http://p15i7i801.bkt.clouddn.com/43e55f2e9f72d5d917becb7c40778281.png\" alt=\"ng_computergraph\"></li>\n<li>Python 使用with可自动清理类的显式资源，tensoflow通过tf.variable来定义求取的变量</li>\n<li>总而言之，<strong>它的框架是计算图，核心是损失函数及对应的求解方法，辅以定义自变量、喂养数据的手段，最后运行该图</strong><br><img src=\"http://p15i7i801.bkt.clouddn.com/449c3a1a770e93d143aade68d90f049f.png\" alt=\"ng_feed\"></li>\n</ul>\n"},{"title":"集体智慧编程","date":"2017-07-17T06:27:46.000Z","mathjax":true,"_content":"\n####推荐\n* 对于每人都持有的项，使用权重稀释\n* itemCF，通常我们需要在用户基数和评分数量不是很大的时候去计算，随着用户的增长，物品之间的相似度评价会越来越稳定\n* 文本聚类，注意过滤太常用的the     ，可以定一个上下界【10%，50%】\n####聚类\n* 分层聚类，优点在于直观，不用指定聚类数，缺点计算量惊人\n####搜索\n* 流程：\n爬虫——设计SCHEMA——索引——查询——评分排名（单词频度 、文档位置、单词距离）——\n* 评分\n * 普通指标：单词频度 、文档位置、单词距离\n * 利用外部回指-简单计数\n *　pagerank\n","source":"_posts/集体智慧编程.md","raw":"---\ntitle: 集体智慧编程\ndate: 2017-07-17 14:27:46\ntags:\n      - 机器学习\n      - 经典著作\ncategories: AI梦\nmathjax: true\n---\n\n####推荐\n* 对于每人都持有的项，使用权重稀释\n* itemCF，通常我们需要在用户基数和评分数量不是很大的时候去计算，随着用户的增长，物品之间的相似度评价会越来越稳定\n* 文本聚类，注意过滤太常用的the     ，可以定一个上下界【10%，50%】\n####聚类\n* 分层聚类，优点在于直观，不用指定聚类数，缺点计算量惊人\n####搜索\n* 流程：\n爬虫——设计SCHEMA——索引——查询——评分排名（单词频度 、文档位置、单词距离）——\n* 评分\n * 普通指标：单词频度 、文档位置、单词距离\n * 利用外部回指-简单计数\n *　pagerank\n","slug":"集体智慧编程","published":1,"updated":"2017-12-27T12:36:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjclxs5hv000oajslvaabrh6v","content":"<h4 id=\"推荐\"><a href=\"#推荐\" class=\"headerlink\" title=\"推荐\"></a>推荐</h4><ul>\n<li>对于每人都持有的项，使用权重稀释</li>\n<li>itemCF，通常我们需要在用户基数和评分数量不是很大的时候去计算，随着用户的增长，物品之间的相似度评价会越来越稳定</li>\n<li>文本聚类，注意过滤太常用的the     ，可以定一个上下界【10%，50%】<h4 id=\"聚类\"><a href=\"#聚类\" class=\"headerlink\" title=\"聚类\"></a>聚类</h4></li>\n<li>分层聚类，优点在于直观，不用指定聚类数，缺点计算量惊人<h4 id=\"搜索\"><a href=\"#搜索\" class=\"headerlink\" title=\"搜索\"></a>搜索</h4></li>\n<li>流程：<br>爬虫——设计SCHEMA——索引——查询——评分排名（单词频度 、文档位置、单词距离）——</li>\n<li>评分<ul>\n<li>普通指标：单词频度 、文档位置、单词距离</li>\n<li>利用外部回指-简单计数<br>*　pagerank</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"推荐\"><a href=\"#推荐\" class=\"headerlink\" title=\"推荐\"></a>推荐</h4><ul>\n<li>对于每人都持有的项，使用权重稀释</li>\n<li>itemCF，通常我们需要在用户基数和评分数量不是很大的时候去计算，随着用户的增长，物品之间的相似度评价会越来越稳定</li>\n<li>文本聚类，注意过滤太常用的the     ，可以定一个上下界【10%，50%】<h4 id=\"聚类\"><a href=\"#聚类\" class=\"headerlink\" title=\"聚类\"></a>聚类</h4></li>\n<li>分层聚类，优点在于直观，不用指定聚类数，缺点计算量惊人<h4 id=\"搜索\"><a href=\"#搜索\" class=\"headerlink\" title=\"搜索\"></a>搜索</h4></li>\n<li>流程：<br>爬虫——设计SCHEMA——索引——查询——评分排名（单词频度 、文档位置、单词距离）——</li>\n<li>评分<ul>\n<li>普通指标：单词频度 、文档位置、单词距离</li>\n<li>利用外部回指-简单计数<br>*　pagerank</li>\n</ul>\n</li>\n</ul>\n"},{"title":"结构化机器学习项目","date":"2018-01-08T10:36:40.000Z","mathjax":true,"_content":"\n\n## 机器学习策略\n\n### 常见方法及局限\n搭建深度学习模型后，模型达到了一定的准确率，若要再度提升算法效率，或许你会\n![ng_ff](http://p15i7i801.bkt.clouddn.com/a6cbb32e4ed57cb8919ab52d32385fa6.png)\n这里的问题在于，如此尝试诸多方法，或许半年就过去了，并且不见得会多有效。因此我们需要策略来加速这一过程，这正是此门课的独特之处。\n### 正交化\n使用早期的电视屏幕调整作比喻，分为上下钮、左右钮、梯形钮，此种情况更容易控制屏幕。这样在训练集、开发集、测试集达到好的效果之后，再将模型在真实世界使用。\n### 使用单个评估指标\n* 如查准及查全，使用F1 score代替，F1分数值是查准及查全的调和平均值——harmanic mean,当然它近似平均值\n* 又如在真实的不同场景中，比如美国、中国、印度的图片识别猫的准确率不一致，可以直接求它们的平均值\n* 以上针对算法的迭代优化选择而言，可以提高效率\n\n### 优化指标及满足指标\n即最优的，优化项目及满足项目，有多个指标时，可以先限定某些指标而专注于优化其中一个指标。举例而言，比如流行的智能音箱，优化唤醒率，而限制假阳率为24小时内最多1次，以此标准迭代算法。\n\n### 训练、开发、测试集\n* 开发集，叫称为交叉验证集，开发集的分布应于测试集一致\n* 在早期机器学习阶段7：3，6：2：2是常见的划分方法，而在现代深度学习算法习惯使用大量的数据，故可用更小的比例使用到开发集和测试集。谨记住测试集的作用，它是为了评估投产系统的性能，它能高置信度评估的效果即可 ，所以它可能只要1万或十万即可，这远远比所有数据20%的比例少\n* 在反色情猫分类器的例子中，将机器学习的过程分为独立两步，每一步找目标，即确定单一评估指标，第二步对准和射击目标\n![ng_catclassify](http://p15i7i801.bkt.clouddn.com/908a5e3dc3f7dc7d1f94c07c53ded776.png)\n\n### 机器的表现和人的表现\n* 机器学习的准确度基本符合以下曲线,在追上人类以前发展很快，超越人类后发展很慢，它的上限即贝叶斯误差，也叫贝叶斯最优误差。\n![ng_bayeserror](http://p15i7i801.bkt.clouddn.com/fb704ed94c43e03f1ae2ef08d9cd3506.png)\n* 低于人类的时候发展快的原因可以从三个角度分析，一为可从人类视角分析，二为可获取人工标签，三可以更好的分析偏差和方差\n\n### 可避免的偏差与方差\n* 加入概念：[泛化误差](https://www.zhihu.com/question/27068705)：在机器学习中泛化误差是用来衡量一个学习机器推广未知数据的能力，一图胜千言\n![ng_add_error](http://p15i7i801.bkt.clouddn.com/8e88161f43c636e5bebe07028dbe1934.png)\n* 举猫分类器而言，可以使用人类的偏差率来接近贝叶斯误差，因为人类在这方面很擅长。若偏差已没有增进空间，可以在考虑减少方差，方差可以通过正则或加数据来减少。\n![ng_catvoidbias](http://p15i7i801.bkt.clouddn.com/e9853c9e01bb2c77abd481430fb06680.png)\n* 请注意人类水平的误差取决于具体目的，比如医学图像分类，不同的人的误差差率是不同的，估计贝叶斯误差可以使用最专业的团队的误差率\n![ng_reducingavoid](http://p15i7i801.bkt.clouddn.com/22bcdef589b4b7470d106dd2f19a7e21.png)\n\n### 理解人类的表现\n* 人类擅长处理自然感知任务，如图像、语音及NLP识别，在这些方面超越人类比较困难。在近几年的发展中，特定的几个领域内，如医学图像（皮肤癌、BCG判断）等已经取得了比人类更好的结果。\n* 而在以下方面，机器学习已经远远超越了人类，它们的共同点是有大量的历史数据\n  * 广告点击\n  * 推荐\n  * 预测用时——快递及运输\n  * 还贷\n\n### 误差分析\n在猫分类的例子中，假若你的错误率是10%，你的队员可能注意到有不少猫被识别成了猫，他可能建议你针对狗的优化算法，试想你可以收集更多狗的照片或设计一些只处理狗的算法，这样单独起一个项目会花费上月的时间，在这里建议一个简单的法子\n* 在开发集中取出100个被错误分类的例子\n* 取出是狗的例子,找出它的特点，比如模糊的图像、滤镜的图像，这样你就更能理解你的分类器，具体看情况去处理它了。\n![ng_catdect](http://p15i7i801.bkt.clouddn.com/3cc21a20da564a3f00f4af0954e0937b.png)\n* 对于错误标记的数据是是否需要修正，依情况而定，同样用错误表来分析，若主要问题在其它因素上，暂暂时不用管，反之则分析。如图。\n![ng_bias](http://p15i7i801.bkt.clouddn.com/09d9124ea0149ce6c349e1657a4fc015.png)\n* 不管什么模型，也许很多研究人员、工程师都认为喂数据即可，其它人工分析是很有必要的，至少NG经常这么干，手工和这些错误做斗争可以快速的了解到工作的重点，这是很值得做的。强烈建议\n\n\n### 快速的搭建系统并迅速迭代\n* 寻找好特定领域的方向，划分训练开发集并设定单一评估指标，迅速搭建初始系统并使用偏差/方差分析或误差分析理论去优化下一步。\n* 实际情况训练和测试分布往往不一致，两者数据都打散然后划分是不可取的，目标的准心就有偏差了。经常的做法是，将测试数据加入训练集，而开发集和测试集只使用目标真实数据。\n![ng_diffdis](http://p15i7i801.bkt.clouddn.com/001e95d226208589b6ac79293b6e7cd0.png)\n* 在不匹配数据划分的情况下的误差分析，我们往往加入训练开发混合集去定位误差是否是数据匹配问题导致的。对于下图最右边deverror和testerror比trainerror还小的情况，可以通过第二张片子来进一步分析，图上是车镜语音场景的一个例子。\n![NG_trainde](http://p15i7i801.bkt.clouddn.com/9765181a9909f8540596dcdb5cd05d75.png)\n![ng_moreformula](http://p15i7i801.bkt.clouddn.com/a13ab0be285d7f4d6ee8e3220231b042.png)\n* 对于确定是数据不匹配的问题导致的误差，没有系统的解决方案，可以从以下两个角度尝试：\n  * 手动分析误差并理解训练和开发测试集的分布差异\n  * 使得测试集的数据分布向测试集靠近，可能通过增加数据或人造数据的方式\n* 人造数据要注意的是，防止增加一小部分数据而造成模型对这一部分数据过拟合，用汽车识别的例子，若只取游戏中的车的合成图片，即便有海量的张数，但汽车的各类只有20种，这样就不行。\n![ng_artificalsynthesis](http://p15i7i801.bkt.clouddn.com/c7c6fb7cc985c944c32ca31d9eaef00b.png)\n\n### 迁移学习\n* 迁移学习的过程，如下图所示，把输出层换成自己的层和输出层，再用自有数据训练。从原理上来说即利用其它模型已经学习到的特征，如轮廓、边、角等等基础特征\n![ng_transfer](http://p15i7i801.bkt.clouddn.com/561cacd18c3d8ecdc8a228a19260f6ae.png)\n* 从原理上理解，什么场景下适合迁移学习呢，对于那些比较难收集数据的场景，如射线科，如果有基础特征类似的基础场景已经较好的训练了便可以尝试迁移\n\n### 多任务学习\n* 原理，注意和softmax的区别，它的目标有多个\n![ng_MULTITASK](http://p15i7i801.bkt.clouddn.com/23c7fec2fb7b2bc578289f69ddeb0fa1.png)\n* 什么情况下多任务学习\n![ng_shouldmulti](http://p15i7i801.bkt.clouddn.com/d01097fa3af60c0103a5921c0bd989d0.png)\n\n### 端到端的深度学习及适用场景\n* 它省去了传统机器学习的各类环节，在以往也许有的研究人员终其一生都在设计这些环节\n![ng_dotdot](http://p15i7i801.bkt.clouddn.com/3d78944e1e5fa81760fc5ac9700e0182.png)\n* 在人脸识别中我们先目标检测再做识别，如果要端对端的话，这需要大量的数据。传统机器翻译需要先做文本分析，再提取特征等等，端对端的学习在这种场景下非常适应，因为它有大量的数据。\n* 因此关键的问题是有没有\n","source":"_posts/结构化机器学习项目.md","raw":"---\ntitle: 结构化机器学习项目\ndate: 2018-01-08 18:36:40\ntags:\n      - 深度学习\n      - Andrew NG\n      - 公开课\ncategories: AI梦\nmathjax: true\n---\n\n\n## 机器学习策略\n\n### 常见方法及局限\n搭建深度学习模型后，模型达到了一定的准确率，若要再度提升算法效率，或许你会\n![ng_ff](http://p15i7i801.bkt.clouddn.com/a6cbb32e4ed57cb8919ab52d32385fa6.png)\n这里的问题在于，如此尝试诸多方法，或许半年就过去了，并且不见得会多有效。因此我们需要策略来加速这一过程，这正是此门课的独特之处。\n### 正交化\n使用早期的电视屏幕调整作比喻，分为上下钮、左右钮、梯形钮，此种情况更容易控制屏幕。这样在训练集、开发集、测试集达到好的效果之后，再将模型在真实世界使用。\n### 使用单个评估指标\n* 如查准及查全，使用F1 score代替，F1分数值是查准及查全的调和平均值——harmanic mean,当然它近似平均值\n* 又如在真实的不同场景中，比如美国、中国、印度的图片识别猫的准确率不一致，可以直接求它们的平均值\n* 以上针对算法的迭代优化选择而言，可以提高效率\n\n### 优化指标及满足指标\n即最优的，优化项目及满足项目，有多个指标时，可以先限定某些指标而专注于优化其中一个指标。举例而言，比如流行的智能音箱，优化唤醒率，而限制假阳率为24小时内最多1次，以此标准迭代算法。\n\n### 训练、开发、测试集\n* 开发集，叫称为交叉验证集，开发集的分布应于测试集一致\n* 在早期机器学习阶段7：3，6：2：2是常见的划分方法，而在现代深度学习算法习惯使用大量的数据，故可用更小的比例使用到开发集和测试集。谨记住测试集的作用，它是为了评估投产系统的性能，它能高置信度评估的效果即可 ，所以它可能只要1万或十万即可，这远远比所有数据20%的比例少\n* 在反色情猫分类器的例子中，将机器学习的过程分为独立两步，每一步找目标，即确定单一评估指标，第二步对准和射击目标\n![ng_catclassify](http://p15i7i801.bkt.clouddn.com/908a5e3dc3f7dc7d1f94c07c53ded776.png)\n\n### 机器的表现和人的表现\n* 机器学习的准确度基本符合以下曲线,在追上人类以前发展很快，超越人类后发展很慢，它的上限即贝叶斯误差，也叫贝叶斯最优误差。\n![ng_bayeserror](http://p15i7i801.bkt.clouddn.com/fb704ed94c43e03f1ae2ef08d9cd3506.png)\n* 低于人类的时候发展快的原因可以从三个角度分析，一为可从人类视角分析，二为可获取人工标签，三可以更好的分析偏差和方差\n\n### 可避免的偏差与方差\n* 加入概念：[泛化误差](https://www.zhihu.com/question/27068705)：在机器学习中泛化误差是用来衡量一个学习机器推广未知数据的能力，一图胜千言\n![ng_add_error](http://p15i7i801.bkt.clouddn.com/8e88161f43c636e5bebe07028dbe1934.png)\n* 举猫分类器而言，可以使用人类的偏差率来接近贝叶斯误差，因为人类在这方面很擅长。若偏差已没有增进空间，可以在考虑减少方差，方差可以通过正则或加数据来减少。\n![ng_catvoidbias](http://p15i7i801.bkt.clouddn.com/e9853c9e01bb2c77abd481430fb06680.png)\n* 请注意人类水平的误差取决于具体目的，比如医学图像分类，不同的人的误差差率是不同的，估计贝叶斯误差可以使用最专业的团队的误差率\n![ng_reducingavoid](http://p15i7i801.bkt.clouddn.com/22bcdef589b4b7470d106dd2f19a7e21.png)\n\n### 理解人类的表现\n* 人类擅长处理自然感知任务，如图像、语音及NLP识别，在这些方面超越人类比较困难。在近几年的发展中，特定的几个领域内，如医学图像（皮肤癌、BCG判断）等已经取得了比人类更好的结果。\n* 而在以下方面，机器学习已经远远超越了人类，它们的共同点是有大量的历史数据\n  * 广告点击\n  * 推荐\n  * 预测用时——快递及运输\n  * 还贷\n\n### 误差分析\n在猫分类的例子中，假若你的错误率是10%，你的队员可能注意到有不少猫被识别成了猫，他可能建议你针对狗的优化算法，试想你可以收集更多狗的照片或设计一些只处理狗的算法，这样单独起一个项目会花费上月的时间，在这里建议一个简单的法子\n* 在开发集中取出100个被错误分类的例子\n* 取出是狗的例子,找出它的特点，比如模糊的图像、滤镜的图像，这样你就更能理解你的分类器，具体看情况去处理它了。\n![ng_catdect](http://p15i7i801.bkt.clouddn.com/3cc21a20da564a3f00f4af0954e0937b.png)\n* 对于错误标记的数据是是否需要修正，依情况而定，同样用错误表来分析，若主要问题在其它因素上，暂暂时不用管，反之则分析。如图。\n![ng_bias](http://p15i7i801.bkt.clouddn.com/09d9124ea0149ce6c349e1657a4fc015.png)\n* 不管什么模型，也许很多研究人员、工程师都认为喂数据即可，其它人工分析是很有必要的，至少NG经常这么干，手工和这些错误做斗争可以快速的了解到工作的重点，这是很值得做的。强烈建议\n\n\n### 快速的搭建系统并迅速迭代\n* 寻找好特定领域的方向，划分训练开发集并设定单一评估指标，迅速搭建初始系统并使用偏差/方差分析或误差分析理论去优化下一步。\n* 实际情况训练和测试分布往往不一致，两者数据都打散然后划分是不可取的，目标的准心就有偏差了。经常的做法是，将测试数据加入训练集，而开发集和测试集只使用目标真实数据。\n![ng_diffdis](http://p15i7i801.bkt.clouddn.com/001e95d226208589b6ac79293b6e7cd0.png)\n* 在不匹配数据划分的情况下的误差分析，我们往往加入训练开发混合集去定位误差是否是数据匹配问题导致的。对于下图最右边deverror和testerror比trainerror还小的情况，可以通过第二张片子来进一步分析，图上是车镜语音场景的一个例子。\n![NG_trainde](http://p15i7i801.bkt.clouddn.com/9765181a9909f8540596dcdb5cd05d75.png)\n![ng_moreformula](http://p15i7i801.bkt.clouddn.com/a13ab0be285d7f4d6ee8e3220231b042.png)\n* 对于确定是数据不匹配的问题导致的误差，没有系统的解决方案，可以从以下两个角度尝试：\n  * 手动分析误差并理解训练和开发测试集的分布差异\n  * 使得测试集的数据分布向测试集靠近，可能通过增加数据或人造数据的方式\n* 人造数据要注意的是，防止增加一小部分数据而造成模型对这一部分数据过拟合，用汽车识别的例子，若只取游戏中的车的合成图片，即便有海量的张数，但汽车的各类只有20种，这样就不行。\n![ng_artificalsynthesis](http://p15i7i801.bkt.clouddn.com/c7c6fb7cc985c944c32ca31d9eaef00b.png)\n\n### 迁移学习\n* 迁移学习的过程，如下图所示，把输出层换成自己的层和输出层，再用自有数据训练。从原理上来说即利用其它模型已经学习到的特征，如轮廓、边、角等等基础特征\n![ng_transfer](http://p15i7i801.bkt.clouddn.com/561cacd18c3d8ecdc8a228a19260f6ae.png)\n* 从原理上理解，什么场景下适合迁移学习呢，对于那些比较难收集数据的场景，如射线科，如果有基础特征类似的基础场景已经较好的训练了便可以尝试迁移\n\n### 多任务学习\n* 原理，注意和softmax的区别，它的目标有多个\n![ng_MULTITASK](http://p15i7i801.bkt.clouddn.com/23c7fec2fb7b2bc578289f69ddeb0fa1.png)\n* 什么情况下多任务学习\n![ng_shouldmulti](http://p15i7i801.bkt.clouddn.com/d01097fa3af60c0103a5921c0bd989d0.png)\n\n### 端到端的深度学习及适用场景\n* 它省去了传统机器学习的各类环节，在以往也许有的研究人员终其一生都在设计这些环节\n![ng_dotdot](http://p15i7i801.bkt.clouddn.com/3d78944e1e5fa81760fc5ac9700e0182.png)\n* 在人脸识别中我们先目标检测再做识别，如果要端对端的话，这需要大量的数据。传统机器翻译需要先做文本分析，再提取特征等等，端对端的学习在这种场景下非常适应，因为它有大量的数据。\n* 因此关键的问题是有没有\n","slug":"结构化机器学习项目","published":1,"updated":"2018-01-15T13:09:04.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjclxs5hw000qajslbyrm3baz","content":"<h2 id=\"机器学习策略\"><a href=\"#机器学习策略\" class=\"headerlink\" title=\"机器学习策略\"></a>机器学习策略</h2><h3 id=\"常见方法及局限\"><a href=\"#常见方法及局限\" class=\"headerlink\" title=\"常见方法及局限\"></a>常见方法及局限</h3><p>搭建深度学习模型后，模型达到了一定的准确率，若要再度提升算法效率，或许你会<br><img src=\"http://p15i7i801.bkt.clouddn.com/a6cbb32e4ed57cb8919ab52d32385fa6.png\" alt=\"ng_ff\"><br>这里的问题在于，如此尝试诸多方法，或许半年就过去了，并且不见得会多有效。因此我们需要策略来加速这一过程，这正是此门课的独特之处。</p>\n<h3 id=\"正交化\"><a href=\"#正交化\" class=\"headerlink\" title=\"正交化\"></a>正交化</h3><p>使用早期的电视屏幕调整作比喻，分为上下钮、左右钮、梯形钮，此种情况更容易控制屏幕。这样在训练集、开发集、测试集达到好的效果之后，再将模型在真实世界使用。</p>\n<h3 id=\"使用单个评估指标\"><a href=\"#使用单个评估指标\" class=\"headerlink\" title=\"使用单个评估指标\"></a>使用单个评估指标</h3><ul>\n<li>如查准及查全，使用F1 score代替，F1分数值是查准及查全的调和平均值——harmanic mean,当然它近似平均值</li>\n<li>又如在真实的不同场景中，比如美国、中国、印度的图片识别猫的准确率不一致，可以直接求它们的平均值</li>\n<li>以上针对算法的迭代优化选择而言，可以提高效率</li>\n</ul>\n<h3 id=\"优化指标及满足指标\"><a href=\"#优化指标及满足指标\" class=\"headerlink\" title=\"优化指标及满足指标\"></a>优化指标及满足指标</h3><p>即最优的，优化项目及满足项目，有多个指标时，可以先限定某些指标而专注于优化其中一个指标。举例而言，比如流行的智能音箱，优化唤醒率，而限制假阳率为24小时内最多1次，以此标准迭代算法。</p>\n<h3 id=\"训练、开发、测试集\"><a href=\"#训练、开发、测试集\" class=\"headerlink\" title=\"训练、开发、测试集\"></a>训练、开发、测试集</h3><ul>\n<li>开发集，叫称为交叉验证集，开发集的分布应于测试集一致</li>\n<li>在早期机器学习阶段7：3，6：2：2是常见的划分方法，而在现代深度学习算法习惯使用大量的数据，故可用更小的比例使用到开发集和测试集。谨记住测试集的作用，它是为了评估投产系统的性能，它能高置信度评估的效果即可 ，所以它可能只要1万或十万即可，这远远比所有数据20%的比例少</li>\n<li>在反色情猫分类器的例子中，将机器学习的过程分为独立两步，每一步找目标，即确定单一评估指标，第二步对准和射击目标<br><img src=\"http://p15i7i801.bkt.clouddn.com/908a5e3dc3f7dc7d1f94c07c53ded776.png\" alt=\"ng_catclassify\"></li>\n</ul>\n<h3 id=\"机器的表现和人的表现\"><a href=\"#机器的表现和人的表现\" class=\"headerlink\" title=\"机器的表现和人的表现\"></a>机器的表现和人的表现</h3><ul>\n<li>机器学习的准确度基本符合以下曲线,在追上人类以前发展很快，超越人类后发展很慢，它的上限即贝叶斯误差，也叫贝叶斯最优误差。<br><img src=\"http://p15i7i801.bkt.clouddn.com/fb704ed94c43e03f1ae2ef08d9cd3506.png\" alt=\"ng_bayeserror\"></li>\n<li>低于人类的时候发展快的原因可以从三个角度分析，一为可从人类视角分析，二为可获取人工标签，三可以更好的分析偏差和方差</li>\n</ul>\n<h3 id=\"可避免的偏差与方差\"><a href=\"#可避免的偏差与方差\" class=\"headerlink\" title=\"可避免的偏差与方差\"></a>可避免的偏差与方差</h3><ul>\n<li>加入概念：<a href=\"https://www.zhihu.com/question/27068705\" target=\"_blank\" rel=\"noopener\">泛化误差</a>：在机器学习中泛化误差是用来衡量一个学习机器推广未知数据的能力，一图胜千言<br><img src=\"http://p15i7i801.bkt.clouddn.com/8e88161f43c636e5bebe07028dbe1934.png\" alt=\"ng_add_error\"></li>\n<li>举猫分类器而言，可以使用人类的偏差率来接近贝叶斯误差，因为人类在这方面很擅长。若偏差已没有增进空间，可以在考虑减少方差，方差可以通过正则或加数据来减少。<br><img src=\"http://p15i7i801.bkt.clouddn.com/e9853c9e01bb2c77abd481430fb06680.png\" alt=\"ng_catvoidbias\"></li>\n<li>请注意人类水平的误差取决于具体目的，比如医学图像分类，不同的人的误差差率是不同的，估计贝叶斯误差可以使用最专业的团队的误差率<br><img src=\"http://p15i7i801.bkt.clouddn.com/22bcdef589b4b7470d106dd2f19a7e21.png\" alt=\"ng_reducingavoid\"></li>\n</ul>\n<h3 id=\"理解人类的表现\"><a href=\"#理解人类的表现\" class=\"headerlink\" title=\"理解人类的表现\"></a>理解人类的表现</h3><ul>\n<li>人类擅长处理自然感知任务，如图像、语音及NLP识别，在这些方面超越人类比较困难。在近几年的发展中，特定的几个领域内，如医学图像（皮肤癌、BCG判断）等已经取得了比人类更好的结果。</li>\n<li>而在以下方面，机器学习已经远远超越了人类，它们的共同点是有大量的历史数据<ul>\n<li>广告点击</li>\n<li>推荐</li>\n<li>预测用时——快递及运输</li>\n<li>还贷</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"误差分析\"><a href=\"#误差分析\" class=\"headerlink\" title=\"误差分析\"></a>误差分析</h3><p>在猫分类的例子中，假若你的错误率是10%，你的队员可能注意到有不少猫被识别成了猫，他可能建议你针对狗的优化算法，试想你可以收集更多狗的照片或设计一些只处理狗的算法，这样单独起一个项目会花费上月的时间，在这里建议一个简单的法子</p>\n<ul>\n<li>在开发集中取出100个被错误分类的例子</li>\n<li>取出是狗的例子,找出它的特点，比如模糊的图像、滤镜的图像，这样你就更能理解你的分类器，具体看情况去处理它了。<br><img src=\"http://p15i7i801.bkt.clouddn.com/3cc21a20da564a3f00f4af0954e0937b.png\" alt=\"ng_catdect\"></li>\n<li>对于错误标记的数据是是否需要修正，依情况而定，同样用错误表来分析，若主要问题在其它因素上，暂暂时不用管，反之则分析。如图。<br><img src=\"http://p15i7i801.bkt.clouddn.com/09d9124ea0149ce6c349e1657a4fc015.png\" alt=\"ng_bias\"></li>\n<li>不管什么模型，也许很多研究人员、工程师都认为喂数据即可，其它人工分析是很有必要的，至少NG经常这么干，手工和这些错误做斗争可以快速的了解到工作的重点，这是很值得做的。强烈建议</li>\n</ul>\n<h3 id=\"快速的搭建系统并迅速迭代\"><a href=\"#快速的搭建系统并迅速迭代\" class=\"headerlink\" title=\"快速的搭建系统并迅速迭代\"></a>快速的搭建系统并迅速迭代</h3><ul>\n<li>寻找好特定领域的方向，划分训练开发集并设定单一评估指标，迅速搭建初始系统并使用偏差/方差分析或误差分析理论去优化下一步。</li>\n<li>实际情况训练和测试分布往往不一致，两者数据都打散然后划分是不可取的，目标的准心就有偏差了。经常的做法是，将测试数据加入训练集，而开发集和测试集只使用目标真实数据。<br><img src=\"http://p15i7i801.bkt.clouddn.com/001e95d226208589b6ac79293b6e7cd0.png\" alt=\"ng_diffdis\"></li>\n<li>在不匹配数据划分的情况下的误差分析，我们往往加入训练开发混合集去定位误差是否是数据匹配问题导致的。对于下图最右边deverror和testerror比trainerror还小的情况，可以通过第二张片子来进一步分析，图上是车镜语音场景的一个例子。<br><img src=\"http://p15i7i801.bkt.clouddn.com/9765181a9909f8540596dcdb5cd05d75.png\" alt=\"NG_trainde\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/a13ab0be285d7f4d6ee8e3220231b042.png\" alt=\"ng_moreformula\"></li>\n<li>对于确定是数据不匹配的问题导致的误差，没有系统的解决方案，可以从以下两个角度尝试：<ul>\n<li>手动分析误差并理解训练和开发测试集的分布差异</li>\n<li>使得测试集的数据分布向测试集靠近，可能通过增加数据或人造数据的方式</li>\n</ul>\n</li>\n<li>人造数据要注意的是，防止增加一小部分数据而造成模型对这一部分数据过拟合，用汽车识别的例子，若只取游戏中的车的合成图片，即便有海量的张数，但汽车的各类只有20种，这样就不行。<br><img src=\"http://p15i7i801.bkt.clouddn.com/c7c6fb7cc985c944c32ca31d9eaef00b.png\" alt=\"ng_artificalsynthesis\"></li>\n</ul>\n<h3 id=\"迁移学习\"><a href=\"#迁移学习\" class=\"headerlink\" title=\"迁移学习\"></a>迁移学习</h3><ul>\n<li>迁移学习的过程，如下图所示，把输出层换成自己的层和输出层，再用自有数据训练。从原理上来说即利用其它模型已经学习到的特征，如轮廓、边、角等等基础特征<br><img src=\"http://p15i7i801.bkt.clouddn.com/561cacd18c3d8ecdc8a228a19260f6ae.png\" alt=\"ng_transfer\"></li>\n<li>从原理上理解，什么场景下适合迁移学习呢，对于那些比较难收集数据的场景，如射线科，如果有基础特征类似的基础场景已经较好的训练了便可以尝试迁移</li>\n</ul>\n<h3 id=\"多任务学习\"><a href=\"#多任务学习\" class=\"headerlink\" title=\"多任务学习\"></a>多任务学习</h3><ul>\n<li>原理，注意和softmax的区别，它的目标有多个<br><img src=\"http://p15i7i801.bkt.clouddn.com/23c7fec2fb7b2bc578289f69ddeb0fa1.png\" alt=\"ng_MULTITASK\"></li>\n<li>什么情况下多任务学习<br><img src=\"http://p15i7i801.bkt.clouddn.com/d01097fa3af60c0103a5921c0bd989d0.png\" alt=\"ng_shouldmulti\"></li>\n</ul>\n<h3 id=\"端到端的深度学习及适用场景\"><a href=\"#端到端的深度学习及适用场景\" class=\"headerlink\" title=\"端到端的深度学习及适用场景\"></a>端到端的深度学习及适用场景</h3><ul>\n<li>它省去了传统机器学习的各类环节，在以往也许有的研究人员终其一生都在设计这些环节<br><img src=\"http://p15i7i801.bkt.clouddn.com/3d78944e1e5fa81760fc5ac9700e0182.png\" alt=\"ng_dotdot\"></li>\n<li>在人脸识别中我们先目标检测再做识别，如果要端对端的话，这需要大量的数据。传统机器翻译需要先做文本分析，再提取特征等等，端对端的学习在这种场景下非常适应，因为它有大量的数据。</li>\n<li>因此关键的问题是有没有</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"机器学习策略\"><a href=\"#机器学习策略\" class=\"headerlink\" title=\"机器学习策略\"></a>机器学习策略</h2><h3 id=\"常见方法及局限\"><a href=\"#常见方法及局限\" class=\"headerlink\" title=\"常见方法及局限\"></a>常见方法及局限</h3><p>搭建深度学习模型后，模型达到了一定的准确率，若要再度提升算法效率，或许你会<br><img src=\"http://p15i7i801.bkt.clouddn.com/a6cbb32e4ed57cb8919ab52d32385fa6.png\" alt=\"ng_ff\"><br>这里的问题在于，如此尝试诸多方法，或许半年就过去了，并且不见得会多有效。因此我们需要策略来加速这一过程，这正是此门课的独特之处。</p>\n<h3 id=\"正交化\"><a href=\"#正交化\" class=\"headerlink\" title=\"正交化\"></a>正交化</h3><p>使用早期的电视屏幕调整作比喻，分为上下钮、左右钮、梯形钮，此种情况更容易控制屏幕。这样在训练集、开发集、测试集达到好的效果之后，再将模型在真实世界使用。</p>\n<h3 id=\"使用单个评估指标\"><a href=\"#使用单个评估指标\" class=\"headerlink\" title=\"使用单个评估指标\"></a>使用单个评估指标</h3><ul>\n<li>如查准及查全，使用F1 score代替，F1分数值是查准及查全的调和平均值——harmanic mean,当然它近似平均值</li>\n<li>又如在真实的不同场景中，比如美国、中国、印度的图片识别猫的准确率不一致，可以直接求它们的平均值</li>\n<li>以上针对算法的迭代优化选择而言，可以提高效率</li>\n</ul>\n<h3 id=\"优化指标及满足指标\"><a href=\"#优化指标及满足指标\" class=\"headerlink\" title=\"优化指标及满足指标\"></a>优化指标及满足指标</h3><p>即最优的，优化项目及满足项目，有多个指标时，可以先限定某些指标而专注于优化其中一个指标。举例而言，比如流行的智能音箱，优化唤醒率，而限制假阳率为24小时内最多1次，以此标准迭代算法。</p>\n<h3 id=\"训练、开发、测试集\"><a href=\"#训练、开发、测试集\" class=\"headerlink\" title=\"训练、开发、测试集\"></a>训练、开发、测试集</h3><ul>\n<li>开发集，叫称为交叉验证集，开发集的分布应于测试集一致</li>\n<li>在早期机器学习阶段7：3，6：2：2是常见的划分方法，而在现代深度学习算法习惯使用大量的数据，故可用更小的比例使用到开发集和测试集。谨记住测试集的作用，它是为了评估投产系统的性能，它能高置信度评估的效果即可 ，所以它可能只要1万或十万即可，这远远比所有数据20%的比例少</li>\n<li>在反色情猫分类器的例子中，将机器学习的过程分为独立两步，每一步找目标，即确定单一评估指标，第二步对准和射击目标<br><img src=\"http://p15i7i801.bkt.clouddn.com/908a5e3dc3f7dc7d1f94c07c53ded776.png\" alt=\"ng_catclassify\"></li>\n</ul>\n<h3 id=\"机器的表现和人的表现\"><a href=\"#机器的表现和人的表现\" class=\"headerlink\" title=\"机器的表现和人的表现\"></a>机器的表现和人的表现</h3><ul>\n<li>机器学习的准确度基本符合以下曲线,在追上人类以前发展很快，超越人类后发展很慢，它的上限即贝叶斯误差，也叫贝叶斯最优误差。<br><img src=\"http://p15i7i801.bkt.clouddn.com/fb704ed94c43e03f1ae2ef08d9cd3506.png\" alt=\"ng_bayeserror\"></li>\n<li>低于人类的时候发展快的原因可以从三个角度分析，一为可从人类视角分析，二为可获取人工标签，三可以更好的分析偏差和方差</li>\n</ul>\n<h3 id=\"可避免的偏差与方差\"><a href=\"#可避免的偏差与方差\" class=\"headerlink\" title=\"可避免的偏差与方差\"></a>可避免的偏差与方差</h3><ul>\n<li>加入概念：<a href=\"https://www.zhihu.com/question/27068705\" target=\"_blank\" rel=\"noopener\">泛化误差</a>：在机器学习中泛化误差是用来衡量一个学习机器推广未知数据的能力，一图胜千言<br><img src=\"http://p15i7i801.bkt.clouddn.com/8e88161f43c636e5bebe07028dbe1934.png\" alt=\"ng_add_error\"></li>\n<li>举猫分类器而言，可以使用人类的偏差率来接近贝叶斯误差，因为人类在这方面很擅长。若偏差已没有增进空间，可以在考虑减少方差，方差可以通过正则或加数据来减少。<br><img src=\"http://p15i7i801.bkt.clouddn.com/e9853c9e01bb2c77abd481430fb06680.png\" alt=\"ng_catvoidbias\"></li>\n<li>请注意人类水平的误差取决于具体目的，比如医学图像分类，不同的人的误差差率是不同的，估计贝叶斯误差可以使用最专业的团队的误差率<br><img src=\"http://p15i7i801.bkt.clouddn.com/22bcdef589b4b7470d106dd2f19a7e21.png\" alt=\"ng_reducingavoid\"></li>\n</ul>\n<h3 id=\"理解人类的表现\"><a href=\"#理解人类的表现\" class=\"headerlink\" title=\"理解人类的表现\"></a>理解人类的表现</h3><ul>\n<li>人类擅长处理自然感知任务，如图像、语音及NLP识别，在这些方面超越人类比较困难。在近几年的发展中，特定的几个领域内，如医学图像（皮肤癌、BCG判断）等已经取得了比人类更好的结果。</li>\n<li>而在以下方面，机器学习已经远远超越了人类，它们的共同点是有大量的历史数据<ul>\n<li>广告点击</li>\n<li>推荐</li>\n<li>预测用时——快递及运输</li>\n<li>还贷</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"误差分析\"><a href=\"#误差分析\" class=\"headerlink\" title=\"误差分析\"></a>误差分析</h3><p>在猫分类的例子中，假若你的错误率是10%，你的队员可能注意到有不少猫被识别成了猫，他可能建议你针对狗的优化算法，试想你可以收集更多狗的照片或设计一些只处理狗的算法，这样单独起一个项目会花费上月的时间，在这里建议一个简单的法子</p>\n<ul>\n<li>在开发集中取出100个被错误分类的例子</li>\n<li>取出是狗的例子,找出它的特点，比如模糊的图像、滤镜的图像，这样你就更能理解你的分类器，具体看情况去处理它了。<br><img src=\"http://p15i7i801.bkt.clouddn.com/3cc21a20da564a3f00f4af0954e0937b.png\" alt=\"ng_catdect\"></li>\n<li>对于错误标记的数据是是否需要修正，依情况而定，同样用错误表来分析，若主要问题在其它因素上，暂暂时不用管，反之则分析。如图。<br><img src=\"http://p15i7i801.bkt.clouddn.com/09d9124ea0149ce6c349e1657a4fc015.png\" alt=\"ng_bias\"></li>\n<li>不管什么模型，也许很多研究人员、工程师都认为喂数据即可，其它人工分析是很有必要的，至少NG经常这么干，手工和这些错误做斗争可以快速的了解到工作的重点，这是很值得做的。强烈建议</li>\n</ul>\n<h3 id=\"快速的搭建系统并迅速迭代\"><a href=\"#快速的搭建系统并迅速迭代\" class=\"headerlink\" title=\"快速的搭建系统并迅速迭代\"></a>快速的搭建系统并迅速迭代</h3><ul>\n<li>寻找好特定领域的方向，划分训练开发集并设定单一评估指标，迅速搭建初始系统并使用偏差/方差分析或误差分析理论去优化下一步。</li>\n<li>实际情况训练和测试分布往往不一致，两者数据都打散然后划分是不可取的，目标的准心就有偏差了。经常的做法是，将测试数据加入训练集，而开发集和测试集只使用目标真实数据。<br><img src=\"http://p15i7i801.bkt.clouddn.com/001e95d226208589b6ac79293b6e7cd0.png\" alt=\"ng_diffdis\"></li>\n<li>在不匹配数据划分的情况下的误差分析，我们往往加入训练开发混合集去定位误差是否是数据匹配问题导致的。对于下图最右边deverror和testerror比trainerror还小的情况，可以通过第二张片子来进一步分析，图上是车镜语音场景的一个例子。<br><img src=\"http://p15i7i801.bkt.clouddn.com/9765181a9909f8540596dcdb5cd05d75.png\" alt=\"NG_trainde\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/a13ab0be285d7f4d6ee8e3220231b042.png\" alt=\"ng_moreformula\"></li>\n<li>对于确定是数据不匹配的问题导致的误差，没有系统的解决方案，可以从以下两个角度尝试：<ul>\n<li>手动分析误差并理解训练和开发测试集的分布差异</li>\n<li>使得测试集的数据分布向测试集靠近，可能通过增加数据或人造数据的方式</li>\n</ul>\n</li>\n<li>人造数据要注意的是，防止增加一小部分数据而造成模型对这一部分数据过拟合，用汽车识别的例子，若只取游戏中的车的合成图片，即便有海量的张数，但汽车的各类只有20种，这样就不行。<br><img src=\"http://p15i7i801.bkt.clouddn.com/c7c6fb7cc985c944c32ca31d9eaef00b.png\" alt=\"ng_artificalsynthesis\"></li>\n</ul>\n<h3 id=\"迁移学习\"><a href=\"#迁移学习\" class=\"headerlink\" title=\"迁移学习\"></a>迁移学习</h3><ul>\n<li>迁移学习的过程，如下图所示，把输出层换成自己的层和输出层，再用自有数据训练。从原理上来说即利用其它模型已经学习到的特征，如轮廓、边、角等等基础特征<br><img src=\"http://p15i7i801.bkt.clouddn.com/561cacd18c3d8ecdc8a228a19260f6ae.png\" alt=\"ng_transfer\"></li>\n<li>从原理上理解，什么场景下适合迁移学习呢，对于那些比较难收集数据的场景，如射线科，如果有基础特征类似的基础场景已经较好的训练了便可以尝试迁移</li>\n</ul>\n<h3 id=\"多任务学习\"><a href=\"#多任务学习\" class=\"headerlink\" title=\"多任务学习\"></a>多任务学习</h3><ul>\n<li>原理，注意和softmax的区别，它的目标有多个<br><img src=\"http://p15i7i801.bkt.clouddn.com/23c7fec2fb7b2bc578289f69ddeb0fa1.png\" alt=\"ng_MULTITASK\"></li>\n<li>什么情况下多任务学习<br><img src=\"http://p15i7i801.bkt.clouddn.com/d01097fa3af60c0103a5921c0bd989d0.png\" alt=\"ng_shouldmulti\"></li>\n</ul>\n<h3 id=\"端到端的深度学习及适用场景\"><a href=\"#端到端的深度学习及适用场景\" class=\"headerlink\" title=\"端到端的深度学习及适用场景\"></a>端到端的深度学习及适用场景</h3><ul>\n<li>它省去了传统机器学习的各类环节，在以往也许有的研究人员终其一生都在设计这些环节<br><img src=\"http://p15i7i801.bkt.clouddn.com/3d78944e1e5fa81760fc5ac9700e0182.png\" alt=\"ng_dotdot\"></li>\n<li>在人脸识别中我们先目标检测再做识别，如果要端对端的话，这需要大量的数据。传统机器翻译需要先做文本分析，再提取特征等等，端对端的学习在这种场景下非常适应，因为它有大量的数据。</li>\n<li>因此关键的问题是有没有</li>\n</ul>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjclxs5gx0001ajslt1hr2gn2","category_id":"cjclxs5h60004ajslm15bu3oe","_id":"cjclxs5hl000eajslgush030d"},{"post_id":"cjclxs5he0009ajsl89ve49sj","category_id":"cjclxs5h60004ajslm15bu3oe","_id":"cjclxs5hr000iajslcss5p3dq"},{"post_id":"cjclxs5hh000cajslvjgm1flm","category_id":"cjclxs5hf000aajslq2jdguop","_id":"cjclxs5hu000lajslgkbskbkr"},{"post_id":"cjclxs5h20003ajslnytlim7t","category_id":"cjclxs5hf000aajslq2jdguop","_id":"cjclxs5hw000pajslc6hjobtx"},{"post_id":"cjclxs5hj000dajslb0bif253","category_id":"cjclxs5hf000aajslq2jdguop","_id":"cjclxs5hx000rajsldrrpl6op"},{"post_id":"cjclxs5hq000hajslfndxkf2b","category_id":"cjclxs5hf000aajslq2jdguop","_id":"cjclxs5hy000tajslv2plhk7r"},{"post_id":"cjclxs5hb0007ajsl6obb6ujy","category_id":"cjclxs5h60004ajslm15bu3oe","_id":"cjclxs5hy000uajsl5fd5743y"},{"post_id":"cjclxs5hs000kajslkvpdc1qr","category_id":"cjclxs5hf000aajslq2jdguop","_id":"cjclxs5hz000vajslogfsqhpv"},{"post_id":"cjclxs5hv000oajslvaabrh6v","category_id":"cjclxs5hf000aajslq2jdguop","_id":"cjclxs5hz000yajslce7n4bcl"},{"post_id":"cjclxs5hw000qajslbyrm3baz","category_id":"cjclxs5hf000aajslq2jdguop","_id":"cjclxs5i00010ajslo1t8ti1e"}],"PostTag":[{"post_id":"cjclxs5gx0001ajslt1hr2gn2","tag_id":"cjclxs5h90005ajslvl85ng6d","_id":"cjclxs5hs000jajslsl0ptmpn"},{"post_id":"cjclxs5gx0001ajslt1hr2gn2","tag_id":"cjclxs5hg000bajslp0knoqm2","_id":"cjclxs5hu000majsl2csq3rgx"},{"post_id":"cjclxs5h20003ajslnytlim7t","tag_id":"cjclxs5hl000gajslyqimq0dt","_id":"cjclxs5hz000xajsl7oj5t3lr"},{"post_id":"cjclxs5h20003ajslnytlim7t","tag_id":"cjclxs5hu000najslhoowzuv3","_id":"cjclxs5hz000zajslpatssabf"},{"post_id":"cjclxs5h20003ajslnytlim7t","tag_id":"cjclxs5hy000sajsls00nzy8f","_id":"cjclxs5i10012ajslp4r6tjhn"},{"post_id":"cjclxs5hb0007ajsl6obb6ujy","tag_id":"cjclxs5hz000wajslfumh8fib","_id":"cjclxs5i30014ajslubesn0tc"},{"post_id":"cjclxs5hb0007ajsl6obb6ujy","tag_id":"cjclxs5hg000bajslp0knoqm2","_id":"cjclxs5i30015ajsl2pvrc6nc"},{"post_id":"cjclxs5hd0008ajslx5lun51t","tag_id":"cjclxs5i20013ajsl7lryqdp8","_id":"cjclxs5i30017ajslkzmehgyt"},{"post_id":"cjclxs5he0009ajsl89ve49sj","tag_id":"cjclxs5i30016ajslturgm6kv","_id":"cjclxs5i5001aajsli87l8jc4"},{"post_id":"cjclxs5he0009ajsl89ve49sj","tag_id":"cjclxs5hg000bajslp0knoqm2","_id":"cjclxs5i5001bajslbegc4vrq"},{"post_id":"cjclxs5hh000cajslvjgm1flm","tag_id":"cjclxs5i50019ajsl0779fxnu","_id":"cjclxs5i6001dajslwd55pb6y"},{"post_id":"cjclxs5hh000cajslvjgm1flm","tag_id":"cjclxs5hg000bajslp0knoqm2","_id":"cjclxs5i6001eajslv33bnyvi"},{"post_id":"cjclxs5hj000dajslb0bif253","tag_id":"cjclxs5i5001cajslo57x07q3","_id":"cjclxs5i9001iajslcd6mhegr"},{"post_id":"cjclxs5hj000dajslb0bif253","tag_id":"cjclxs5i6001fajslxeouahp8","_id":"cjclxs5i9001jajslbow3ri45"},{"post_id":"cjclxs5hj000dajslb0bif253","tag_id":"cjclxs5hy000sajsls00nzy8f","_id":"cjclxs5ia001lajsl0k8drhca"},{"post_id":"cjclxs5hq000hajslfndxkf2b","tag_id":"cjclxs5i5001cajslo57x07q3","_id":"cjclxs5ib001oajslbxoes7b6"},{"post_id":"cjclxs5hq000hajslfndxkf2b","tag_id":"cjclxs5i6001fajslxeouahp8","_id":"cjclxs5ib001pajsluw5mg8xj"},{"post_id":"cjclxs5hq000hajslfndxkf2b","tag_id":"cjclxs5hy000sajsls00nzy8f","_id":"cjclxs5ic001rajslvnqsb7k8"},{"post_id":"cjclxs5hs000kajslkvpdc1qr","tag_id":"cjclxs5i5001cajslo57x07q3","_id":"cjclxs5ie001uajsl4mshk1o8"},{"post_id":"cjclxs5hs000kajslkvpdc1qr","tag_id":"cjclxs5i6001fajslxeouahp8","_id":"cjclxs5ie001vajsldzs5okdw"},{"post_id":"cjclxs5hs000kajslkvpdc1qr","tag_id":"cjclxs5hy000sajsls00nzy8f","_id":"cjclxs5if001xajsl96g9dycl"},{"post_id":"cjclxs5hv000oajslvaabrh6v","tag_id":"cjclxs5i50019ajsl0779fxnu","_id":"cjclxs5if001yajsl57bdnkqj"},{"post_id":"cjclxs5hv000oajslvaabrh6v","tag_id":"cjclxs5hg000bajslp0knoqm2","_id":"cjclxs5ig0020ajsl4l3bb0hm"},{"post_id":"cjclxs5hw000qajslbyrm3baz","tag_id":"cjclxs5i5001cajslo57x07q3","_id":"cjclxs5ih0022ajsl8ik4efow"},{"post_id":"cjclxs5hw000qajslbyrm3baz","tag_id":"cjclxs5i6001fajslxeouahp8","_id":"cjclxs5ii0023ajsl8x8towxa"},{"post_id":"cjclxs5hw000qajslbyrm3baz","tag_id":"cjclxs5hy000sajsls00nzy8f","_id":"cjclxs5ii0024ajslyq0xux9a"}],"Tag":[{"name":"系统结构","_id":"cjclxs5h90005ajslvl85ng6d"},{"name":"经典著作","_id":"cjclxs5hg000bajslp0knoqm2"},{"name":"神经网络","_id":"cjclxs5hl000gajslyqimq0dt"},{"name":"Geoffrey hinton","_id":"cjclxs5hu000najslhoowzuv3"},{"name":"公开课","_id":"cjclxs5hy000sajsls00nzy8f"},{"name":"计算机语言","_id":"cjclxs5hz000wajslfumh8fib"},{"name":"draft","_id":"cjclxs5i20013ajsl7lryqdp8"},{"name":"数据结构","_id":"cjclxs5i30016ajslturgm6kv"},{"name":"机器学习","_id":"cjclxs5i50019ajsl0779fxnu"},{"name":"深度学习","_id":"cjclxs5i5001cajslo57x07q3"},{"name":"Andrew NG","_id":"cjclxs5i6001fajslxeouahp8"}]}}