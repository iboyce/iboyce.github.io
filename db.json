{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"5a4ab475cc1f2119b2043c94519bd8a184f46992","modified":1512468766876},{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1512468451209},{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1512468451211},{"_id":"themes/next/.git","hash":"042ff34da0707513a5681580b37513c890c671ef","modified":1512711688940},{"_id":"themes/next/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1512468451212},{"_id":"themes/next/.gitignore","hash":"ee0b13c268cc8695d3883a5da84930af02d4ed08","modified":1512468451218},{"_id":"themes/next/.hound.yml","hash":"289dcf5bfe92dbd680d54d6e0668f41c9c9c0c78","modified":1512468451219},{"_id":"themes/next/.javascript_ignore","hash":"cd250ad74ca22bd2c054476456a73d9687f05f87","modified":1512468451219},{"_id":"themes/next/.jshintrc","hash":"b7d23f2ce8d99fa073f22f9960605f318acd7710","modified":1512468451220},{"_id":"themes/next/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1512468451221},{"_id":"themes/next/LICENSE","hash":"ec44503d7e617144909e54533754f0147845f0c5","modified":1512468451223},{"_id":"themes/next/.travis.yml","hash":"6674fbdfe0d0c03b8a04527ffb8ab66a94253acd","modified":1512468451222},{"_id":"themes/next/README.cn.md","hash":"419b60d064a4ac66565ddeec1be55802acf68c8b","modified":1512468451224},{"_id":"themes/next/README.md","hash":"631d68e9cbced2f11cd976bf883b7d8b08b9b365","modified":1512468451225},{"_id":"themes/next/_config.yml","hash":"55c41dd2fc38ff03b18187f34e5ebbaff9bd7dc4","modified":1513573813329},{"_id":"themes/next/bower.json","hash":"47471a8f13528dc4052b746db5b4be2375682173","modified":1512468451227},{"_id":"themes/next/gulpfile.coffee","hash":"412defab3d93d404b7c26aaa0279e2e586e97454","modified":1512468451228},{"_id":"themes/next/package.json","hash":"39370e2aadf1f9a7c105edff064c6e47682b3932","modified":1512468451327},{"_id":"source/_posts/Computer-System-A-Programmer-s-Perspective.md","hash":"fc300385a77e399f6c8b9a050195a3d66ba776c3","modified":1514356117417},{"_id":"source/_posts/Thinking-in-Java.md","hash":"f05d7bdf99ed8b60c817647964c7525bc7c60996","modified":1514368486376},{"_id":"source/_posts/Practical-introduction-to-data-structures-and-algorithm-analysis.md","hash":"c30a5effe453d7c7a6499c4af8754a598c9c494c","modified":1513574555120},{"_id":"source/_posts/action-in-machine-learning.md","hash":"4fb098f0991077a1bd1acc151e1e2f45175b7fb1","modified":1513679173696},{"_id":"source/_posts/Neural-Networks-for-Machine-Learning.md","hash":"aa14e7b846cbf886c262d3ba8c1fe3967e99e8d2","modified":1513675361763},{"_id":"source/_posts/改善深层神经网络：超参数调试、正则化以及优化.md","hash":"555c0586d8215f7f194a12a804ad67c9b344e071","modified":1513829783274},{"_id":"source/_posts/draft.md","hash":"01ad389a4e594c441c81c47f928f68c900ee5345","modified":1514369036224},{"_id":"source/_posts/神经网络与深度学习.md","hash":"6aa49dc307dbbceffc5f903f0cc59b2213256f01","modified":1513829778786},{"_id":"source/_posts/集体智慧编程.md","hash":"16bc4669f333c7b22e8214ae0442ae0833f1fc2a","modified":1514356214171},{"_id":"source/categories/index.md","hash":"050f7dc3ea25c3fa162d9720fa885e69fdc2962e","modified":1512552165295},{"_id":"source/tags/index.md","hash":"047bc48ea113247ae48eece5212d62ccae459654","modified":1512551944816},{"_id":"source/about/index.md","hash":"9433482f87818b221e552b822e9fcd5876191868","modified":1512552163070},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"1228506a940114288d61812bfe60c045a0abeac1","modified":1512468451216},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5adfad3ef1b870063e621bc0838268eb2c7c697a","modified":1512468451214},{"_id":"themes/next/languages/de.yml","hash":"fd02d9c2035798d5dc7c1a96b4c3e24b05b31a47","modified":1512468451230},{"_id":"themes/next/languages/fr-FR.yml","hash":"efeeb55d5c4add54ad59a612fc0630ee1300388c","modified":1512468451232},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1512468451217},{"_id":"themes/next/languages/default.yml","hash":"b3bcd8934327448a43d9bfada5dd11b1b8c1402e","modified":1512468451231},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"b1ec000babd42bb7ffd26f5ad8aac9b5bec79ae5","modified":1512468451215},{"_id":"themes/next/languages/ja.yml","hash":"37f954e47a3bc669620ca559e3edb3b0072a4be5","modified":1512468451235},{"_id":"themes/next/languages/ko.yml","hash":"dc8f3e8c64eb7c4bb2385025b3006b8efec8b31d","modified":1512468451236},{"_id":"themes/next/languages/it.yml","hash":"a215d016146b1bd92cef046042081cbe0c7f976f","modified":1512468451234},{"_id":"themes/next/languages/nl-NL.yml","hash":"213e7a002b82fb265f69dabafbbc382cfd460030","modified":1512468451237},{"_id":"themes/next/languages/pt-BR.yml","hash":"568d494a1f37726a5375b11452a45c71c3e2852d","modified":1512468451238},{"_id":"themes/next/languages/vi.yml","hash":"a9b89ebd3e5933033d1386c7c56b66c44aca299a","modified":1512468451240},{"_id":"themes/next/languages/pt.yml","hash":"2efcd240c66ab1a122f061505ca0fb1e8819877b","modified":1512468451239},{"_id":"themes/next/languages/zh-Hans.yml","hash":"39e4d2087a8f529ad3dbe3a1d7f8e2e6d31d915a","modified":1513573813329},{"_id":"themes/next/languages/zh-hk.yml","hash":"fe0d45807d015082049f05b54714988c244888da","modified":1512468451242},{"_id":"themes/next/languages/ru.yml","hash":"e33ee44e80f82e329900fc41eb0bb6823397a4d6","modified":1512468451239},{"_id":"themes/next/languages/zh-tw.yml","hash":"432463b481e105073accda16c3e590e54c8e7b74","modified":1512468451243},{"_id":"themes/next/languages/en.yml","hash":"2f4b4776ca1a08cc266a19afb0d1350a3926f42c","modified":1512468451231},{"_id":"themes/next/languages/id.yml","hash":"dccae33e2a5b3c9f11c0e05ec4a7201af1b25745","modified":1512468451233},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1512468451621},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1512468451622},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1512468451623},{"_id":"themes/next/layout/_layout.swig","hash":"2164570bb05db11ee4bcfbbb5d183a759afe9d07","modified":1512468451247},{"_id":"themes/next/layout/category.swig","hash":"3cbb3f72429647411f9e85f2544bdf0e3ad2e6b2","modified":1512468451322},{"_id":"themes/next/layout/archive.swig","hash":"9a2c14874a75c7085d2bada5e39201d3fc4fd2b4","modified":1512468451321},{"_id":"themes/next/layout/post.swig","hash":"7a6ce102ca82c3a80f776e555dddae1a9981e1ed","modified":1512468451324},{"_id":"themes/next/layout/page.swig","hash":"e8fcaa641d46930237675d2ad4b56964d9e262e9","modified":1512468451324},{"_id":"themes/next/layout/schedule.swig","hash":"87ad6055df01fa2e63e51887d34a2d8f0fbd2f5a","modified":1512468451325},{"_id":"themes/next/layout/tag.swig","hash":"34e1c016cbdf94a31f9c5d494854ff46b2a182e9","modified":1512468451326},{"_id":"themes/next/scripts/merge-configs.js","hash":"5758f8f3f12d17bc80da65bb808a20b3a8aae186","modified":1512468451337},{"_id":"themes/next/layout/index.swig","hash":"555a357ecf17128db4e29346c92bb6298e66547a","modified":1512468451323},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1512468451347},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512468451461},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1512468451462},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1512468451464},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1512468451465},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1512468451463},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1512468451467},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1512468451467},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1512468451468},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1512468451469},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1512468451471},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1512468451470},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1512468451472},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1512468451473},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1512468451474},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1512468451475},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1512468451476},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1512468451477},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1512468451479},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1512468451245},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1512468451246},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1512468451478},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"8c56dd26157cbc580ae41d97ac34b90ab48ced3f","modified":1512468451249},{"_id":"themes/next/layout/_macro/post.swig","hash":"4ba938822d56c597490f0731893eaa2443942e0f","modified":1512468451251},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"f83befdc740beb8dc88805efd7fbb0fef9ed19be","modified":1512468451250},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"b9f9959225876fb56fb3fba96306d19396e704d4","modified":1512468451253},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4adc65a602d1276615da3b887dcbf2ac68e7382b","modified":1512468451255},{"_id":"themes/next/layout/_macro/reward.swig","hash":"357d86ec9586705bfbb2c40a8c7d247a407db21a","modified":1512468451252},{"_id":"themes/next/layout/_partials/footer.swig","hash":"ba96bab0c127aea98c9fddf8ebe283fddb6d0a61","modified":1513573813329},{"_id":"themes/next/layout/_partials/head.swig","hash":"f14a39dad1ddd98e6d3ceb25dda092ba80d391b5","modified":1512468451257},{"_id":"themes/next/layout/_partials/header.swig","hash":"c54b32263bc8d75918688fb21f795103b3f57f03","modified":1512468451261},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"77c61e0baea3544df361b7338c3cd13dc84dde22","modified":1512468451262},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"1634fb887842698e01ff6e632597fe03c75d2d01","modified":1512468451263},{"_id":"themes/next/layout/_partials/search.swig","hash":"b4ebe4a52a3b51efe549dd1cdee846103664f5eb","modified":1512468451264},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"c0f5a0955f69ca4ed9ee64a2d5f8aa75064935ad","modified":1512468451274},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"931808ad9b8d8390c0dcf9bdeb0954eeb9185d68","modified":1512468451275},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9be624634703be496a5d2535228bc568a8373af9","modified":1512468451282},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"ba75672183d94f1de7c8bd0eeee497a58c70e889","modified":1512468451307},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"8301c9600bb3e47f7fb98b0e0332ef3c51bb1688","modified":1512468451308},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"e2e4eae391476da994045ed4c7faf5e05aca2cd7","modified":1512468451254},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"fa882641da3bd83d9a58a8a97f9d4c62a9ee7b5c","modified":1512468451310},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"554ec568e9d2c71e4a624a8de3cb5929050811d6","modified":1512468451310},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"a0bd3388587fd943baae0d84ca779a707fbcad89","modified":1512468451309},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"db15d7e1552aa2d2386a6b8a33b3b3a40bf9e43d","modified":1512468451311},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"9a188938d46931d5f3882a140aa1c48b3a893f0c","modified":1512468451312},{"_id":"themes/next/scripts/tags/button.js","hash":"eddbb612c15ac27faf11c59c019ce188f33dec2c","modified":1512468451347},{"_id":"themes/next/scripts/tags/exturl.js","hash":"5022c0ba9f1d13192677cf1fd66005c57c3d0f53","modified":1512468451347},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"99b66949f18398689b904907af23c013be1b978f","modified":1512468451347},{"_id":"themes/next/scripts/tags/label.js","hash":"6f00952d70aadece844ce7fd27adc52816cc7374","modified":1512468451347},{"_id":"themes/next/scripts/tags/full-image.js","hash":"c9f833158c66bd72f627a0559cf96550e867aa72","modified":1512468451347},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"bcba2ff25cd7850ce6da322d8bd85a8dd00b5ceb","modified":1512468451347},{"_id":"themes/next/scripts/tags/tabs.js","hash":"aa7fc94a5ec27737458d9fe1a75c0db7593352fd","modified":1512468451347},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"ac681b0d0d8d39ba3817336c0270c6787c2b6b70","modified":1512468451347},{"_id":"themes/next/scripts/tags/note.js","hash":"f7eae135f35cdab23728e9d0d88b76e00715faa0","modified":1512468451347},{"_id":"themes/next/source/css/main.styl","hash":"a91dbb7ef799f0a171b5e726c801139efe545176","modified":1512468451461},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512468451279},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512468451279},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512468451430},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512468451430},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512468451440},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512468451461},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1512468451461},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1512468451523},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1512468451523},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"b02737510e9b89aeed6b54f89f602a9c24b06ff2","modified":1512468451533},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"9be892a4e14e0da18ff9cb962c9ef71f163b1b22","modified":1512468451533},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"672d3b5767e0eacd83bb41b188c913f2cf754793","modified":1512468451533},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"bf3eef9d647cd7c9b62feda3bc708c6cdd7c0877","modified":1512468451553},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"a9b3ee1e4db71a0e4ea6d5bed292d176dd68b261","modified":1512468451553},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1512468451563},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1512468451553},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1512468451563},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1512468451553},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1512468451563},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"90fa628f156d8045357ff11eaf32e61abacf10e8","modified":1512468451574},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"68a9b9d53126405b0fa5f3324f1fb96dbcc547aa","modified":1512468451553},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"b930297cb98b8e1dbd5abe9bc1ed9d5935d18ce8","modified":1512468451574},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4ded6fee668544778e97e38c2b211fc56c848e77","modified":1512468451574},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"f4a570908f6c89c6edfb1c74959e733eaadea4f2","modified":1512468451575},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"e0acf1db27b0cc16128a59c46db1db406b5c4c58","modified":1512468451574},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1512468451584},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"bf773ad48a0b9aa77681a89d7569eefc0f7b7b18","modified":1512468451576},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1512468451585},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1512468451587},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1512468451588},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1512468451590},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1512468451591},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1512468451589},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1512468451594},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1512468451593},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1512468451592},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1512468451553},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"14264a210bf94232d58d7599ea2ba93bfa4fb458","modified":1512468451579},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1512468451597},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"8aaa675f577d5501f5f22d5ccb07c2b76310b690","modified":1512468451598},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1512468451595},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"865d6c1328ab209a4376b9d2b7a7824369565f28","modified":1512468451574},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1512468451596},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"e33aa8fa48b6639d8d8b937d13261597dd473b3a","modified":1512468451580},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"2ce5f3bf15c523b9bfc97720d8884bb22602a454","modified":1512468451582},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1512468451617},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"63da5e80ebb61bb66a2794d5936315ca44231f0c","modified":1512468451612},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"92d92860418c4216aa59eb4cb4a556290a7ad9c3","modified":1512468451612},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1512468451619},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1512468451616},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"1512c751d219577d338ac0780fb2bbd9075d5298","modified":1512468451486},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"0289031200c3d4c2bdd801ee10fff13bb2c353e4","modified":1512468451484},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"cb431b54ba9c692165a1f5a12e4c564a560f8058","modified":1512468451483},{"_id":"themes/next/source/js/src/motion.js","hash":"885176ed51d468f662fbf0fc09611f45c7e5a3b1","modified":1512468451487},{"_id":"themes/next/source/js/src/affix.js","hash":"1b509c3b5b290a6f4607f0f06461a0c33acb69b1","modified":1512468451482},{"_id":"themes/next/source/js/src/exturl.js","hash":"a2a0f0de07e46211f74942a468f42ee270aa555c","modified":1512468451484},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"b35a7dc47b634197b93487cea8671a40a9fdffce","modified":1512468451486},{"_id":"themes/next/source/js/src/post-details.js","hash":"93a18271b4123dd8f94f09d1439b47c3c19a8712","modified":1512468451488},{"_id":"themes/next/source/js/src/utils.js","hash":"b7ddc240208d57596a67c78a04a11b0f0bdecc97","modified":1512468451494},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"b7657be25fc52ec67c75ab5481bdcb483573338b","modified":1512468451492},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"02cf91514e41200bc9df5d8bdbeb58575ec06074","modified":1512468451491},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"8148492dd49aa876d32bb7d5b728d3f5bf6f5074","modified":1512468451602},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"2d9a9f38c493fdf7c0b833bb9184b6a1645c11b2","modified":1512468451600},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"46a50b91c98b639c9a2b9265c5a1e66a5c656881","modified":1512468451601},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"f5e487b0d213ca0bd94aa30bc23b240d65081627","modified":1512468451260},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"9e3d133ac5bcc6cb51702c83b2611a49811abad1","modified":1512468451269},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1512468451259},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a8c7f9ca7c605d039a1f3bf4e4d3183700a3dd62","modified":1512468451266},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1512468451265},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1512468451267},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"9b84ab576982b2c3bb0291da49143bc77fba3cc6","modified":1512468451276},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"d9e2d9282f9be6e04eae105964abb81e512bffed","modified":1512468451270},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"d4fbffd7fa8f2090eb32a871872665d90a885fac","modified":1512468451271},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1512468451281},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"0a9cdd6958395fcdffc80ab60f0c6301b63664a5","modified":1512468451272},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"71397a5823e8ec8aad3b68aace13150623b3e19d","modified":1512468451286},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1512468451278},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"ff947f3561b229bc528cb1837d4ca19612219411","modified":1512468451285},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"a10b7f19d7b5725527514622899df413a34a89db","modified":1512468451289},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"753d262911c27baf663fcaf199267133528656af","modified":1512468451287},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"7b11eac3a0685fa1ab2ab6ecff60afc4f15f0d16","modified":1512468451288},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"45f3f629c2aacc381095750e1c8649041a71a84b","modified":1512468451293},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"7d94845f96197d9d84a405fa5d4ede75fb81b225","modified":1512468451290},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"b1e13df83fb2b1d5d513b30b7aa6158b0837daab","modified":1512468451292},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"ccc443b22bd4f8c7ac4145664686c756395b90e0","modified":1512468451291},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"e6d10ee4fb70b3ae1cd37e9e36e000306734aa2e","modified":1512468451294},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"8a399df90dadba5ad4e781445b58f4765aeb701e","modified":1512468451295},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"5a8027328f060f965b3014060bebec1d7cf149c1","modified":1512468451296},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"4c501ea0b9c494181eb3c607c5526a5754e7fbd8","modified":1512468451299},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b83a51bbe0f1e2ded9819070840b0ea145f003a6","modified":1512468451299},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"f9a1647a8f1866deeb94052d1f87a5df99cb1e70","modified":1512468451297},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"1600f340e0225361580c44890568dc07dbcf2c89","modified":1512468451300},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"4dcc3213c033994d342d02b800b6229295433d30","modified":1512468451301},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"9246162d4bc7e949ce1d12d135cbbaf5dc3024ec","modified":1512468451304},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"493bd5999a1061b981922be92d8277a0f9152447","modified":1512468451303},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"4050553d44ba1396174161c9a6bb0f89fa779eca","modified":1512468451305},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"93479642fd076a1257fecc25fcf5d20ccdefe509","modified":1512468451317},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"7e65ff8fe586cd655b0e9d1ad2912663ff9bd36c","modified":1512468451306},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"34599633658f3b0ffb487728b7766e1c7b551f5a","modified":1512468451317},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1512468451318},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"d8c98938719284fa06492c114d99a1904652a555","modified":1512468451320},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1512468451430},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"af7f3e43cbdc4f88c13f101f0f341af96ace3383","modified":1512468451302},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"0e55cbd93852dc3f8ccb44df74d35d9918f847e0","modified":1512468451440},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"7896c3ee107e1a8b9108b6019f1c070600a1e8cc","modified":1512468451440},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"58e7dd5947817d9fc30770712fc39b2f52230d1e","modified":1512468451461},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"4069f918ccc312da86db6c51205fc6c6eaabb116","modified":1512468451461},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"a25408534f8fe6e321db4bbf9dd03335d648fe17","modified":1512468451461},{"_id":"themes/next/source/css/_variables/base.styl","hash":"b1f6ea881a4938a54603d68282b0f8efb4d7915d","modified":1512468451461},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1512468451430},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1512468451574},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"6c26cdb36687d4f0a11dabf5290a909c3506be5c","modified":1512468451503},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"6d586bfcfb7ae48f1b12f76eec82d3ad31947501","modified":1512468451523},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"16b03db23a52623348f37c04544f2792032c1fb6","modified":1512468451523},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1512468451533},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1512468451533},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1512468451533},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1512468451533},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1512468451533},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"82f33ad0842aa9c154d029e0dada2497d4eb1d57","modified":1512468451543},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1512468451533},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"ae6318aeb62ad4ce7a7e9a4cdacd93ffb004f0fb","modified":1512468451543},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"d71602cbca33b9ecdb7ab291b7f86a49530f3601","modified":1512468451543},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"1d6aeda0480d0e4cb6198edf7719d601d4ae2ccc","modified":1512468451553},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1512468451553},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1512468451563},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1512468451563},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1512468451563},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1512468451609},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1512468451610},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"f1d0b5d7af32c423eaa8bb93ab6a0b45655645dc","modified":1512468451490},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"218cc936ba3518a3591b2c9eda46bc701edf7710","modified":1512468451315},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1512468451316},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"237d185ac62ec9877e300947fa0109c44fb8db19","modified":1512468451357},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"8f86f694c0749a18ab3ad6f6df75466ca137a4bc","modified":1512468451357},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1512468451357},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"8b32928686c327151e13d3ab100157f9a03cd59f","modified":1512468451357},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"7ad4081466b397e2a6204141bb7768b7c01bd93c","modified":1512468451357},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"4f2801fc4cf3f31bf2069f41db8c6ce0e3da9e39","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"6eb4bcc3056bd279d000607e8b4dad50d368ca69","modified":1512468451390},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"12662536c7a07fff548abe94171f34b768dd610f","modified":1512468451420},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"1da5c800d025345f212a3bf1be035060f4e5e6ed","modified":1512468451430},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"24ee4b356ff55fc6e58f26a929fa07750002cf29","modified":1512468451430},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"3f40e8a9fe8e7bd5cfc4cf4cbbbcb9539462e973","modified":1512468451430},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1512468451430},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"416988dca389e6e2fdfa51fa7f4ee07eb53f82fb","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"bce344d3a665b4c55230d2a91eac2ad16d6f32fd","modified":1512468451460},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"91ca75492cd51f2553f4d294ed2f48239fcd55eb","modified":1512468451430},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"ea9069645696f86c5df64208490876fe150c8cae","modified":1512468451430},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"4642e30010af8b2b037f5b43146b10a934941958","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"30561ed60fc64f3e4ce85143bdb55faa814743a6","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"86197902dfd3bededba10ba62b8f9f22e0420bde","modified":1512468451461},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"60fa84aa7731760f05f52dd7d8f79b5f74ac478d","modified":1512468451440},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"31127dcbf4c7b4ada53ffbf1638b5fe325b7cbc0","modified":1512468451450},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"a98ad885ee4f48d85b2578a0b9c2bbf166e96733","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"748dbfbf9c08e719ddc775958003c64b00d39dab","modified":1512468451450},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"25d5e45a355ee2093f3b8b8eeac125ebf3905026","modified":1512468451440},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"26666c1f472bf5f3fb9bc62081cca22b4de15ccb","modified":1512468451450},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"d0bfd1bef988c76f7d7dd72d88af6f0908a8b0db","modified":1512468451440},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1512468451440},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"9c99034f8e00d47e978b3959f51eb4a9ded0fcc8","modified":1512468451450},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1512468451450},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9b913b73d31d21f057f97115ffab93cfa578b884","modified":1512468451450},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"4ac683b2bc8531c84d98f51b86957be0e6f830f3","modified":1512468451513},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1512468451574},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1512468451574},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1512468451615},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1512468451499},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1512468451501},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1512468451500},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1512468451498},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1512468451502},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1512468451543},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"6394c48092085788a8c0ef72670b0652006231a1","modified":1512468451543},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1512468451573},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1512468451543},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1512468451573},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"d22b1629cb23a6181bebb70d0cf653ffe4b835c8","modified":1512468451543},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"ee948b4489aedeb548a77c9e45d8c7c5732fd62d","modified":1512468451543},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"51139a4c79573d372a347ef01a493222a1eaf10a","modified":1512468451543},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1512468451573},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"53cde051e0337f4bf42fb8d6d7a79fa3fa6d4ef2","modified":1512468451367},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1512468451367},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"9f73c4696f0907aa451a855444f88fc0698fa472","modified":1512468451357},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"0656e753f182c9f47fef7304c847b7587a85ef0d","modified":1512468451367},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1512468451367},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"50450d9fdc8a2b2be8cfca51e3e1a01ffd636c0b","modified":1512468451367},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"167986d0f649516671ddf7193eebba7b421cd115","modified":1512468451367},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"1a0d059799a298fe17c49a44298d32cebde93785","modified":1512468451367},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1512468451367},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"b6f3a06a94a6ee5470c956663164d58eda818a64","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"f9760ecf186954cee3ba4a149be334e9ba296b89","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"8cf318644acc8b4978537c263290363e21c7f5af","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"caf263d1928496688c0e1419801eafd7e6919ce5","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a200c0a1c5a895ac9dc41e0641a5dfcd766be99b","modified":1512468451369},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"875cbe88d5c7f6248990e2beb97c9828920e7e24","modified":1512468451368},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"a6c6eb8adba0a090ad1f4b9124e866887f20d10d","modified":1512468451370},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"cd9e214e502697f2f2db84eb721bac57a49b0fce","modified":1512468451371},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"27deb3d3a243d30022055dac7dad851024099a8b","modified":1512468451372},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1512468451375},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ca88ea6999a61fb905eb6e72eba5f92d4ee31e6e","modified":1512468451373},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"ccb34c52be8adba5996c6b94f9e723bd07d34c16","modified":1512468451376},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1512468451377},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"d0d7a5c90d62b685520d2b47fea8ba6019ff5402","modified":1512468451371},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"39f04c4c7237a4e10acd3002331992b79945d241","modified":1512468451381},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"761eba9811b050b25d548cc0854de4824b41eb08","modified":1512468451382},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"89d6c3b697efc63de42afd2e89194b1be14152af","modified":1512468451379},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"7968343e41f8b94b318c36289dff1196c3eb1791","modified":1512468451378},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"11c22f0fb3f6beb13e5a425ec064a4ff974c13b7","modified":1512468451383},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"8dd9a1c6f4f6baa00c2cf01837e7617120cf9660","modified":1512468451383},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"61f8cea3c01acd600e90e1bc2a07def405503748","modified":1512468451384},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"1153bb71edf253765145559674390e16dd67c633","modified":1512468451385},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"28a8737c090fbffd188d73a00b42e90b9ee57df2","modified":1512468451386},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"5ef6343835f484a2c0770bd1eb9cc443609e4c39","modified":1512468451388},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1512468451374},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"e71652d3216e289c8548b1ea2357822c1476a425","modified":1512468451389},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1512468451391},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"a3bdd71237afc112b2aa255f278cab6baeb25351","modified":1512468451392},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"3159b55f35c40bd08e55b00148c523760a708c51","modified":1512468451393},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1512468451394},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1512468451395},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"9a409b798decdefdaf7a23f0b11004a8c27e82f3","modified":1512468451397},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"154a87a32d2fead480d5e909c37f6c476671c5e6","modified":1512468451398},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"b7076e58d647265ee0ad2b461fe8ce72c9373bc5","modified":1512468451396},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"b80604868e4f5cf20fccafd7ee415c20c804f700","modified":1512468451399},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"a6e7d698702c2e383dde3fde2abde27951679084","modified":1512468451420},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"bba4f3bdb7517cd85376df3e1209b570c0548c69","modified":1512468451410},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"717cc7f82be9cc151e23a7678601ff2fd3a7fa1d","modified":1512468451420},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"10599e16414a8b7a76c4e79e6617b5fe3d4d1adf","modified":1512468451420},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"874278147115601d2abf15987f5f7a84ada1ac6b","modified":1512468451420},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"15975ba7456b96916b1dbac448a1a0d2c38b8f3d","modified":1512468451420},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"16087276945fa038f199692e3eabb1c52b8ea633","modified":1512468451420},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a1521d48bb06d8d703753f52a198baa197af7da2","modified":1512468451387},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"28825ae15fa20ae3942cdaa7bcc1f3523ce59acc","modified":1512468451420},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"9c8196394a89dfa40b87bf0019e80144365a9c93","modified":1512468451420},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1512468451460},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1512468451450},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1512468451450},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"5dbeed535d63a50265d96b396a5440f9bb31e4ba","modified":1512468451410},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"90a1b22129efc172e2dfcceeeb76bff58bc3192f","modified":1512468451523},{"_id":"themes/next/source/lib/three/three.min.js","hash":"26273b1cb4914850a89529b48091dc584f2c57b8","modified":1512468451606},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1512468451573},{"_id":"public/search.xml","hash":"b2cf7c1d248f355fbb985ea9b02db7e5e216c4c1","modified":1514369426469},{"_id":"public/categories/index.html","hash":"49ba953e874e0f4f4b93b977666e7075a29a2d2b","modified":1514369427159},{"_id":"public/tags/index.html","hash":"3efefe1574b6af1cfe999745081c8b61066c72a6","modified":1514369427162},{"_id":"public/about/index.html","hash":"363bb35355d24eaca190a24cde6d1145dd124ea5","modified":1514369427162},{"_id":"public/blog/Practical-introduction-to-data-structures-and-algorithm-analysis.html","hash":"b1eb9dcaaa13215c94d7e37de252e9dfebfe05c0","modified":1514369427162},{"_id":"public/blog/改善深层神经网络：超参数调试、正则化以及优化.html","hash":"0919d3c3c72a1cf4e31711134e53afeffb762359","modified":1514369427162},{"_id":"public/blog/神经网络与深度学习.html","hash":"f970a6b1e113a986c382d0d9f494ca95325156ff","modified":1514369427162},{"_id":"public/blog/action-in-machine-learning.html","hash":"2609b5e7a9eb56e54c781057dab273a350247387","modified":1514369427162},{"_id":"public/blog/Neural-Networks-for-Machine-Learning.html","hash":"60b0a37e40235bca0d9db314935a9293e9c00a22","modified":1514369427162},{"_id":"public/archives/index.html","hash":"26a7d2bcbe13f4eab0eed9913a358d46b03b21a8","modified":1514369427162},{"_id":"public/archives/2016/index.html","hash":"4363abfaf72f936a8572df17f686c5aa7fbc353c","modified":1514369427162},{"_id":"public/archives/2016/06/index.html","hash":"7f913f5728babce276f5e20c8e6e48e7356952ba","modified":1514369427162},{"_id":"public/archives/2017/index.html","hash":"bbfbc0e19d80345ffd97704d7378e69c7dbd1190","modified":1514369427162},{"_id":"public/archives/2017/08/index.html","hash":"4ef37cab4e4a649ca4aecc3edbdbb57a4bd976d8","modified":1514369427162},{"_id":"public/archives/2017/09/index.html","hash":"e545eb70e8f1b230a67e92c1f0addf847d867b78","modified":1514369427162},{"_id":"public/categories/深耕码农/index.html","hash":"964085e67fc70bd18f71a0eeba492a7105cd4ac3","modified":1514369427162},{"_id":"public/categories/AI梦/index.html","hash":"5c561d9cc2a4f989d3a8726ad9b210a28a318c19","modified":1514369427162},{"_id":"public/index.html","hash":"fd2355b618bd708b5207cd81471b73d1b01e3a0d","modified":1514369427163},{"_id":"public/tags/经典著作/index.html","hash":"ae97741634c35a6e2f6e5b9a3a23aa37ad5f2aaa","modified":1514369427163},{"_id":"public/tags/机器学习/index.html","hash":"2bb4a7378180f7901550a4fee2087ed486729687","modified":1514369427163},{"_id":"public/tags/数据结构/index.html","hash":"ebd382d94553733acc778df893117da018196cb7","modified":1514369427163},{"_id":"public/tags/神经网络/index.html","hash":"c9cc8a7870fff511c796c2b88c53c3f12514aa2a","modified":1514369427163},{"_id":"public/tags/Geoffrey-hinton/index.html","hash":"a0abca35fc4eabb3b39f1771b7473f5b485f1b04","modified":1514369427163},{"_id":"public/tags/公开课/index.html","hash":"5642c7219e82dab6cc1ca418157f4fb9da088ef6","modified":1514369427163},{"_id":"public/tags/深度学习/index.html","hash":"71cfafd2ba70f2953dae53baa82487c07e6b4d3c","modified":1514369427163},{"_id":"public/tags/Andrew-NG/index.html","hash":"23c7902e89a6686f8cd89160e0834a808b3dae4e","modified":1514369427163},{"_id":"public/blog/draft.html","hash":"cd321314b2cbd2f7f14514fb6646059e980c71ea","modified":1514369427241},{"_id":"public/blog/Computer-System-A-Programmer-s-Perspective.html","hash":"25c3fc6ad37132fe4ca38609a51a7384401ab505","modified":1514369427242},{"_id":"public/blog/集体智慧编程.html","hash":"96045e6787f20235d34bfe5e9f33a99c81da42fe","modified":1514369427242},{"_id":"public/blog/Thinking-in-Java.html","hash":"859f1a6c3811eb59806b9b01bde505c0495d39ad","modified":1514369427242},{"_id":"public/archives/2017/07/index.html","hash":"af9551c1a4ae8a8e23a48177b3ae30398b8b96d5","modified":1514369427242},{"_id":"public/archives/2017/04/index.html","hash":"d4f03f4cd43cfa4e5937146ec984dbc8b794604f","modified":1514369427242},{"_id":"public/archives/2017/12/index.html","hash":"fc9e74f81b300c611e2ca8b8006782f8a70dd612","modified":1514369427242},{"_id":"public/tags/系统结构/index.html","hash":"c9a92743fac2547340a6550b9815fd58acfe239d","modified":1514369427242},{"_id":"public/tags/计算机语言/index.html","hash":"59f5aeb93269e7611bc2bd9d192d846270b10bff","modified":1514369427242},{"_id":"public/tags/draft/index.html","hash":"261285f5452a8468480d03208363cf870b9cc587","modified":1514369427244},{"_id":"public/CNAME","hash":"5a4ab475cc1f2119b2043c94519bd8a184f46992","modified":1514369427244},{"_id":"public/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1514369427244},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1514369427244},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1514369427244},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1514369427244},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1514369427244},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1514369427244},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1514369427244},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1514369427244},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1514369427244},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1514369427244},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1514369427244},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1514369427244},{"_id":"public/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1514369427245},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1514369427245},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1514369427245},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1514369427245},{"_id":"public/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1514369427245},{"_id":"public/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1514369427245},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1514369427245},{"_id":"public/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1514369427245},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1514369427245},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1514369427245},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1514369427247},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1514369427247},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1514369427247},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1514369427247},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1514369427247},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1514369427247},{"_id":"public/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1514369427247},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1514369427247},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1514369427247},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1514369427247},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1514369427248},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1514369427978},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1514369427979},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1514369427985},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1514369427985},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1514369427985},{"_id":"public/lib/fastclick/README.html","hash":"d6e90449a2c09f3033f7e43d68b0cc8208e22e09","modified":1514369427985},{"_id":"public/lib/jquery_lazyload/README.html","hash":"a08fccd381c8fdb70ba8974b208254c5ba23a95f","modified":1514369427985},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"06811ca2f722dead021493457f27cdc264ef928d","modified":1514369427985},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1514369427985},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1514369427985},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1514369427985},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1514369427985},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1514369427985},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1514369427985},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1514369427985},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1514369427985},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1514369427986},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1514369427986},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1514369427986},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1514369427986},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1514369427986},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1514369427986},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1514369427986},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1514369427986},{"_id":"public/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1514369427986},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1514369427986},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1514369427986},{"_id":"public/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1514369427986},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1514369427986},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1514369427986},{"_id":"public/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1514369427986},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1514369427986},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1514369427986},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1514369427987},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1514369427987},{"_id":"public/js/src/utils.js","hash":"dbdc3d1300eec7da9632608ebc0e5b697779dad7","modified":1514369427987},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1514369427987},{"_id":"public/css/main.css","hash":"3901b820c593f62353e976ceb2623995cabb1366","modified":1514369427987},{"_id":"public/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1514369427987},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1514369427987},{"_id":"public/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1514369427987},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1514369427987},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1514369427987},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1514369427987},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1514369427990},{"_id":"public/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1514369427990},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1514369427990},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1514369427990},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1514369427990},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1514369427990},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1514369427990},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1514369427990},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1514369427990},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1514369427990},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1514369427991},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1514369427991},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1514369427991},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1514369427991},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1514369427991},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1514369427991},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1514369427992},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1514369427992},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1514369427992},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1514369427992},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1514369427993},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1514369427993},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1514369427993},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1514369427993},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1514369427993},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1514369427993},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1514369427993},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1514369427993},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1514369427993},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1514369427994},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1514369427994},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1514369428020}],"Category":[{"name":"深耕码农","_id":"cjbowgdpb0004j4n7vfgupkd5"},{"name":"AI梦","_id":"cjbowgdpm000fj4n7a5hxbi4h"}],"Data":[],"Page":[{"title":"categories","date":"2017-12-06T09:19:28.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2017-12-06 17:19:28\ntype: \"categories\"\n---\n","updated":"2017-12-06T09:22:45.295Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjbowgdp70001j4n7du4wyytu","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2017-12-06T09:18:31.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2017-12-06 17:18:31\ntype: \"tags\"\n---\n","updated":"2017-12-06T09:19:04.816Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjbowgdp90003j4n7fzxg8kpn","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"about","date":"2017-12-06T09:20:41.000Z","_content":"\n\n## 关于我\n\n\nQQ：563593589\nEmail: zw_kprs@126.com","source":"about/index.md","raw":"---\ntitle: about\ndate: 2017-12-06 17:20:41\n---\n\n\n## 关于我\n\n\nQQ：563593589\nEmail: zw_kprs@126.com","updated":"2017-12-06T09:22:43.070Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjbowgdpe0007j4n72d5vvh08","content":"<h2 id=\"关于我\"><a href=\"#关于我\" class=\"headerlink\" title=\"关于我\"></a>关于我</h2><p>QQ：563593589<br>Email: zw_kprs@126.com</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"关于我\"><a href=\"#关于我\" class=\"headerlink\" title=\"关于我\"></a>关于我</h2><p>QQ：563593589<br>Email: zw_kprs@126.com</p>\n"}],"Post":[{"title":"Computer System, A Programmer's Perspective","date":"2017-08-21T06:22:46.000Z","mathjax":true,"_content":"\n\n# 导言\n1. 信息就是位+上下文，如文本文件、二进制文件；\n2. 程序：文本——预处理器——编译器——汇编器——链接器——可执行文件\n  ![CS_compile](http://p15i7i801.bkt.clouddn.com/d8403afcfd487487262309d0137d406f.png)\n\n从JAVA的角度上看，JAVACODE在JVM上亦符合上述过程。\nJVM这个代码级机器可参考百度百科，包括对JVM指令系统、寄存器、栈结构及子系统（GC、类加载器）等\n![JVM](http://p15i7i801.bkt.clouddn.com/8c5d39071eb21c9f569e9e46119377d3.png)\n![cs_jvm_stack](http://p15i7i801.bkt.clouddn.com/32f2e3db6434701a8206f6ce128bf139.png)\n\n**基本上，一个线程启动后分配一个栈（-XSSS），保存局部变量、操作数、指向常量池的引用及返回地址，这些称为一个桢，进入新方法后，再压入一个帧；同时分配一个私有PC寄存器用于任何分支，循环，方法调用，判断，异常处理，线程等待以及恢复线程，递归等等； 所有的线程共享方法区（PERM)及堆**\n","source":"_posts/Computer-System-A-Programmer-s-Perspective.md","raw":"---\ntitle: 'Computer System, A Programmer''s Perspective'\ndate: 2017-08-21 14:22:46\ntags:\n      - 系统结构\n      - 经典著作\ncategories: 深耕码农\nmathjax: true\n---\n\n\n# 导言\n1. 信息就是位+上下文，如文本文件、二进制文件；\n2. 程序：文本——预处理器——编译器——汇编器——链接器——可执行文件\n  ![CS_compile](http://p15i7i801.bkt.clouddn.com/d8403afcfd487487262309d0137d406f.png)\n\n从JAVA的角度上看，JAVACODE在JVM上亦符合上述过程。\nJVM这个代码级机器可参考百度百科，包括对JVM指令系统、寄存器、栈结构及子系统（GC、类加载器）等\n![JVM](http://p15i7i801.bkt.clouddn.com/8c5d39071eb21c9f569e9e46119377d3.png)\n![cs_jvm_stack](http://p15i7i801.bkt.clouddn.com/32f2e3db6434701a8206f6ce128bf139.png)\n\n**基本上，一个线程启动后分配一个栈（-XSSS），保存局部变量、操作数、指向常量池的引用及返回地址，这些称为一个桢，进入新方法后，再压入一个帧；同时分配一个私有PC寄存器用于任何分支，循环，方法调用，判断，异常处理，线程等待以及恢复线程，递归等等； 所有的线程共享方法区（PERM)及堆**\n","slug":"Computer-System-A-Programmer-s-Perspective","published":1,"updated":"2017-12-27T06:28:37.417Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbowgdp30000j4n74qhuc5nj","content":"<h1 id=\"导言\"><a href=\"#导言\" class=\"headerlink\" title=\"导言\"></a>导言</h1><ol>\n<li>信息就是位+上下文，如文本文件、二进制文件；</li>\n<li>程序：文本——预处理器——编译器——汇编器——链接器——可执行文件<br><img src=\"http://p15i7i801.bkt.clouddn.com/d8403afcfd487487262309d0137d406f.png\" alt=\"CS_compile\"></li>\n</ol>\n<p>从JAVA的角度上看，JAVACODE在JVM上亦符合上述过程。<br>JVM这个代码级机器可参考百度百科，包括对JVM指令系统、寄存器、栈结构及子系统（GC、类加载器）等<br><img src=\"http://p15i7i801.bkt.clouddn.com/8c5d39071eb21c9f569e9e46119377d3.png\" alt=\"JVM\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/32f2e3db6434701a8206f6ce128bf139.png\" alt=\"cs_jvm_stack\"></p>\n<p><strong>基本上，一个线程启动后分配一个栈（-XSSS），保存局部变量、操作数、指向常量池的引用及返回地址，这些称为一个桢，进入新方法后，再压入一个帧；同时分配一个私有PC寄存器用于任何分支，循环，方法调用，判断，异常处理，线程等待以及恢复线程，递归等等； 所有的线程共享方法区（PERM)及堆</strong></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"导言\"><a href=\"#导言\" class=\"headerlink\" title=\"导言\"></a>导言</h1><ol>\n<li>信息就是位+上下文，如文本文件、二进制文件；</li>\n<li>程序：文本——预处理器——编译器——汇编器——链接器——可执行文件<br><img src=\"http://p15i7i801.bkt.clouddn.com/d8403afcfd487487262309d0137d406f.png\" alt=\"CS_compile\"></li>\n</ol>\n<p>从JAVA的角度上看，JAVACODE在JVM上亦符合上述过程。<br>JVM这个代码级机器可参考百度百科，包括对JVM指令系统、寄存器、栈结构及子系统（GC、类加载器）等<br><img src=\"http://p15i7i801.bkt.clouddn.com/8c5d39071eb21c9f569e9e46119377d3.png\" alt=\"JVM\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/32f2e3db6434701a8206f6ce128bf139.png\" alt=\"cs_jvm_stack\"></p>\n<p><strong>基本上，一个线程启动后分配一个栈（-XSSS），保存局部变量、操作数、指向常量池的引用及返回地址，这些称为一个桢，进入新方法后，再压入一个帧；同时分配一个私有PC寄存器用于任何分支，循环，方法调用，判断，异常处理，线程等待以及恢复线程，递归等等； 所有的线程共享方法区（PERM)及堆</strong></p>\n"},{"title":"Thinking in Java","date":"2017-04-11T06:29:14.000Z","mathjax":false,"_content":"\n\n# 一、导论\n* 对象都有一个接口\n* private, protected, public和无限定词——包访问权限\n* 组合与继承（覆盖方法、添加新方法）\n* 多态\n  * C++用Virtual表示，JAVA内部用一小段特殊代码表示，默认方法皆如此，自动进行upcasting\n* 单根继承——Object\n* 首先便于基本操作，heap new\n* 其次GC\n* 范型\n* 异常\n* WEB发展，前端JS APLLET 跨平台 后端  servlet JSP 跨平台消除异构浏览器的复杂性\n\n# 二、一切为对象\n* 引用\n* 对象存储（寄存器、堆栈、堆、常量、持久化存储）\n* 特例：基本类型\n* 基本数据类型的作用域及对象的自动GC\n* 类，成员默认值，局部变量依然是随机值，强制初始化\n* 命名空间(com.apaceh.)，import，static\n  * 静态变量提供与对象无关的存储空间分配；\n  * 静态方法提供，直接调用 的方法，如main\n* JAVADOC，帮你自动生成JAVA API文档\n* 编码风格：驼峰\n\n# 三四、操作符、控制流\n* 引用，包括赋值、传递\n* 一元加减操作符\n* 逻辑运算符的短路现象：（）&&（）&&（），最短前两个即能判断该表达式\n* 指数计数 、移位（注意-1的表示 方法）、三元操作符\n* e作为10的幂次\n* 字符串操作符+、+=，定义若表达式以一个字符串开头，后续所有操作数都必须是字符串，要不然强制转\n* round()\n* 没有sizeof\n* for中的逗号操作符，foreach\n\n# 五、初始化与清理\n* 构造、重载（参数、顺序皆可，返回值不可，对于基本类型，传递参数可上升不可下降，因此要人工强制窄化）\n* this返回本对象\n* finalize，非析构，在GC时调用，可以认为是验证对象的终结，在调用NATIVE CODE时尤其有用\n* GC\n\n# 六、继承\n* 类的加载：找到类文件，加载基类，加载主类，静态初始化、创建对象，成员初始化（基础类型为默认值，对象引用置空），构造器\n* final成员、参数 、方法与类\n\n# 七、多态（OO三种基本特征，抽象、继承、多态）\n* 原理：upcasting,自动后期binding，除非final\n20170709  只有普通方法的调用可以是多态的，如私有方法、域及静态方法\n* 若要编码一个基础类，除非必需使用方法和成员，那么第一选择应该是使用接口；\n* 基类——抽象方法——抽象类——接口，由具体到抽象，若一个基类是纯抽象的，而未告诉编译器它是抽象的或是个接口，那么，可能会因为误重载（本来是覆盖，因为参数不同，导致动态绑定失效）的原因，调用基类中的“伪”方法，造成非预知的问题。\n* 接口：类默认为public，成员默认为staic和final ，可多重继承，\n* 适配接口，策略设计模式\n被适配的类 继承和实现基策略 任何类都可以通过为多重继承的方式被适配\n* 接口中的任何域都是static和final的\n* 接口可以嵌套，在类中或接口中 ，在类中private只能交给有权使用它的对象，即使他被一个public接口给实现；而嵌套在接口中则必须是public。\n* 工厂模式，较通用，一般用于框架，代码可复用\n\n# 十、内部类\n# 十一、异常\n* finally 在此处有用，处理异常下清理 ，但注意在构造方法中，不适合使用，而适在catch中处理\n* 注意exception有丢失的可能可能性，比如被下一个exception覆盖（即catch内层有几层异常发生），又比如被finnally return .\n* 异常匹配就近原则\n* 异常及类型检查是必要的，但可以编译器和运行发生，只要它存在。反射及范型也是JAVA为编译期过多的检查所作的补偿。\n\n# 十二、字符串\n* string +  性能不如stringbuilder 使用javap可反编译分析，且JAVA5前是stringbuffer,而stringbuffer是线程安全的，效率不高。\n* toString 可能会造成无穷递归调用 tostring, 如   tostring {  return 'a' + this ;},防止的方法是调用super.this\n* system.out.printf/formatter 及formatter类、string.format()  用于格式化修饰%[argument_index$][flags][width][.precision]conversion， flags用于对齐， 如%05x,    右对齐，不足5位用0补齐\n* string.match()/split()/replace()/replaceall()\n# 十四：RTTI\n* RTTI及反射使得运行时识别对象和类信息，而对象实际执行什么代码，这是有它指向的引用决定的，即多态机制。\n* 通常希望大部分代码尽可能少了解对象具体类型，而只与对象家族的一个通用表示打交道。因此，多态是OOP的基本目标。\n# 十五：范型\n* 范型方法的 类型参数推断 ，基本类型的自动打包机制。类型参数推断的局限性在于只对赋值有效，这种局限可以通过显式的类型说明来弥补\n* 范型方法可与可变参数配合\n十六、数组\n* 相对容器高效，但功能更少\n* 初始化，对象为null，基本类型 char \\0000 即转整型后为0\n* 有个取不重复随机数的方法：\n  ```java\n  do\n  t = rand.nextInt(len);\n  while(picked[i]);\n  ```\n","source":"_posts/Thinking-in-Java.md","raw":"---\ntitle: Thinking in Java\ndate: 2017-04-11 14:29:14\ntags:\n      - 计算机语言\n      - 经典著作\ncategories: 深耕码农\nmathjax: false\n---\n\n\n# 一、导论\n* 对象都有一个接口\n* private, protected, public和无限定词——包访问权限\n* 组合与继承（覆盖方法、添加新方法）\n* 多态\n  * C++用Virtual表示，JAVA内部用一小段特殊代码表示，默认方法皆如此，自动进行upcasting\n* 单根继承——Object\n* 首先便于基本操作，heap new\n* 其次GC\n* 范型\n* 异常\n* WEB发展，前端JS APLLET 跨平台 后端  servlet JSP 跨平台消除异构浏览器的复杂性\n\n# 二、一切为对象\n* 引用\n* 对象存储（寄存器、堆栈、堆、常量、持久化存储）\n* 特例：基本类型\n* 基本数据类型的作用域及对象的自动GC\n* 类，成员默认值，局部变量依然是随机值，强制初始化\n* 命名空间(com.apaceh.)，import，static\n  * 静态变量提供与对象无关的存储空间分配；\n  * 静态方法提供，直接调用 的方法，如main\n* JAVADOC，帮你自动生成JAVA API文档\n* 编码风格：驼峰\n\n# 三四、操作符、控制流\n* 引用，包括赋值、传递\n* 一元加减操作符\n* 逻辑运算符的短路现象：（）&&（）&&（），最短前两个即能判断该表达式\n* 指数计数 、移位（注意-1的表示 方法）、三元操作符\n* e作为10的幂次\n* 字符串操作符+、+=，定义若表达式以一个字符串开头，后续所有操作数都必须是字符串，要不然强制转\n* round()\n* 没有sizeof\n* for中的逗号操作符，foreach\n\n# 五、初始化与清理\n* 构造、重载（参数、顺序皆可，返回值不可，对于基本类型，传递参数可上升不可下降，因此要人工强制窄化）\n* this返回本对象\n* finalize，非析构，在GC时调用，可以认为是验证对象的终结，在调用NATIVE CODE时尤其有用\n* GC\n\n# 六、继承\n* 类的加载：找到类文件，加载基类，加载主类，静态初始化、创建对象，成员初始化（基础类型为默认值，对象引用置空），构造器\n* final成员、参数 、方法与类\n\n# 七、多态（OO三种基本特征，抽象、继承、多态）\n* 原理：upcasting,自动后期binding，除非final\n20170709  只有普通方法的调用可以是多态的，如私有方法、域及静态方法\n* 若要编码一个基础类，除非必需使用方法和成员，那么第一选择应该是使用接口；\n* 基类——抽象方法——抽象类——接口，由具体到抽象，若一个基类是纯抽象的，而未告诉编译器它是抽象的或是个接口，那么，可能会因为误重载（本来是覆盖，因为参数不同，导致动态绑定失效）的原因，调用基类中的“伪”方法，造成非预知的问题。\n* 接口：类默认为public，成员默认为staic和final ，可多重继承，\n* 适配接口，策略设计模式\n被适配的类 继承和实现基策略 任何类都可以通过为多重继承的方式被适配\n* 接口中的任何域都是static和final的\n* 接口可以嵌套，在类中或接口中 ，在类中private只能交给有权使用它的对象，即使他被一个public接口给实现；而嵌套在接口中则必须是public。\n* 工厂模式，较通用，一般用于框架，代码可复用\n\n# 十、内部类\n# 十一、异常\n* finally 在此处有用，处理异常下清理 ，但注意在构造方法中，不适合使用，而适在catch中处理\n* 注意exception有丢失的可能可能性，比如被下一个exception覆盖（即catch内层有几层异常发生），又比如被finnally return .\n* 异常匹配就近原则\n* 异常及类型检查是必要的，但可以编译器和运行发生，只要它存在。反射及范型也是JAVA为编译期过多的检查所作的补偿。\n\n# 十二、字符串\n* string +  性能不如stringbuilder 使用javap可反编译分析，且JAVA5前是stringbuffer,而stringbuffer是线程安全的，效率不高。\n* toString 可能会造成无穷递归调用 tostring, 如   tostring {  return 'a' + this ;},防止的方法是调用super.this\n* system.out.printf/formatter 及formatter类、string.format()  用于格式化修饰%[argument_index$][flags][width][.precision]conversion， flags用于对齐， 如%05x,    右对齐，不足5位用0补齐\n* string.match()/split()/replace()/replaceall()\n# 十四：RTTI\n* RTTI及反射使得运行时识别对象和类信息，而对象实际执行什么代码，这是有它指向的引用决定的，即多态机制。\n* 通常希望大部分代码尽可能少了解对象具体类型，而只与对象家族的一个通用表示打交道。因此，多态是OOP的基本目标。\n# 十五：范型\n* 范型方法的 类型参数推断 ，基本类型的自动打包机制。类型参数推断的局限性在于只对赋值有效，这种局限可以通过显式的类型说明来弥补\n* 范型方法可与可变参数配合\n十六、数组\n* 相对容器高效，但功能更少\n* 初始化，对象为null，基本类型 char \\0000 即转整型后为0\n* 有个取不重复随机数的方法：\n  ```java\n  do\n  t = rand.nextInt(len);\n  while(picked[i]);\n  ```\n","slug":"Thinking-in-Java","published":1,"updated":"2017-12-27T09:54:46.376Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbowgdp80002j4n7m3eegig5","content":"<h1 id=\"一、导论\"><a href=\"#一、导论\" class=\"headerlink\" title=\"一、导论\"></a>一、导论</h1><ul>\n<li>对象都有一个接口</li>\n<li>private, protected, public和无限定词——包访问权限</li>\n<li>组合与继承（覆盖方法、添加新方法）</li>\n<li>多态<ul>\n<li>C++用Virtual表示，JAVA内部用一小段特殊代码表示，默认方法皆如此，自动进行upcasting</li>\n</ul>\n</li>\n<li>单根继承——Object</li>\n<li>首先便于基本操作，heap new</li>\n<li>其次GC</li>\n<li>范型</li>\n<li>异常</li>\n<li>WEB发展，前端JS APLLET 跨平台 后端  servlet JSP 跨平台消除异构浏览器的复杂性</li>\n</ul>\n<h1 id=\"二、一切为对象\"><a href=\"#二、一切为对象\" class=\"headerlink\" title=\"二、一切为对象\"></a>二、一切为对象</h1><ul>\n<li>引用</li>\n<li>对象存储（寄存器、堆栈、堆、常量、持久化存储）</li>\n<li>特例：基本类型</li>\n<li>基本数据类型的作用域及对象的自动GC</li>\n<li>类，成员默认值，局部变量依然是随机值，强制初始化</li>\n<li>命名空间(com.apaceh.)，import，static<ul>\n<li>静态变量提供与对象无关的存储空间分配；</li>\n<li>静态方法提供，直接调用 的方法，如main</li>\n</ul>\n</li>\n<li>JAVADOC，帮你自动生成JAVA API文档</li>\n<li>编码风格：驼峰</li>\n</ul>\n<h1 id=\"三四、操作符、控制流\"><a href=\"#三四、操作符、控制流\" class=\"headerlink\" title=\"三四、操作符、控制流\"></a>三四、操作符、控制流</h1><ul>\n<li>引用，包括赋值、传递</li>\n<li>一元加减操作符</li>\n<li>逻辑运算符的短路现象：（）&amp;&amp;（）&amp;&amp;（），最短前两个即能判断该表达式</li>\n<li>指数计数 、移位（注意-1的表示 方法）、三元操作符</li>\n<li>e作为10的幂次</li>\n<li>字符串操作符+、+=，定义若表达式以一个字符串开头，后续所有操作数都必须是字符串，要不然强制转</li>\n<li>round()</li>\n<li>没有sizeof</li>\n<li>for中的逗号操作符，foreach</li>\n</ul>\n<h1 id=\"五、初始化与清理\"><a href=\"#五、初始化与清理\" class=\"headerlink\" title=\"五、初始化与清理\"></a>五、初始化与清理</h1><ul>\n<li>构造、重载（参数、顺序皆可，返回值不可，对于基本类型，传递参数可上升不可下降，因此要人工强制窄化）</li>\n<li>this返回本对象</li>\n<li>finalize，非析构，在GC时调用，可以认为是验证对象的终结，在调用NATIVE CODE时尤其有用</li>\n<li>GC</li>\n</ul>\n<h1 id=\"六、继承\"><a href=\"#六、继承\" class=\"headerlink\" title=\"六、继承\"></a>六、继承</h1><ul>\n<li>类的加载：找到类文件，加载基类，加载主类，静态初始化、创建对象，成员初始化（基础类型为默认值，对象引用置空），构造器</li>\n<li>final成员、参数 、方法与类</li>\n</ul>\n<h1 id=\"七、多态（OO三种基本特征，抽象、继承、多态）\"><a href=\"#七、多态（OO三种基本特征，抽象、继承、多态）\" class=\"headerlink\" title=\"七、多态（OO三种基本特征，抽象、继承、多态）\"></a>七、多态（OO三种基本特征，抽象、继承、多态）</h1><ul>\n<li>原理：upcasting,自动后期binding，除非final<br>20170709  只有普通方法的调用可以是多态的，如私有方法、域及静态方法</li>\n<li>若要编码一个基础类，除非必需使用方法和成员，那么第一选择应该是使用接口；</li>\n<li>基类——抽象方法——抽象类——接口，由具体到抽象，若一个基类是纯抽象的，而未告诉编译器它是抽象的或是个接口，那么，可能会因为误重载（本来是覆盖，因为参数不同，导致动态绑定失效）的原因，调用基类中的“伪”方法，造成非预知的问题。</li>\n<li>接口：类默认为public，成员默认为staic和final ，可多重继承，</li>\n<li>适配接口，策略设计模式<br>被适配的类 继承和实现基策略 任何类都可以通过为多重继承的方式被适配</li>\n<li>接口中的任何域都是static和final的</li>\n<li>接口可以嵌套，在类中或接口中 ，在类中private只能交给有权使用它的对象，即使他被一个public接口给实现；而嵌套在接口中则必须是public。</li>\n<li>工厂模式，较通用，一般用于框架，代码可复用</li>\n</ul>\n<h1 id=\"十、内部类\"><a href=\"#十、内部类\" class=\"headerlink\" title=\"十、内部类\"></a>十、内部类</h1><h1 id=\"十一、异常\"><a href=\"#十一、异常\" class=\"headerlink\" title=\"十一、异常\"></a>十一、异常</h1><ul>\n<li>finally 在此处有用，处理异常下清理 ，但注意在构造方法中，不适合使用，而适在catch中处理</li>\n<li>注意exception有丢失的可能可能性，比如被下一个exception覆盖（即catch内层有几层异常发生），又比如被finnally return .</li>\n<li>异常匹配就近原则</li>\n<li>异常及类型检查是必要的，但可以编译器和运行发生，只要它存在。反射及范型也是JAVA为编译期过多的检查所作的补偿。</li>\n</ul>\n<h1 id=\"十二、字符串\"><a href=\"#十二、字符串\" class=\"headerlink\" title=\"十二、字符串\"></a>十二、字符串</h1><ul>\n<li>string +  性能不如stringbuilder 使用javap可反编译分析，且JAVA5前是stringbuffer,而stringbuffer是线程安全的，效率不高。</li>\n<li>toString 可能会造成无穷递归调用 tostring, 如   tostring {  return ‘a’ + this ;},防止的方法是调用super.this</li>\n<li>system.out.printf/formatter 及formatter类、string.format()  用于格式化修饰%[argument_index$][flags][width][.precision]conversion， flags用于对齐， 如%05x,    右对齐，不足5位用0补齐</li>\n<li>string.match()/split()/replace()/replaceall()<h1 id=\"十四：RTTI\"><a href=\"#十四：RTTI\" class=\"headerlink\" title=\"十四：RTTI\"></a>十四：RTTI</h1></li>\n<li>RTTI及反射使得运行时识别对象和类信息，而对象实际执行什么代码，这是有它指向的引用决定的，即多态机制。</li>\n<li>通常希望大部分代码尽可能少了解对象具体类型，而只与对象家族的一个通用表示打交道。因此，多态是OOP的基本目标。<h1 id=\"十五：范型\"><a href=\"#十五：范型\" class=\"headerlink\" title=\"十五：范型\"></a>十五：范型</h1></li>\n<li>范型方法的 类型参数推断 ，基本类型的自动打包机制。类型参数推断的局限性在于只对赋值有效，这种局限可以通过显式的类型说明来弥补</li>\n<li>范型方法可与可变参数配合<br>十六、数组</li>\n<li>相对容器高效，但功能更少</li>\n<li>初始化，对象为null，基本类型 char \\0000 即转整型后为0</li>\n<li>有个取不重复随机数的方法：<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">do</span></span><br><span class=\"line\">t = rand.nextInt(len);</span><br><span class=\"line\"><span class=\"keyword\">while</span>(picked[i]);</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"一、导论\"><a href=\"#一、导论\" class=\"headerlink\" title=\"一、导论\"></a>一、导论</h1><ul>\n<li>对象都有一个接口</li>\n<li>private, protected, public和无限定词——包访问权限</li>\n<li>组合与继承（覆盖方法、添加新方法）</li>\n<li>多态<ul>\n<li>C++用Virtual表示，JAVA内部用一小段特殊代码表示，默认方法皆如此，自动进行upcasting</li>\n</ul>\n</li>\n<li>单根继承——Object</li>\n<li>首先便于基本操作，heap new</li>\n<li>其次GC</li>\n<li>范型</li>\n<li>异常</li>\n<li>WEB发展，前端JS APLLET 跨平台 后端  servlet JSP 跨平台消除异构浏览器的复杂性</li>\n</ul>\n<h1 id=\"二、一切为对象\"><a href=\"#二、一切为对象\" class=\"headerlink\" title=\"二、一切为对象\"></a>二、一切为对象</h1><ul>\n<li>引用</li>\n<li>对象存储（寄存器、堆栈、堆、常量、持久化存储）</li>\n<li>特例：基本类型</li>\n<li>基本数据类型的作用域及对象的自动GC</li>\n<li>类，成员默认值，局部变量依然是随机值，强制初始化</li>\n<li>命名空间(com.apaceh.)，import，static<ul>\n<li>静态变量提供与对象无关的存储空间分配；</li>\n<li>静态方法提供，直接调用 的方法，如main</li>\n</ul>\n</li>\n<li>JAVADOC，帮你自动生成JAVA API文档</li>\n<li>编码风格：驼峰</li>\n</ul>\n<h1 id=\"三四、操作符、控制流\"><a href=\"#三四、操作符、控制流\" class=\"headerlink\" title=\"三四、操作符、控制流\"></a>三四、操作符、控制流</h1><ul>\n<li>引用，包括赋值、传递</li>\n<li>一元加减操作符</li>\n<li>逻辑运算符的短路现象：（）&amp;&amp;（）&amp;&amp;（），最短前两个即能判断该表达式</li>\n<li>指数计数 、移位（注意-1的表示 方法）、三元操作符</li>\n<li>e作为10的幂次</li>\n<li>字符串操作符+、+=，定义若表达式以一个字符串开头，后续所有操作数都必须是字符串，要不然强制转</li>\n<li>round()</li>\n<li>没有sizeof</li>\n<li>for中的逗号操作符，foreach</li>\n</ul>\n<h1 id=\"五、初始化与清理\"><a href=\"#五、初始化与清理\" class=\"headerlink\" title=\"五、初始化与清理\"></a>五、初始化与清理</h1><ul>\n<li>构造、重载（参数、顺序皆可，返回值不可，对于基本类型，传递参数可上升不可下降，因此要人工强制窄化）</li>\n<li>this返回本对象</li>\n<li>finalize，非析构，在GC时调用，可以认为是验证对象的终结，在调用NATIVE CODE时尤其有用</li>\n<li>GC</li>\n</ul>\n<h1 id=\"六、继承\"><a href=\"#六、继承\" class=\"headerlink\" title=\"六、继承\"></a>六、继承</h1><ul>\n<li>类的加载：找到类文件，加载基类，加载主类，静态初始化、创建对象，成员初始化（基础类型为默认值，对象引用置空），构造器</li>\n<li>final成员、参数 、方法与类</li>\n</ul>\n<h1 id=\"七、多态（OO三种基本特征，抽象、继承、多态）\"><a href=\"#七、多态（OO三种基本特征，抽象、继承、多态）\" class=\"headerlink\" title=\"七、多态（OO三种基本特征，抽象、继承、多态）\"></a>七、多态（OO三种基本特征，抽象、继承、多态）</h1><ul>\n<li>原理：upcasting,自动后期binding，除非final<br>20170709  只有普通方法的调用可以是多态的，如私有方法、域及静态方法</li>\n<li>若要编码一个基础类，除非必需使用方法和成员，那么第一选择应该是使用接口；</li>\n<li>基类——抽象方法——抽象类——接口，由具体到抽象，若一个基类是纯抽象的，而未告诉编译器它是抽象的或是个接口，那么，可能会因为误重载（本来是覆盖，因为参数不同，导致动态绑定失效）的原因，调用基类中的“伪”方法，造成非预知的问题。</li>\n<li>接口：类默认为public，成员默认为staic和final ，可多重继承，</li>\n<li>适配接口，策略设计模式<br>被适配的类 继承和实现基策略 任何类都可以通过为多重继承的方式被适配</li>\n<li>接口中的任何域都是static和final的</li>\n<li>接口可以嵌套，在类中或接口中 ，在类中private只能交给有权使用它的对象，即使他被一个public接口给实现；而嵌套在接口中则必须是public。</li>\n<li>工厂模式，较通用，一般用于框架，代码可复用</li>\n</ul>\n<h1 id=\"十、内部类\"><a href=\"#十、内部类\" class=\"headerlink\" title=\"十、内部类\"></a>十、内部类</h1><h1 id=\"十一、异常\"><a href=\"#十一、异常\" class=\"headerlink\" title=\"十一、异常\"></a>十一、异常</h1><ul>\n<li>finally 在此处有用，处理异常下清理 ，但注意在构造方法中，不适合使用，而适在catch中处理</li>\n<li>注意exception有丢失的可能可能性，比如被下一个exception覆盖（即catch内层有几层异常发生），又比如被finnally return .</li>\n<li>异常匹配就近原则</li>\n<li>异常及类型检查是必要的，但可以编译器和运行发生，只要它存在。反射及范型也是JAVA为编译期过多的检查所作的补偿。</li>\n</ul>\n<h1 id=\"十二、字符串\"><a href=\"#十二、字符串\" class=\"headerlink\" title=\"十二、字符串\"></a>十二、字符串</h1><ul>\n<li>string +  性能不如stringbuilder 使用javap可反编译分析，且JAVA5前是stringbuffer,而stringbuffer是线程安全的，效率不高。</li>\n<li>toString 可能会造成无穷递归调用 tostring, 如   tostring {  return ‘a’ + this ;},防止的方法是调用super.this</li>\n<li>system.out.printf/formatter 及formatter类、string.format()  用于格式化修饰%[argument_index$][flags][width][.precision]conversion， flags用于对齐， 如%05x,    右对齐，不足5位用0补齐</li>\n<li>string.match()/split()/replace()/replaceall()<h1 id=\"十四：RTTI\"><a href=\"#十四：RTTI\" class=\"headerlink\" title=\"十四：RTTI\"></a>十四：RTTI</h1></li>\n<li>RTTI及反射使得运行时识别对象和类信息，而对象实际执行什么代码，这是有它指向的引用决定的，即多态机制。</li>\n<li>通常希望大部分代码尽可能少了解对象具体类型，而只与对象家族的一个通用表示打交道。因此，多态是OOP的基本目标。<h1 id=\"十五：范型\"><a href=\"#十五：范型\" class=\"headerlink\" title=\"十五：范型\"></a>十五：范型</h1></li>\n<li>范型方法的 类型参数推断 ，基本类型的自动打包机制。类型参数推断的局限性在于只对赋值有效，这种局限可以通过显式的类型说明来弥补</li>\n<li>范型方法可与可变参数配合<br>十六、数组</li>\n<li>相对容器高效，但功能更少</li>\n<li>初始化，对象为null，基本类型 char \\0000 即转整型后为0</li>\n<li>有个取不重复随机数的方法：<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">do</span></span><br><span class=\"line\">t = rand.nextInt(len);</span><br><span class=\"line\"><span class=\"keyword\">while</span>(picked[i]);</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n"},{"title":"action in machine learning","date":"2017-08-29T10:21:54.000Z","mathjax":true,"_content":"\n### 树回归\n* 连续型数值的混乱度：总平方误差来表示，即方差*m\n* 离散型的标称的混乱度可以用香农熵表示\n* 若叶节使用的模型是分段常数则称为回归树，若叶节点使用的模型是线性回归方程则称为模型树\n* 使用CART构建出的树倾向于过拟合，分别使用预剪枝和后剪枝解决这个问题。预剪枝设置停止条件来调整拟合问题，这需要不断调整，见P168。而后剪枝首先构造足够复杂的树，然后使用测试集来判断叶节点合并能否降低误差\n* 决策树通过迭代实现分类，离散情况下，叶节点的类别即预测的类别，连续情况下，若是回归树则是该子树叶子节点的平均值，若是模型树，则是该（线性）模型的值\n\n### Random Forest/GBDT\n[参考介绍一：二分类](http://blog.csdn.net/google19890102/article/details/51746402)\n[参考介绍一:多分类](http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html)\n*  随机建立树的方式，随机采样包括行采样（样本），列采样（特征）\n* 即本质是每颗树是窄领域内的专家，众专家一起投票\n\n###预测数值型数据:回归\n#### 线性回归\n  使用最小二乘法，得$若X非奇异，则有w=(X^tX)^{-1}X^ty$\n#### 局部加权线性回归（Local Weighted Leaner Regression）\n  线性回归的问题是容易欠拟合，因此引入局部加权\n  $$\\hat{w}=(X^twX)^{-1}X^twy$$\n  通常使用高斯核用来赋于权重$w(i,i)=e^{\\frac {x^{(i)}-x}{-2k^2}}$，即有离预测点越近的点权重越大，同时参数K控制了参与运算的数据，这样就有如下图:\n![aiml-localweightedLR](http://p15i7i801.bkt.clouddn.com/d1676ee6aefd1a476cc7f40036c87f5c.png)\n\n  此方法缺点在于**计算量增加，每次预测必须在全集上运行**，好处在于可以控制拟合程度\n![aiml-LR](http://p15i7i801.bkt.clouddn.com/e9cc4d793fd511a4e7749fa6660005c5.png)\n\n#### 缩减系数——用于更好的理解数据并处理特征比样本多的问题\n  对于$X^TX$为奇异矩阵，在$X^TX$基础加上 $\\lambda I$使得矩阵非奇异，这种方法称为*岭回归*。此方法有三个用途：\n  * 用于处理特征比样本多的情况\n  * 用于在估计中加入偏差，从而得到更好的估计\n  * 通过引入   $\\lambda$ 来限制 $w$ 之和，通过引入惩罚，能够减少不重要的参数，这类技术在统计学称为衰减。（迹不变）\n  * 此公式$$等价于最小二乘加入约束\n\n### 附加篇：各类方法的一个归纳\n* 树模型：DT, GBDT, RF\n* 概率模型：Bayes\n* 最优化：LR, SVM\n    * LR：简单实用，互联网CTR预估被广泛使用\n    * SVM：简单、多分类问题比SVM好，不好解释；\n* 距离划分，判别模型\n    * KNN：简单、多分类问题比SVM好，不好解释；\n* 集成方法：Adaboost\n* 方法比较\n    * NB与LR\n      * 相同点：都是特征的线性表述，解释性对较好；\n      * 不同点：NB要求严格独立，NB有利于并行化，但随着数据量和维度增多，LR更合适\n* 可持久化的模型：决策树\n\n### 决策树|随机森林|adaboost|GBDT\n* [决策树模型组合之（在线）随机森林与GBDT](http://www.cnblogs.com/wentingtu/archive/2011/12/13/2286212.html)\n* [Bagging（Bootstrap aggregating）、随机森林（random forests）、AdaBoost](http://blog.csdn.net/xlinsist/article/details/51475345 )\n* [决策树ID3、C4.5、C5.0以及CART算法之间的比较-并用scikit-learn决策树拟合Iris数据集](http://blog.csdn.net/xlinsist/article/details/51468741)\n* ID3缺点:\n    *  对于多值的属性非常之敏感\n    *  无法处理连续值\n    *  容易产生过拟合，即训练数据训练太彻底，但大一点数据集错误率上升，关注了些局部信息\n* C4.5---C5.0\n  * 解决上述1、2的问题，核心思想是加入了增益率\n* CART\n  * 能预测出值（回归），同时能全用训练和交叉验证不断评估，修剪决策树\n\n### K-nearest\n* 思路：基于现有已知标签的样本数据集，寻找被预测样本的最近的K个样本，返回K个里频率最多的那个标签。\n* 优点：最简单有效的分类、对异常值不敏感\n* 缺点：解释性差、类别评分无规格化，不平衡问题\n* 适应：[brute force\\kd_tree\\balltree的选择](http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors )\n\n### PCA与SVD\nhttp://blog.csdn.net/jacke121/article/details/59057192\npca是单方向上的降维，SVD是两个方向上的降维。\nSVD表示是$A=U\\Sigma V^T$，其中U,V是正交的。\n若要降维：使用这个公式，其具体数学推导，暂不明。现实含义可以这样理解：\n在SVD的三个矩阵（假设叫1，2，3）中，1-2表示用户降维转换，2-3表示物品降维转换，在用户推荐那个场景，由于是使用基于物品的推荐，要计算两物品的相似度，则可理解为只需要用户角度降维，即1-2的转换。即有\n$$U_k \\Sigma V=> A^T U_k \\Sigma$$\n推导点：**如如此在用户上降维，那是否可以直接通过PCA在用户角度降维？**\n\n### 关联分析\n支持度，可信度\n* Apriori\n\n### 附加篇一\n> 张同学的问题：SVM中，如下代码中权重 $w$ 为何不能做特征的权重值。\n\n```python\ndef calcWs(alphas, dataArr, classLabels):\n    \"\"\"\n    输入：alphas, 数据集, 类别标签\n    输出：目标w\n    \"\"\"\n    X = mat(dataArr)\n    labelMat = mat(classLabels).transpose()\n    m, n = shape(X)\n    w = zeros((n, 1))\n    for i in range(m):\n        w += multiply(alphas[i] * labelMat[i], X[i, :].T)\n    return w\n```\n对比线性情况而言,有 $w= \\Sigma \\alpha y_i x_i$ ，书中有下代码\n```python\nw += multiply(alphas[i] * labelMat[i], X[i, :].T)\n```\n目标函数为：$y=(\\Sigma \\alpha y_i x_i) x + b$ 而对于映射核空间的情况，书中有描述\n![aiml-weightbook](http://p15i7i801.bkt.clouddn.com/c9fb298cf712884a5ad1c6d2407c4b95.png)\n此时目标函数则为 $y=\\Sigma \\alpha y_i k(\\phi(x_i),\\phi(x_j))+b$。显然，特征向量 $w$ 是无法拆出来的。\n\n> 对weights = weights + alpha * dataMatrix.transpose() * error 这一行的理解\n\n* 角度一：http://blog.csdn.net/lu597203933/article/details/38468303  AndrewNg\n![aiml-weight](http://p15i7i801.bkt.clouddn.com/79f4eb72472afa25f82690b0823b603d.png)\n* 角度二：矩阵微分的角度理解 http://blog.csdn.net/aichipmunk/article/details/9382503\n\n### 附加篇二：机器学习算法路径图\n![aiml-algorithm chea-sheet](http://p15i7i801.bkt.clouddn.com/e161673fc2063c42f4d407fd7057fc1f.png)\n![aiml-td](http://p15i7i801.bkt.clouddn.com/daa070eafedebe29f976a12a0b65b08c.png)\n","source":"_posts/action-in-machine-learning.md","raw":"---\ntitle: action in machine learning\ndate: 2017-08-29 18:21:54\ntags:\n      - 机器学习\n      - 经典著作\ncategories: AI梦\nmathjax: true\n---\n\n### 树回归\n* 连续型数值的混乱度：总平方误差来表示，即方差*m\n* 离散型的标称的混乱度可以用香农熵表示\n* 若叶节使用的模型是分段常数则称为回归树，若叶节点使用的模型是线性回归方程则称为模型树\n* 使用CART构建出的树倾向于过拟合，分别使用预剪枝和后剪枝解决这个问题。预剪枝设置停止条件来调整拟合问题，这需要不断调整，见P168。而后剪枝首先构造足够复杂的树，然后使用测试集来判断叶节点合并能否降低误差\n* 决策树通过迭代实现分类，离散情况下，叶节点的类别即预测的类别，连续情况下，若是回归树则是该子树叶子节点的平均值，若是模型树，则是该（线性）模型的值\n\n### Random Forest/GBDT\n[参考介绍一：二分类](http://blog.csdn.net/google19890102/article/details/51746402)\n[参考介绍一:多分类](http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html)\n*  随机建立树的方式，随机采样包括行采样（样本），列采样（特征）\n* 即本质是每颗树是窄领域内的专家，众专家一起投票\n\n###预测数值型数据:回归\n#### 线性回归\n  使用最小二乘法，得$若X非奇异，则有w=(X^tX)^{-1}X^ty$\n#### 局部加权线性回归（Local Weighted Leaner Regression）\n  线性回归的问题是容易欠拟合，因此引入局部加权\n  $$\\hat{w}=(X^twX)^{-1}X^twy$$\n  通常使用高斯核用来赋于权重$w(i,i)=e^{\\frac {x^{(i)}-x}{-2k^2}}$，即有离预测点越近的点权重越大，同时参数K控制了参与运算的数据，这样就有如下图:\n![aiml-localweightedLR](http://p15i7i801.bkt.clouddn.com/d1676ee6aefd1a476cc7f40036c87f5c.png)\n\n  此方法缺点在于**计算量增加，每次预测必须在全集上运行**，好处在于可以控制拟合程度\n![aiml-LR](http://p15i7i801.bkt.clouddn.com/e9cc4d793fd511a4e7749fa6660005c5.png)\n\n#### 缩减系数——用于更好的理解数据并处理特征比样本多的问题\n  对于$X^TX$为奇异矩阵，在$X^TX$基础加上 $\\lambda I$使得矩阵非奇异，这种方法称为*岭回归*。此方法有三个用途：\n  * 用于处理特征比样本多的情况\n  * 用于在估计中加入偏差，从而得到更好的估计\n  * 通过引入   $\\lambda$ 来限制 $w$ 之和，通过引入惩罚，能够减少不重要的参数，这类技术在统计学称为衰减。（迹不变）\n  * 此公式$$等价于最小二乘加入约束\n\n### 附加篇：各类方法的一个归纳\n* 树模型：DT, GBDT, RF\n* 概率模型：Bayes\n* 最优化：LR, SVM\n    * LR：简单实用，互联网CTR预估被广泛使用\n    * SVM：简单、多分类问题比SVM好，不好解释；\n* 距离划分，判别模型\n    * KNN：简单、多分类问题比SVM好，不好解释；\n* 集成方法：Adaboost\n* 方法比较\n    * NB与LR\n      * 相同点：都是特征的线性表述，解释性对较好；\n      * 不同点：NB要求严格独立，NB有利于并行化，但随着数据量和维度增多，LR更合适\n* 可持久化的模型：决策树\n\n### 决策树|随机森林|adaboost|GBDT\n* [决策树模型组合之（在线）随机森林与GBDT](http://www.cnblogs.com/wentingtu/archive/2011/12/13/2286212.html)\n* [Bagging（Bootstrap aggregating）、随机森林（random forests）、AdaBoost](http://blog.csdn.net/xlinsist/article/details/51475345 )\n* [决策树ID3、C4.5、C5.0以及CART算法之间的比较-并用scikit-learn决策树拟合Iris数据集](http://blog.csdn.net/xlinsist/article/details/51468741)\n* ID3缺点:\n    *  对于多值的属性非常之敏感\n    *  无法处理连续值\n    *  容易产生过拟合，即训练数据训练太彻底，但大一点数据集错误率上升，关注了些局部信息\n* C4.5---C5.0\n  * 解决上述1、2的问题，核心思想是加入了增益率\n* CART\n  * 能预测出值（回归），同时能全用训练和交叉验证不断评估，修剪决策树\n\n### K-nearest\n* 思路：基于现有已知标签的样本数据集，寻找被预测样本的最近的K个样本，返回K个里频率最多的那个标签。\n* 优点：最简单有效的分类、对异常值不敏感\n* 缺点：解释性差、类别评分无规格化，不平衡问题\n* 适应：[brute force\\kd_tree\\balltree的选择](http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors )\n\n### PCA与SVD\nhttp://blog.csdn.net/jacke121/article/details/59057192\npca是单方向上的降维，SVD是两个方向上的降维。\nSVD表示是$A=U\\Sigma V^T$，其中U,V是正交的。\n若要降维：使用这个公式，其具体数学推导，暂不明。现实含义可以这样理解：\n在SVD的三个矩阵（假设叫1，2，3）中，1-2表示用户降维转换，2-3表示物品降维转换，在用户推荐那个场景，由于是使用基于物品的推荐，要计算两物品的相似度，则可理解为只需要用户角度降维，即1-2的转换。即有\n$$U_k \\Sigma V=> A^T U_k \\Sigma$$\n推导点：**如如此在用户上降维，那是否可以直接通过PCA在用户角度降维？**\n\n### 关联分析\n支持度，可信度\n* Apriori\n\n### 附加篇一\n> 张同学的问题：SVM中，如下代码中权重 $w$ 为何不能做特征的权重值。\n\n```python\ndef calcWs(alphas, dataArr, classLabels):\n    \"\"\"\n    输入：alphas, 数据集, 类别标签\n    输出：目标w\n    \"\"\"\n    X = mat(dataArr)\n    labelMat = mat(classLabels).transpose()\n    m, n = shape(X)\n    w = zeros((n, 1))\n    for i in range(m):\n        w += multiply(alphas[i] * labelMat[i], X[i, :].T)\n    return w\n```\n对比线性情况而言,有 $w= \\Sigma \\alpha y_i x_i$ ，书中有下代码\n```python\nw += multiply(alphas[i] * labelMat[i], X[i, :].T)\n```\n目标函数为：$y=(\\Sigma \\alpha y_i x_i) x + b$ 而对于映射核空间的情况，书中有描述\n![aiml-weightbook](http://p15i7i801.bkt.clouddn.com/c9fb298cf712884a5ad1c6d2407c4b95.png)\n此时目标函数则为 $y=\\Sigma \\alpha y_i k(\\phi(x_i),\\phi(x_j))+b$。显然，特征向量 $w$ 是无法拆出来的。\n\n> 对weights = weights + alpha * dataMatrix.transpose() * error 这一行的理解\n\n* 角度一：http://blog.csdn.net/lu597203933/article/details/38468303  AndrewNg\n![aiml-weight](http://p15i7i801.bkt.clouddn.com/79f4eb72472afa25f82690b0823b603d.png)\n* 角度二：矩阵微分的角度理解 http://blog.csdn.net/aichipmunk/article/details/9382503\n\n### 附加篇二：机器学习算法路径图\n![aiml-algorithm chea-sheet](http://p15i7i801.bkt.clouddn.com/e161673fc2063c42f4d407fd7057fc1f.png)\n![aiml-td](http://p15i7i801.bkt.clouddn.com/daa070eafedebe29f976a12a0b65b08c.png)\n","slug":"action-in-machine-learning","published":1,"updated":"2017-12-19T10:26:13.696Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbowgdpd0006j4n7llxzttvz","content":"<h3 id=\"树回归\"><a href=\"#树回归\" class=\"headerlink\" title=\"树回归\"></a>树回归</h3><ul>\n<li>连续型数值的混乱度：总平方误差来表示，即方差*m</li>\n<li>离散型的标称的混乱度可以用香农熵表示</li>\n<li>若叶节使用的模型是分段常数则称为回归树，若叶节点使用的模型是线性回归方程则称为模型树</li>\n<li>使用CART构建出的树倾向于过拟合，分别使用预剪枝和后剪枝解决这个问题。预剪枝设置停止条件来调整拟合问题，这需要不断调整，见P168。而后剪枝首先构造足够复杂的树，然后使用测试集来判断叶节点合并能否降低误差</li>\n<li>决策树通过迭代实现分类，离散情况下，叶节点的类别即预测的类别，连续情况下，若是回归树则是该子树叶子节点的平均值，若是模型树，则是该（线性）模型的值</li>\n</ul>\n<h3 id=\"Random-Forest-GBDT\"><a href=\"#Random-Forest-GBDT\" class=\"headerlink\" title=\"Random Forest/GBDT\"></a>Random Forest/GBDT</h3><p><a href=\"http://blog.csdn.net/google19890102/article/details/51746402\" target=\"_blank\" rel=\"noopener\">参考介绍一：二分类</a><br><a href=\"http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html\" target=\"_blank\" rel=\"noopener\">参考介绍一:多分类</a></p>\n<ul>\n<li>随机建立树的方式，随机采样包括行采样（样本），列采样（特征）</li>\n<li>即本质是每颗树是窄领域内的专家，众专家一起投票</li>\n</ul>\n<h3 id=\"预测数值型数据-回归\"><a href=\"#预测数值型数据-回归\" class=\"headerlink\" title=\"预测数值型数据:回归\"></a>预测数值型数据:回归</h3><h4 id=\"线性回归\"><a href=\"#线性回归\" class=\"headerlink\" title=\"线性回归\"></a>线性回归</h4><p>  使用最小二乘法，得$若X非奇异，则有w=(X^tX)^{-1}X^ty$</p>\n<h4 id=\"局部加权线性回归（Local-Weighted-Leaner-Regression）\"><a href=\"#局部加权线性回归（Local-Weighted-Leaner-Regression）\" class=\"headerlink\" title=\"局部加权线性回归（Local Weighted Leaner Regression）\"></a>局部加权线性回归（Local Weighted Leaner Regression）</h4><p>  线性回归的问题是容易欠拟合，因此引入局部加权</p>\n<script type=\"math/tex; mode=display\">\\hat{w}=(X^twX)^{-1}X^twy</script><p>  通常使用高斯核用来赋于权重$w(i,i)=e^{\\frac {x^{(i)}-x}{-2k^2}}$，即有离预测点越近的点权重越大，同时参数K控制了参与运算的数据，这样就有如下图:<br><img src=\"http://p15i7i801.bkt.clouddn.com/d1676ee6aefd1a476cc7f40036c87f5c.png\" alt=\"aiml-localweightedLR\"></p>\n<p>  此方法缺点在于<strong>计算量增加，每次预测必须在全集上运行</strong>，好处在于可以控制拟合程度<br><img src=\"http://p15i7i801.bkt.clouddn.com/e9cc4d793fd511a4e7749fa6660005c5.png\" alt=\"aiml-LR\"></p>\n<h4 id=\"缩减系数——用于更好的理解数据并处理特征比样本多的问题\"><a href=\"#缩减系数——用于更好的理解数据并处理特征比样本多的问题\" class=\"headerlink\" title=\"缩减系数——用于更好的理解数据并处理特征比样本多的问题\"></a>缩减系数——用于更好的理解数据并处理特征比样本多的问题</h4><p>  对于$X^TX$为奇异矩阵，在$X^TX$基础加上 $\\lambda I$使得矩阵非奇异，这种方法称为<em>岭回归</em>。此方法有三个用途：</p>\n<ul>\n<li>用于处理特征比样本多的情况</li>\n<li>用于在估计中加入偏差，从而得到更好的估计</li>\n<li>通过引入   $\\lambda$ 来限制 $w$ 之和，通过引入惩罚，能够减少不重要的参数，这类技术在统计学称为衰减。（迹不变）</li>\n<li>此公式$$等价于最小二乘加入约束</li>\n</ul>\n<h3 id=\"附加篇：各类方法的一个归纳\"><a href=\"#附加篇：各类方法的一个归纳\" class=\"headerlink\" title=\"附加篇：各类方法的一个归纳\"></a>附加篇：各类方法的一个归纳</h3><ul>\n<li>树模型：DT, GBDT, RF</li>\n<li>概率模型：Bayes</li>\n<li>最优化：LR, SVM<ul>\n<li>LR：简单实用，互联网CTR预估被广泛使用</li>\n<li>SVM：简单、多分类问题比SVM好，不好解释；</li>\n</ul>\n</li>\n<li>距离划分，判别模型<ul>\n<li>KNN：简单、多分类问题比SVM好，不好解释；</li>\n</ul>\n</li>\n<li>集成方法：Adaboost</li>\n<li>方法比较<ul>\n<li>NB与LR<ul>\n<li>相同点：都是特征的线性表述，解释性对较好；</li>\n<li>不同点：NB要求严格独立，NB有利于并行化，但随着数据量和维度增多，LR更合适</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>可持久化的模型：决策树</li>\n</ul>\n<h3 id=\"决策树-随机森林-adaboost-GBDT\"><a href=\"#决策树-随机森林-adaboost-GBDT\" class=\"headerlink\" title=\"决策树|随机森林|adaboost|GBDT\"></a>决策树|随机森林|adaboost|GBDT</h3><ul>\n<li><a href=\"http://www.cnblogs.com/wentingtu/archive/2011/12/13/2286212.html\" target=\"_blank\" rel=\"noopener\">决策树模型组合之（在线）随机森林与GBDT</a></li>\n<li><a href=\"http://blog.csdn.net/xlinsist/article/details/51475345\" target=\"_blank\" rel=\"noopener\">Bagging（Bootstrap aggregating）、随机森林（random forests）、AdaBoost</a></li>\n<li><a href=\"http://blog.csdn.net/xlinsist/article/details/51468741\" target=\"_blank\" rel=\"noopener\">决策树ID3、C4.5、C5.0以及CART算法之间的比较-并用scikit-learn决策树拟合Iris数据集</a></li>\n<li>ID3缺点:<ul>\n<li>对于多值的属性非常之敏感</li>\n<li>无法处理连续值</li>\n<li>容易产生过拟合，即训练数据训练太彻底，但大一点数据集错误率上升，关注了些局部信息</li>\n</ul>\n</li>\n<li>C4.5—-C5.0<ul>\n<li>解决上述1、2的问题，核心思想是加入了增益率</li>\n</ul>\n</li>\n<li>CART<ul>\n<li>能预测出值（回归），同时能全用训练和交叉验证不断评估，修剪决策树</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"K-nearest\"><a href=\"#K-nearest\" class=\"headerlink\" title=\"K-nearest\"></a>K-nearest</h3><ul>\n<li>思路：基于现有已知标签的样本数据集，寻找被预测样本的最近的K个样本，返回K个里频率最多的那个标签。</li>\n<li>优点：最简单有效的分类、对异常值不敏感</li>\n<li>缺点：解释性差、类别评分无规格化，不平衡问题</li>\n<li>适应：<a href=\"http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors\" target=\"_blank\" rel=\"noopener\">brute force\\kd_tree\\balltree的选择</a></li>\n</ul>\n<h3 id=\"PCA与SVD\"><a href=\"#PCA与SVD\" class=\"headerlink\" title=\"PCA与SVD\"></a>PCA与SVD</h3><p><a href=\"http://blog.csdn.net/jacke121/article/details/59057192\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/jacke121/article/details/59057192</a><br>pca是单方向上的降维，SVD是两个方向上的降维。<br>SVD表示是$A=U\\Sigma V^T$，其中U,V是正交的。<br>若要降维：使用这个公式，其具体数学推导，暂不明。现实含义可以这样理解：<br>在SVD的三个矩阵（假设叫1，2，3）中，1-2表示用户降维转换，2-3表示物品降维转换，在用户推荐那个场景，由于是使用基于物品的推荐，要计算两物品的相似度，则可理解为只需要用户角度降维，即1-2的转换。即有</p>\n<script type=\"math/tex; mode=display\">U_k \\Sigma V=> A^T U_k \\Sigma</script><p>推导点：<strong>如如此在用户上降维，那是否可以直接通过PCA在用户角度降维？</strong></p>\n<h3 id=\"关联分析\"><a href=\"#关联分析\" class=\"headerlink\" title=\"关联分析\"></a>关联分析</h3><p>支持度，可信度</p>\n<ul>\n<li>Apriori</li>\n</ul>\n<h3 id=\"附加篇一\"><a href=\"#附加篇一\" class=\"headerlink\" title=\"附加篇一\"></a>附加篇一</h3><blockquote>\n<p>张同学的问题：SVM中，如下代码中权重 $w$ 为何不能做特征的权重值。</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calcWs</span><span class=\"params\">(alphas, dataArr, classLabels)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    输入：alphas, 数据集, 类别标签</span></span><br><span class=\"line\"><span class=\"string\">    输出：目标w</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    X = mat(dataArr)</span><br><span class=\"line\">    labelMat = mat(classLabels).transpose()</span><br><span class=\"line\">    m, n = shape(X)</span><br><span class=\"line\">    w = zeros((n, <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(m):</span><br><span class=\"line\">        w += multiply(alphas[i] * labelMat[i], X[i, :].T)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> w</span><br></pre></td></tr></table></figure>\n<p>对比线性情况而言,有 $w= \\Sigma \\alpha y_i x_i$ ，书中有下代码<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w += multiply(alphas[i] * labelMat[i], X[i, :].T)</span><br></pre></td></tr></table></figure></p>\n<p>目标函数为：$y=(\\Sigma \\alpha y_i x_i) x + b$ 而对于映射核空间的情况，书中有描述<br><img src=\"http://p15i7i801.bkt.clouddn.com/c9fb298cf712884a5ad1c6d2407c4b95.png\" alt=\"aiml-weightbook\"><br>此时目标函数则为 $y=\\Sigma \\alpha y_i k(\\phi(x_i),\\phi(x_j))+b$。显然，特征向量 $w$ 是无法拆出来的。</p>\n<blockquote>\n<p>对weights = weights + alpha <em> dataMatrix.transpose() </em> error 这一行的理解</p>\n</blockquote>\n<ul>\n<li>角度一：<a href=\"http://blog.csdn.net/lu597203933/article/details/38468303\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/lu597203933/article/details/38468303</a>  AndrewNg<br><img src=\"http://p15i7i801.bkt.clouddn.com/79f4eb72472afa25f82690b0823b603d.png\" alt=\"aiml-weight\"></li>\n<li>角度二：矩阵微分的角度理解 <a href=\"http://blog.csdn.net/aichipmunk/article/details/9382503\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/aichipmunk/article/details/9382503</a></li>\n</ul>\n<h3 id=\"附加篇二：机器学习算法路径图\"><a href=\"#附加篇二：机器学习算法路径图\" class=\"headerlink\" title=\"附加篇二：机器学习算法路径图\"></a>附加篇二：机器学习算法路径图</h3><p><img src=\"http://p15i7i801.bkt.clouddn.com/e161673fc2063c42f4d407fd7057fc1f.png\" alt=\"aiml-algorithm chea-sheet\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/daa070eafedebe29f976a12a0b65b08c.png\" alt=\"aiml-td\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"树回归\"><a href=\"#树回归\" class=\"headerlink\" title=\"树回归\"></a>树回归</h3><ul>\n<li>连续型数值的混乱度：总平方误差来表示，即方差*m</li>\n<li>离散型的标称的混乱度可以用香农熵表示</li>\n<li>若叶节使用的模型是分段常数则称为回归树，若叶节点使用的模型是线性回归方程则称为模型树</li>\n<li>使用CART构建出的树倾向于过拟合，分别使用预剪枝和后剪枝解决这个问题。预剪枝设置停止条件来调整拟合问题，这需要不断调整，见P168。而后剪枝首先构造足够复杂的树，然后使用测试集来判断叶节点合并能否降低误差</li>\n<li>决策树通过迭代实现分类，离散情况下，叶节点的类别即预测的类别，连续情况下，若是回归树则是该子树叶子节点的平均值，若是模型树，则是该（线性）模型的值</li>\n</ul>\n<h3 id=\"Random-Forest-GBDT\"><a href=\"#Random-Forest-GBDT\" class=\"headerlink\" title=\"Random Forest/GBDT\"></a>Random Forest/GBDT</h3><p><a href=\"http://blog.csdn.net/google19890102/article/details/51746402\" target=\"_blank\" rel=\"noopener\">参考介绍一：二分类</a><br><a href=\"http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html\" target=\"_blank\" rel=\"noopener\">参考介绍一:多分类</a></p>\n<ul>\n<li>随机建立树的方式，随机采样包括行采样（样本），列采样（特征）</li>\n<li>即本质是每颗树是窄领域内的专家，众专家一起投票</li>\n</ul>\n<h3 id=\"预测数值型数据-回归\"><a href=\"#预测数值型数据-回归\" class=\"headerlink\" title=\"预测数值型数据:回归\"></a>预测数值型数据:回归</h3><h4 id=\"线性回归\"><a href=\"#线性回归\" class=\"headerlink\" title=\"线性回归\"></a>线性回归</h4><p>  使用最小二乘法，得$若X非奇异，则有w=(X^tX)^{-1}X^ty$</p>\n<h4 id=\"局部加权线性回归（Local-Weighted-Leaner-Regression）\"><a href=\"#局部加权线性回归（Local-Weighted-Leaner-Regression）\" class=\"headerlink\" title=\"局部加权线性回归（Local Weighted Leaner Regression）\"></a>局部加权线性回归（Local Weighted Leaner Regression）</h4><p>  线性回归的问题是容易欠拟合，因此引入局部加权</p>\n<script type=\"math/tex; mode=display\">\\hat{w}=(X^twX)^{-1}X^twy</script><p>  通常使用高斯核用来赋于权重$w(i,i)=e^{\\frac {x^{(i)}-x}{-2k^2}}$，即有离预测点越近的点权重越大，同时参数K控制了参与运算的数据，这样就有如下图:<br><img src=\"http://p15i7i801.bkt.clouddn.com/d1676ee6aefd1a476cc7f40036c87f5c.png\" alt=\"aiml-localweightedLR\"></p>\n<p>  此方法缺点在于<strong>计算量增加，每次预测必须在全集上运行</strong>，好处在于可以控制拟合程度<br><img src=\"http://p15i7i801.bkt.clouddn.com/e9cc4d793fd511a4e7749fa6660005c5.png\" alt=\"aiml-LR\"></p>\n<h4 id=\"缩减系数——用于更好的理解数据并处理特征比样本多的问题\"><a href=\"#缩减系数——用于更好的理解数据并处理特征比样本多的问题\" class=\"headerlink\" title=\"缩减系数——用于更好的理解数据并处理特征比样本多的问题\"></a>缩减系数——用于更好的理解数据并处理特征比样本多的问题</h4><p>  对于$X^TX$为奇异矩阵，在$X^TX$基础加上 $\\lambda I$使得矩阵非奇异，这种方法称为<em>岭回归</em>。此方法有三个用途：</p>\n<ul>\n<li>用于处理特征比样本多的情况</li>\n<li>用于在估计中加入偏差，从而得到更好的估计</li>\n<li>通过引入   $\\lambda$ 来限制 $w$ 之和，通过引入惩罚，能够减少不重要的参数，这类技术在统计学称为衰减。（迹不变）</li>\n<li>此公式$$等价于最小二乘加入约束</li>\n</ul>\n<h3 id=\"附加篇：各类方法的一个归纳\"><a href=\"#附加篇：各类方法的一个归纳\" class=\"headerlink\" title=\"附加篇：各类方法的一个归纳\"></a>附加篇：各类方法的一个归纳</h3><ul>\n<li>树模型：DT, GBDT, RF</li>\n<li>概率模型：Bayes</li>\n<li>最优化：LR, SVM<ul>\n<li>LR：简单实用，互联网CTR预估被广泛使用</li>\n<li>SVM：简单、多分类问题比SVM好，不好解释；</li>\n</ul>\n</li>\n<li>距离划分，判别模型<ul>\n<li>KNN：简单、多分类问题比SVM好，不好解释；</li>\n</ul>\n</li>\n<li>集成方法：Adaboost</li>\n<li>方法比较<ul>\n<li>NB与LR<ul>\n<li>相同点：都是特征的线性表述，解释性对较好；</li>\n<li>不同点：NB要求严格独立，NB有利于并行化，但随着数据量和维度增多，LR更合适</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>可持久化的模型：决策树</li>\n</ul>\n<h3 id=\"决策树-随机森林-adaboost-GBDT\"><a href=\"#决策树-随机森林-adaboost-GBDT\" class=\"headerlink\" title=\"决策树|随机森林|adaboost|GBDT\"></a>决策树|随机森林|adaboost|GBDT</h3><ul>\n<li><a href=\"http://www.cnblogs.com/wentingtu/archive/2011/12/13/2286212.html\" target=\"_blank\" rel=\"noopener\">决策树模型组合之（在线）随机森林与GBDT</a></li>\n<li><a href=\"http://blog.csdn.net/xlinsist/article/details/51475345\" target=\"_blank\" rel=\"noopener\">Bagging（Bootstrap aggregating）、随机森林（random forests）、AdaBoost</a></li>\n<li><a href=\"http://blog.csdn.net/xlinsist/article/details/51468741\" target=\"_blank\" rel=\"noopener\">决策树ID3、C4.5、C5.0以及CART算法之间的比较-并用scikit-learn决策树拟合Iris数据集</a></li>\n<li>ID3缺点:<ul>\n<li>对于多值的属性非常之敏感</li>\n<li>无法处理连续值</li>\n<li>容易产生过拟合，即训练数据训练太彻底，但大一点数据集错误率上升，关注了些局部信息</li>\n</ul>\n</li>\n<li>C4.5—-C5.0<ul>\n<li>解决上述1、2的问题，核心思想是加入了增益率</li>\n</ul>\n</li>\n<li>CART<ul>\n<li>能预测出值（回归），同时能全用训练和交叉验证不断评估，修剪决策树</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"K-nearest\"><a href=\"#K-nearest\" class=\"headerlink\" title=\"K-nearest\"></a>K-nearest</h3><ul>\n<li>思路：基于现有已知标签的样本数据集，寻找被预测样本的最近的K个样本，返回K个里频率最多的那个标签。</li>\n<li>优点：最简单有效的分类、对异常值不敏感</li>\n<li>缺点：解释性差、类别评分无规格化，不平衡问题</li>\n<li>适应：<a href=\"http://scikit-learn.org/stable/modules/neighbors.html#unsupervised-nearest-neighbors\" target=\"_blank\" rel=\"noopener\">brute force\\kd_tree\\balltree的选择</a></li>\n</ul>\n<h3 id=\"PCA与SVD\"><a href=\"#PCA与SVD\" class=\"headerlink\" title=\"PCA与SVD\"></a>PCA与SVD</h3><p><a href=\"http://blog.csdn.net/jacke121/article/details/59057192\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/jacke121/article/details/59057192</a><br>pca是单方向上的降维，SVD是两个方向上的降维。<br>SVD表示是$A=U\\Sigma V^T$，其中U,V是正交的。<br>若要降维：使用这个公式，其具体数学推导，暂不明。现实含义可以这样理解：<br>在SVD的三个矩阵（假设叫1，2，3）中，1-2表示用户降维转换，2-3表示物品降维转换，在用户推荐那个场景，由于是使用基于物品的推荐，要计算两物品的相似度，则可理解为只需要用户角度降维，即1-2的转换。即有</p>\n<script type=\"math/tex; mode=display\">U_k \\Sigma V=> A^T U_k \\Sigma</script><p>推导点：<strong>如如此在用户上降维，那是否可以直接通过PCA在用户角度降维？</strong></p>\n<h3 id=\"关联分析\"><a href=\"#关联分析\" class=\"headerlink\" title=\"关联分析\"></a>关联分析</h3><p>支持度，可信度</p>\n<ul>\n<li>Apriori</li>\n</ul>\n<h3 id=\"附加篇一\"><a href=\"#附加篇一\" class=\"headerlink\" title=\"附加篇一\"></a>附加篇一</h3><blockquote>\n<p>张同学的问题：SVM中，如下代码中权重 $w$ 为何不能做特征的权重值。</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calcWs</span><span class=\"params\">(alphas, dataArr, classLabels)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    输入：alphas, 数据集, 类别标签</span></span><br><span class=\"line\"><span class=\"string\">    输出：目标w</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    X = mat(dataArr)</span><br><span class=\"line\">    labelMat = mat(classLabels).transpose()</span><br><span class=\"line\">    m, n = shape(X)</span><br><span class=\"line\">    w = zeros((n, <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(m):</span><br><span class=\"line\">        w += multiply(alphas[i] * labelMat[i], X[i, :].T)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> w</span><br></pre></td></tr></table></figure>\n<p>对比线性情况而言,有 $w= \\Sigma \\alpha y_i x_i$ ，书中有下代码<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w += multiply(alphas[i] * labelMat[i], X[i, :].T)</span><br></pre></td></tr></table></figure></p>\n<p>目标函数为：$y=(\\Sigma \\alpha y_i x_i) x + b$ 而对于映射核空间的情况，书中有描述<br><img src=\"http://p15i7i801.bkt.clouddn.com/c9fb298cf712884a5ad1c6d2407c4b95.png\" alt=\"aiml-weightbook\"><br>此时目标函数则为 $y=\\Sigma \\alpha y_i k(\\phi(x_i),\\phi(x_j))+b$。显然，特征向量 $w$ 是无法拆出来的。</p>\n<blockquote>\n<p>对weights = weights + alpha <em> dataMatrix.transpose() </em> error 这一行的理解</p>\n</blockquote>\n<ul>\n<li>角度一：<a href=\"http://blog.csdn.net/lu597203933/article/details/38468303\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/lu597203933/article/details/38468303</a>  AndrewNg<br><img src=\"http://p15i7i801.bkt.clouddn.com/79f4eb72472afa25f82690b0823b603d.png\" alt=\"aiml-weight\"></li>\n<li>角度二：矩阵微分的角度理解 <a href=\"http://blog.csdn.net/aichipmunk/article/details/9382503\" target=\"_blank\" rel=\"noopener\">http://blog.csdn.net/aichipmunk/article/details/9382503</a></li>\n</ul>\n<h3 id=\"附加篇二：机器学习算法路径图\"><a href=\"#附加篇二：机器学习算法路径图\" class=\"headerlink\" title=\"附加篇二：机器学习算法路径图\"></a>附加篇二：机器学习算法路径图</h3><p><img src=\"http://p15i7i801.bkt.clouddn.com/e161673fc2063c42f4d407fd7057fc1f.png\" alt=\"aiml-algorithm chea-sheet\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/daa070eafedebe29f976a12a0b65b08c.png\" alt=\"aiml-td\"></p>\n"},{"title":"Practical introduction to data structures and algorithm analysis","date":"2017-09-28T03:12:18.000Z","mathjax":true,"_content":"\n# 概率论及一些基础思考\n## 代价与效益\n在有效的资源下将问题解决，称该算法是有效的。一个好的数据结构（解决方案）设计首先要考虑到业务场景，在操作上（增删改查），它有什么需求。比如查询上要求并发高、但在插入速度上并无太高的要求，或者反之。\n## 计算复杂度\n* 通常我们考虑算法的在一定数据规模下的平均复杂度，但平均复杂度分析并不是一定可靠的，这建立在输入数据概率分布已知的情况下考虑的，通常这种假设每个元素出现的概率均等。但是不正确的假设也可能带来灾难的性的后果，特别是在搜索类算法上，如散列和搜索树，具体事例可参考9.4、5.4节。有时间考虑特殊的数据分布，也会带来好处，如9.2节（自组织线性表）\n* 在实时系统中不考虑平均情况，而考虑最差的情况。\n* 复杂度分析，通常采用渐近分析的方法，使用化简的方法计算上界，下界和平均情况。\n\n## 最完美算法\n联机算法——举例而言，最大子序列求和的问题\n## 运行时间中的对数\n* 如二叉树，contains操作，较明显，在常数据O(1)时间问题的大小缩减为其一部分（通过是一半或小于一半），则该算法是O(logN)\n* 最大公因(约）数，即使用欧几里德/辗转相除法计算gcd(M,N)。由定理，若M>N，则有M mod N < M/2，可得该算法迭代次数至多是O(logN)\n* 幂次运算，使用递归分解，将乘法降到O(logN)\n\n# 基本数据结构\n## 表、栈、队列\n## 树\n\n### 二叉树\n\n可以使用栈来实现表达式树，也可用递归，这样会方便些。基本操作包括增(insert)、删(remove,两种可能性)、查(contains, findmin,findmax)，平均计算复杂度：\n\n得O(nlogN)，但这无法控制上界，**为控制任意次连续M次操作在最坏的情况下花费时间为O(MlogN)，引入AVL平衡树，当然它无法保证单次的操作的时间界。最后根据二八法则，将引入伸展树**\n### AVL平衡树\n\n  使用单旋和双旋调整高度，结构上每个节点信息加入了高度。核心插入代码：\n```C++\n    void insert( const Comparable & x, AvlNode * & t )\n    {\n        if( t == NULL )\n            t = new AvlNode( x, NULL, NULL );\n        else if( x < t->element )\n        {\n            insert( x, t->left );\n            if( height( t->left ) - height( t->right ) == 2 )\n                if( x < t->left->element )\n                    rotateWithLeftChild( t );\n                else\n                    doubleWithLeftChild( t );\n        }\n        else if( t->element < x )\n        {\n            insert( x, t->right );\n            if( height( t->right ) - height( t->left ) == 2 )\n                if( t->right->element < x )\n                    rotateWithRightChild( t );\n                else\n                    doubleWithRightChild( t );\n        }\n        else\n            ;  // Duplicate; do nothing\n        t->height = max( height( t->left ), height( t->right ) ) + 1;\n    }\n```\n### 伸展树\n之形伸展与一字伸展，目标是沿访问路径旋转  \n### 树的遍历\n### B树——数据库的索引树，用于解决大量数据磁盘检索的问题\n注意B树的定义，5个特性，M\\L参数的确定。如一个区块8K，每个键32B，在一颗M阶的B树中有M-1个键，则可算出M。对于叶子节点若每个记录是256字节，区块是8K，则可算出L=32。再回来看定义：\n1. 数据在叶子节点，并在相同深度上，且数据项介于L/2到L之间；\n2. 非叶节点，存储M-1个键，指示查找方向，其儿子数在M/2到M之间;\n注意B树插入时的分裂操作和删除的认领操作。\n\n### STL中set和map的树形实现\n### 散列\n1. 散列函数:利用horner法则，计算多项式函数$h_k=k_0+37k_1+37k_2=(37+37k_2)k_1+k_0$。通常建议桶大小为素数\n2. 冲突解决方法：分离链接、线性/平方探测、双散列。分离链接法因子可以到1，性能下降不明显，探测方法最好不超过0.5，双散列宜用质数。\n3. 再散列：一般使用途中策略，到达装载因子再散列\n4. hashmap, hashset\n5. 散列比起二叉树而言速度虽有所提升但缺乏排序，通常用在比如编译器的符号表，拼写检查等等  \n\n### 优先队列（堆）\n\n##  排序与检索\n## 高级话题\n","source":"_posts/Practical-introduction-to-data-structures-and-algorithm-analysis.md","raw":"---\ntitle: Practical introduction to data structures and algorithm analysis\ndate: 2017-09-28 11:12:18\ntags:\n      - 数据结构\n      - 经典著作\ncategories: 深耕码农\nmathjax: true\n---\n\n# 概率论及一些基础思考\n## 代价与效益\n在有效的资源下将问题解决，称该算法是有效的。一个好的数据结构（解决方案）设计首先要考虑到业务场景，在操作上（增删改查），它有什么需求。比如查询上要求并发高、但在插入速度上并无太高的要求，或者反之。\n## 计算复杂度\n* 通常我们考虑算法的在一定数据规模下的平均复杂度，但平均复杂度分析并不是一定可靠的，这建立在输入数据概率分布已知的情况下考虑的，通常这种假设每个元素出现的概率均等。但是不正确的假设也可能带来灾难的性的后果，特别是在搜索类算法上，如散列和搜索树，具体事例可参考9.4、5.4节。有时间考虑特殊的数据分布，也会带来好处，如9.2节（自组织线性表）\n* 在实时系统中不考虑平均情况，而考虑最差的情况。\n* 复杂度分析，通常采用渐近分析的方法，使用化简的方法计算上界，下界和平均情况。\n\n## 最完美算法\n联机算法——举例而言，最大子序列求和的问题\n## 运行时间中的对数\n* 如二叉树，contains操作，较明显，在常数据O(1)时间问题的大小缩减为其一部分（通过是一半或小于一半），则该算法是O(logN)\n* 最大公因(约）数，即使用欧几里德/辗转相除法计算gcd(M,N)。由定理，若M>N，则有M mod N < M/2，可得该算法迭代次数至多是O(logN)\n* 幂次运算，使用递归分解，将乘法降到O(logN)\n\n# 基本数据结构\n## 表、栈、队列\n## 树\n\n### 二叉树\n\n可以使用栈来实现表达式树，也可用递归，这样会方便些。基本操作包括增(insert)、删(remove,两种可能性)、查(contains, findmin,findmax)，平均计算复杂度：\n\n得O(nlogN)，但这无法控制上界，**为控制任意次连续M次操作在最坏的情况下花费时间为O(MlogN)，引入AVL平衡树，当然它无法保证单次的操作的时间界。最后根据二八法则，将引入伸展树**\n### AVL平衡树\n\n  使用单旋和双旋调整高度，结构上每个节点信息加入了高度。核心插入代码：\n```C++\n    void insert( const Comparable & x, AvlNode * & t )\n    {\n        if( t == NULL )\n            t = new AvlNode( x, NULL, NULL );\n        else if( x < t->element )\n        {\n            insert( x, t->left );\n            if( height( t->left ) - height( t->right ) == 2 )\n                if( x < t->left->element )\n                    rotateWithLeftChild( t );\n                else\n                    doubleWithLeftChild( t );\n        }\n        else if( t->element < x )\n        {\n            insert( x, t->right );\n            if( height( t->right ) - height( t->left ) == 2 )\n                if( t->right->element < x )\n                    rotateWithRightChild( t );\n                else\n                    doubleWithRightChild( t );\n        }\n        else\n            ;  // Duplicate; do nothing\n        t->height = max( height( t->left ), height( t->right ) ) + 1;\n    }\n```\n### 伸展树\n之形伸展与一字伸展，目标是沿访问路径旋转  \n### 树的遍历\n### B树——数据库的索引树，用于解决大量数据磁盘检索的问题\n注意B树的定义，5个特性，M\\L参数的确定。如一个区块8K，每个键32B，在一颗M阶的B树中有M-1个键，则可算出M。对于叶子节点若每个记录是256字节，区块是8K，则可算出L=32。再回来看定义：\n1. 数据在叶子节点，并在相同深度上，且数据项介于L/2到L之间；\n2. 非叶节点，存储M-1个键，指示查找方向，其儿子数在M/2到M之间;\n注意B树插入时的分裂操作和删除的认领操作。\n\n### STL中set和map的树形实现\n### 散列\n1. 散列函数:利用horner法则，计算多项式函数$h_k=k_0+37k_1+37k_2=(37+37k_2)k_1+k_0$。通常建议桶大小为素数\n2. 冲突解决方法：分离链接、线性/平方探测、双散列。分离链接法因子可以到1，性能下降不明显，探测方法最好不超过0.5，双散列宜用质数。\n3. 再散列：一般使用途中策略，到达装载因子再散列\n4. hashmap, hashset\n5. 散列比起二叉树而言速度虽有所提升但缺乏排序，通常用在比如编译器的符号表，拼写检查等等  \n\n### 优先队列（堆）\n\n##  排序与检索\n## 高级话题\n","slug":"Practical-introduction-to-data-structures-and-algorithm-analysis","published":1,"updated":"2017-12-18T05:22:35.120Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbowgdpf0008j4n7f5a42brx","content":"<h1 id=\"概率论及一些基础思考\"><a href=\"#概率论及一些基础思考\" class=\"headerlink\" title=\"概率论及一些基础思考\"></a>概率论及一些基础思考</h1><h2 id=\"代价与效益\"><a href=\"#代价与效益\" class=\"headerlink\" title=\"代价与效益\"></a>代价与效益</h2><p>在有效的资源下将问题解决，称该算法是有效的。一个好的数据结构（解决方案）设计首先要考虑到业务场景，在操作上（增删改查），它有什么需求。比如查询上要求并发高、但在插入速度上并无太高的要求，或者反之。</p>\n<h2 id=\"计算复杂度\"><a href=\"#计算复杂度\" class=\"headerlink\" title=\"计算复杂度\"></a>计算复杂度</h2><ul>\n<li>通常我们考虑算法的在一定数据规模下的平均复杂度，但平均复杂度分析并不是一定可靠的，这建立在输入数据概率分布已知的情况下考虑的，通常这种假设每个元素出现的概率均等。但是不正确的假设也可能带来灾难的性的后果，特别是在搜索类算法上，如散列和搜索树，具体事例可参考9.4、5.4节。有时间考虑特殊的数据分布，也会带来好处，如9.2节（自组织线性表）</li>\n<li>在实时系统中不考虑平均情况，而考虑最差的情况。</li>\n<li>复杂度分析，通常采用渐近分析的方法，使用化简的方法计算上界，下界和平均情况。</li>\n</ul>\n<h2 id=\"最完美算法\"><a href=\"#最完美算法\" class=\"headerlink\" title=\"最完美算法\"></a>最完美算法</h2><p>联机算法——举例而言，最大子序列求和的问题</p>\n<h2 id=\"运行时间中的对数\"><a href=\"#运行时间中的对数\" class=\"headerlink\" title=\"运行时间中的对数\"></a>运行时间中的对数</h2><ul>\n<li>如二叉树，contains操作，较明显，在常数据O(1)时间问题的大小缩减为其一部分（通过是一半或小于一半），则该算法是O(logN)</li>\n<li>最大公因(约）数，即使用欧几里德/辗转相除法计算gcd(M,N)。由定理，若M&gt;N，则有M mod N &lt; M/2，可得该算法迭代次数至多是O(logN)</li>\n<li>幂次运算，使用递归分解，将乘法降到O(logN)</li>\n</ul>\n<h1 id=\"基本数据结构\"><a href=\"#基本数据结构\" class=\"headerlink\" title=\"基本数据结构\"></a>基本数据结构</h1><h2 id=\"表、栈、队列\"><a href=\"#表、栈、队列\" class=\"headerlink\" title=\"表、栈、队列\"></a>表、栈、队列</h2><h2 id=\"树\"><a href=\"#树\" class=\"headerlink\" title=\"树\"></a>树</h2><h3 id=\"二叉树\"><a href=\"#二叉树\" class=\"headerlink\" title=\"二叉树\"></a>二叉树</h3><p>可以使用栈来实现表达式树，也可用递归，这样会方便些。基本操作包括增(insert)、删(remove,两种可能性)、查(contains, findmin,findmax)，平均计算复杂度：</p>\n<p>得O(nlogN)，但这无法控制上界，<strong>为控制任意次连续M次操作在最坏的情况下花费时间为O(MlogN)，引入AVL平衡树，当然它无法保证单次的操作的时间界。最后根据二八法则，将引入伸展树</strong></p>\n<h3 id=\"AVL平衡树\"><a href=\"#AVL平衡树\" class=\"headerlink\" title=\"AVL平衡树\"></a>AVL平衡树</h3><p>  使用单旋和双旋调整高度，结构上每个节点信息加入了高度。核心插入代码：<br><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">insert</span><span class=\"params\">( <span class=\"keyword\">const</span> Comparable &amp; x, AvlNode * &amp; t )</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>( t == <span class=\"literal\">NULL</span> )</span><br><span class=\"line\">        t = <span class=\"keyword\">new</span> AvlNode( x, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span> );</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>( x &lt; t-&gt;element )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        insert( x, t-&gt;left );</span><br><span class=\"line\">        <span class=\"keyword\">if</span>( height( t-&gt;left ) - height( t-&gt;right ) == <span class=\"number\">2</span> )</span><br><span class=\"line\">            <span class=\"keyword\">if</span>( x &lt; t-&gt;left-&gt;element )</span><br><span class=\"line\">                rotateWithLeftChild( t );</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                doubleWithLeftChild( t );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>( t-&gt;element &lt; x )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        insert( x, t-&gt;right );</span><br><span class=\"line\">        <span class=\"keyword\">if</span>( height( t-&gt;right ) - height( t-&gt;left ) == <span class=\"number\">2</span> )</span><br><span class=\"line\">            <span class=\"keyword\">if</span>( t-&gt;right-&gt;element &lt; x )</span><br><span class=\"line\">                rotateWithRightChild( t );</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                doubleWithRightChild( t );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        ;  <span class=\"comment\">// Duplicate; do nothing</span></span><br><span class=\"line\">    t-&gt;height = max( height( t-&gt;left ), height( t-&gt;right ) ) + <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"伸展树\"><a href=\"#伸展树\" class=\"headerlink\" title=\"伸展树\"></a>伸展树</h3><p>之形伸展与一字伸展，目标是沿访问路径旋转  </p>\n<h3 id=\"树的遍历\"><a href=\"#树的遍历\" class=\"headerlink\" title=\"树的遍历\"></a>树的遍历</h3><h3 id=\"B树——数据库的索引树，用于解决大量数据磁盘检索的问题\"><a href=\"#B树——数据库的索引树，用于解决大量数据磁盘检索的问题\" class=\"headerlink\" title=\"B树——数据库的索引树，用于解决大量数据磁盘检索的问题\"></a>B树——数据库的索引树，用于解决大量数据磁盘检索的问题</h3><p>注意B树的定义，5个特性，M\\L参数的确定。如一个区块8K，每个键32B，在一颗M阶的B树中有M-1个键，则可算出M。对于叶子节点若每个记录是256字节，区块是8K，则可算出L=32。再回来看定义：</p>\n<ol>\n<li>数据在叶子节点，并在相同深度上，且数据项介于L/2到L之间；</li>\n<li>非叶节点，存储M-1个键，指示查找方向，其儿子数在M/2到M之间;<br>注意B树插入时的分裂操作和删除的认领操作。</li>\n</ol>\n<h3 id=\"STL中set和map的树形实现\"><a href=\"#STL中set和map的树形实现\" class=\"headerlink\" title=\"STL中set和map的树形实现\"></a>STL中set和map的树形实现</h3><h3 id=\"散列\"><a href=\"#散列\" class=\"headerlink\" title=\"散列\"></a>散列</h3><ol>\n<li>散列函数:利用horner法则，计算多项式函数$h_k=k_0+37k_1+37k_2=(37+37k_2)k_1+k_0$。通常建议桶大小为素数</li>\n<li>冲突解决方法：分离链接、线性/平方探测、双散列。分离链接法因子可以到1，性能下降不明显，探测方法最好不超过0.5，双散列宜用质数。</li>\n<li>再散列：一般使用途中策略，到达装载因子再散列</li>\n<li>hashmap, hashset</li>\n<li>散列比起二叉树而言速度虽有所提升但缺乏排序，通常用在比如编译器的符号表，拼写检查等等  </li>\n</ol>\n<h3 id=\"优先队列（堆）\"><a href=\"#优先队列（堆）\" class=\"headerlink\" title=\"优先队列（堆）\"></a>优先队列（堆）</h3><h2 id=\"排序与检索\"><a href=\"#排序与检索\" class=\"headerlink\" title=\"排序与检索\"></a>排序与检索</h2><h2 id=\"高级话题\"><a href=\"#高级话题\" class=\"headerlink\" title=\"高级话题\"></a>高级话题</h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"概率论及一些基础思考\"><a href=\"#概率论及一些基础思考\" class=\"headerlink\" title=\"概率论及一些基础思考\"></a>概率论及一些基础思考</h1><h2 id=\"代价与效益\"><a href=\"#代价与效益\" class=\"headerlink\" title=\"代价与效益\"></a>代价与效益</h2><p>在有效的资源下将问题解决，称该算法是有效的。一个好的数据结构（解决方案）设计首先要考虑到业务场景，在操作上（增删改查），它有什么需求。比如查询上要求并发高、但在插入速度上并无太高的要求，或者反之。</p>\n<h2 id=\"计算复杂度\"><a href=\"#计算复杂度\" class=\"headerlink\" title=\"计算复杂度\"></a>计算复杂度</h2><ul>\n<li>通常我们考虑算法的在一定数据规模下的平均复杂度，但平均复杂度分析并不是一定可靠的，这建立在输入数据概率分布已知的情况下考虑的，通常这种假设每个元素出现的概率均等。但是不正确的假设也可能带来灾难的性的后果，特别是在搜索类算法上，如散列和搜索树，具体事例可参考9.4、5.4节。有时间考虑特殊的数据分布，也会带来好处，如9.2节（自组织线性表）</li>\n<li>在实时系统中不考虑平均情况，而考虑最差的情况。</li>\n<li>复杂度分析，通常采用渐近分析的方法，使用化简的方法计算上界，下界和平均情况。</li>\n</ul>\n<h2 id=\"最完美算法\"><a href=\"#最完美算法\" class=\"headerlink\" title=\"最完美算法\"></a>最完美算法</h2><p>联机算法——举例而言，最大子序列求和的问题</p>\n<h2 id=\"运行时间中的对数\"><a href=\"#运行时间中的对数\" class=\"headerlink\" title=\"运行时间中的对数\"></a>运行时间中的对数</h2><ul>\n<li>如二叉树，contains操作，较明显，在常数据O(1)时间问题的大小缩减为其一部分（通过是一半或小于一半），则该算法是O(logN)</li>\n<li>最大公因(约）数，即使用欧几里德/辗转相除法计算gcd(M,N)。由定理，若M&gt;N，则有M mod N &lt; M/2，可得该算法迭代次数至多是O(logN)</li>\n<li>幂次运算，使用递归分解，将乘法降到O(logN)</li>\n</ul>\n<h1 id=\"基本数据结构\"><a href=\"#基本数据结构\" class=\"headerlink\" title=\"基本数据结构\"></a>基本数据结构</h1><h2 id=\"表、栈、队列\"><a href=\"#表、栈、队列\" class=\"headerlink\" title=\"表、栈、队列\"></a>表、栈、队列</h2><h2 id=\"树\"><a href=\"#树\" class=\"headerlink\" title=\"树\"></a>树</h2><h3 id=\"二叉树\"><a href=\"#二叉树\" class=\"headerlink\" title=\"二叉树\"></a>二叉树</h3><p>可以使用栈来实现表达式树，也可用递归，这样会方便些。基本操作包括增(insert)、删(remove,两种可能性)、查(contains, findmin,findmax)，平均计算复杂度：</p>\n<p>得O(nlogN)，但这无法控制上界，<strong>为控制任意次连续M次操作在最坏的情况下花费时间为O(MlogN)，引入AVL平衡树，当然它无法保证单次的操作的时间界。最后根据二八法则，将引入伸展树</strong></p>\n<h3 id=\"AVL平衡树\"><a href=\"#AVL平衡树\" class=\"headerlink\" title=\"AVL平衡树\"></a>AVL平衡树</h3><p>  使用单旋和双旋调整高度，结构上每个节点信息加入了高度。核心插入代码：<br><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">insert</span><span class=\"params\">( <span class=\"keyword\">const</span> Comparable &amp; x, AvlNode * &amp; t )</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>( t == <span class=\"literal\">NULL</span> )</span><br><span class=\"line\">        t = <span class=\"keyword\">new</span> AvlNode( x, <span class=\"literal\">NULL</span>, <span class=\"literal\">NULL</span> );</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>( x &lt; t-&gt;element )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        insert( x, t-&gt;left );</span><br><span class=\"line\">        <span class=\"keyword\">if</span>( height( t-&gt;left ) - height( t-&gt;right ) == <span class=\"number\">2</span> )</span><br><span class=\"line\">            <span class=\"keyword\">if</span>( x &lt; t-&gt;left-&gt;element )</span><br><span class=\"line\">                rotateWithLeftChild( t );</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                doubleWithLeftChild( t );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>( t-&gt;element &lt; x )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        insert( x, t-&gt;right );</span><br><span class=\"line\">        <span class=\"keyword\">if</span>( height( t-&gt;right ) - height( t-&gt;left ) == <span class=\"number\">2</span> )</span><br><span class=\"line\">            <span class=\"keyword\">if</span>( t-&gt;right-&gt;element &lt; x )</span><br><span class=\"line\">                rotateWithRightChild( t );</span><br><span class=\"line\">            <span class=\"keyword\">else</span></span><br><span class=\"line\">                doubleWithRightChild( t );</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">else</span></span><br><span class=\"line\">        ;  <span class=\"comment\">// Duplicate; do nothing</span></span><br><span class=\"line\">    t-&gt;height = max( height( t-&gt;left ), height( t-&gt;right ) ) + <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"伸展树\"><a href=\"#伸展树\" class=\"headerlink\" title=\"伸展树\"></a>伸展树</h3><p>之形伸展与一字伸展，目标是沿访问路径旋转  </p>\n<h3 id=\"树的遍历\"><a href=\"#树的遍历\" class=\"headerlink\" title=\"树的遍历\"></a>树的遍历</h3><h3 id=\"B树——数据库的索引树，用于解决大量数据磁盘检索的问题\"><a href=\"#B树——数据库的索引树，用于解决大量数据磁盘检索的问题\" class=\"headerlink\" title=\"B树——数据库的索引树，用于解决大量数据磁盘检索的问题\"></a>B树——数据库的索引树，用于解决大量数据磁盘检索的问题</h3><p>注意B树的定义，5个特性，M\\L参数的确定。如一个区块8K，每个键32B，在一颗M阶的B树中有M-1个键，则可算出M。对于叶子节点若每个记录是256字节，区块是8K，则可算出L=32。再回来看定义：</p>\n<ol>\n<li>数据在叶子节点，并在相同深度上，且数据项介于L/2到L之间；</li>\n<li>非叶节点，存储M-1个键，指示查找方向，其儿子数在M/2到M之间;<br>注意B树插入时的分裂操作和删除的认领操作。</li>\n</ol>\n<h3 id=\"STL中set和map的树形实现\"><a href=\"#STL中set和map的树形实现\" class=\"headerlink\" title=\"STL中set和map的树形实现\"></a>STL中set和map的树形实现</h3><h3 id=\"散列\"><a href=\"#散列\" class=\"headerlink\" title=\"散列\"></a>散列</h3><ol>\n<li>散列函数:利用horner法则，计算多项式函数$h_k=k_0+37k_1+37k_2=(37+37k_2)k_1+k_0$。通常建议桶大小为素数</li>\n<li>冲突解决方法：分离链接、线性/平方探测、双散列。分离链接法因子可以到1，性能下降不明显，探测方法最好不超过0.5，双散列宜用质数。</li>\n<li>再散列：一般使用途中策略，到达装载因子再散列</li>\n<li>hashmap, hashset</li>\n<li>散列比起二叉树而言速度虽有所提升但缺乏排序，通常用在比如编译器的符号表，拼写检查等等  </li>\n</ol>\n<h3 id=\"优先队列（堆）\"><a href=\"#优先队列（堆）\" class=\"headerlink\" title=\"优先队列（堆）\"></a>优先队列（堆）</h3><h2 id=\"排序与检索\"><a href=\"#排序与检索\" class=\"headerlink\" title=\"排序与检索\"></a>排序与检索</h2><h2 id=\"高级话题\"><a href=\"#高级话题\" class=\"headerlink\" title=\"高级话题\"></a>高级话题</h2>"},{"title":"Neural Networks for Machine Learning","date":"2016-06-07T05:26:48.000Z","mathjax":true,"_content":"\n\n\n# 机器学习：不必为特定任务编写程序\n## 场景落地\n- Recognizing pattern     \n  - Objects    \n  - facial    \n  - Spoken word\n- Recognizing anomalies    \n  - credit card transaction    \n  - Sensor reality\n- Prediction    \n  - stock price,\n  - exchange rate   \n  - movie like\n\n##几种简单模型\n- Linear neurons\n$ y=b+\\sum x_iw_i $\n- Binary threshold neurons\n$$\nz=\\Sigma x_i w_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt 0\\\\\n0&otherwise\n\\end{cases} \\quad or \\qquad\n\\theta=-b \\quad\nz=b+\\Sigma x_i y_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt \\theta\\\\\n0&otherwise\n\\end{cases}\n$$\n- Rectified Linear Neurons 2\n$$z=\\Sigma x_i w_i, y=\n\\begin{cases}\nz,\t&if\\;z \\gt 0 \\\\\n0,\t&\\mbox{otherwise}\n\\end{cases}$$\n- sigmoid neurons\n$$ z=b+\\Sigma x_i w_i  \\quad y=\\frac {1} {1+e^{-z}} $$\n![nnml-sigmoid](http://p15i7i801.bkt.clouddn.com/86069ad24cfd3541f4b33070b67fd749.png)\n- Stochastic binary neurons\n$$z=b+\\Sigma x_i w_i  \\quad p(s=1)=\\frac {1} {1+e^{-z}}$$\n\n## 学习的类型\n- supervised learning\n  * Regression\n    * model class: $y=f(x;w)$\n    * 1/2这个系数在求导时被抵消\n![nnml-regression](http://p15i7i801.bkt.clouddn.com/93512d5be2aafca06ff3d058aff511b7.png)\n  * classification\n- reinforced learning\n\n  the output is an action or sequence of actions and the only supervisory is an occasional scalar reward._This is difficult_, The rewads are typically delayed so its had to know where we went wrong( or right)\n- unsupervised learning\n\n  ![nnml-unsupervised](http://p15i7i801.bkt.clouddn.com/ada9c5e7fe1e06957b0797be4022d724.png)\n## 神经网络的类型\n- Feed-Forward neural network\n\n  ![nnml-feedforward](http://p15i7i801.bkt.clouddn.com/3053d06760561e6169f20829df15a358.png)\n\n  if there is more than one hidden layer, we call them \"deep\" neural networks.\n\n- Recurrent network\n\n  It is hard to train but natural way for modeling sequence, recent algorithm can be able to do this, this also can be used to predict stock price.\n\n  ![nnml-recurrent](http://p15i7i801.bkt.clouddn.com/83abfe6f730b3ae61d9099ff0d5ff300.png)\n![nnml-rnn](http://p15i7i801.bkt.clouddn.com/3a688df70ed6bf1e46d62a03402e2a39.png)\n> 20171218 LSTM 现在貌似更流行\n\n- Symmetrically connected networks\n\n## 感知器\n![nnml-perceptron-arch](http://p15i7i801.bkt.clouddn.com/a1ba9669ccb220d05a8140bc8e0ca797.png)\n> how to learn biases using the same rule as we use for learning weights\n\n  ![nnml-handlebias](http://p15i7i801.bkt.clouddn.com/d73047067e05f31a249dc755851602aa.png)\n\n> 学习的过程：if the output unit is correct, leave its weights alone, if the output unit incorrectly output\n\n![nnml-per-train](http://p15i7i801.bkt.clouddn.com/6b72ab6f556908380fcd6d5561d2e650.png)\n  - __错误的输出了零，则Z需要新增以致其逐渐大于零而产生y=1（往正面），反之则需要减__ ？？此偏移向量为负如何解释，或者必须保证 该向量是指向正确的分类超平面的（perpendicular，正交）。\n  - $(w,b) \\leftarrow (w,b)-(x,1)$\n    $(w,b) \\leftarrow (w,b)+(x,1)$\n![nnml-adpateside](http://p15i7i801.bkt.clouddn.com/b109e1d1e4424d180575bb0f74e75906.png)\n  - *just like penalty function*\n\n> 为什么学习有效: _非正式的收敛证明_\n\n  1. Each time the perceptron makes a mistake, the current weight vector moves to decrease its squared distance from every weight vector in the \"generously feasible\" region.\n  2. <strong> The squared distance decreases by at least the squared length of the input vector. </strong>（向量的运算，几何解释）\n  3. So after a finite number of mistakes, the weight vector must lie in the feasible region if the region exist.\n\n## 感知器的缺陷\n> **right feature**\n\n  Once the hand-coded features have been determined, there are very strong limitations on what a perceptron can learn.\n  _The main question is : This type of table look-up won't generalize, it need too many feature_\n  _此篇习题未理解_\n\n> Group Invariance Theorem---Minsky and Papert\n\ncan't learn to do this if the transformations from a group{欧氏变换之内}，so it needs to use multiple feature unit to recognize transformations of informative sub-patterns.\nSo,more  important was all about how you learn **feature detectors, that's hidden units**. After 20 years, We know,\nwe need multiple layer of adaptive, non-linear hidden units.\n","source":"_posts/Neural-Networks-for-Machine-Learning.md","raw":"---\ntitle: Neural Networks for Machine Learning\ndate: 2016-06-07 13:26:48\ntags:\n      - 神经网络\n      - Geoffrey hinton\n      - 公开课\ncategories: AI梦\nmathjax: true\n---\n\n\n\n# 机器学习：不必为特定任务编写程序\n## 场景落地\n- Recognizing pattern     \n  - Objects    \n  - facial    \n  - Spoken word\n- Recognizing anomalies    \n  - credit card transaction    \n  - Sensor reality\n- Prediction    \n  - stock price,\n  - exchange rate   \n  - movie like\n\n##几种简单模型\n- Linear neurons\n$ y=b+\\sum x_iw_i $\n- Binary threshold neurons\n$$\nz=\\Sigma x_i w_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt 0\\\\\n0&otherwise\n\\end{cases} \\quad or \\qquad\n\\theta=-b \\quad\nz=b+\\Sigma x_i y_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt \\theta\\\\\n0&otherwise\n\\end{cases}\n$$\n- Rectified Linear Neurons 2\n$$z=\\Sigma x_i w_i, y=\n\\begin{cases}\nz,\t&if\\;z \\gt 0 \\\\\n0,\t&\\mbox{otherwise}\n\\end{cases}$$\n- sigmoid neurons\n$$ z=b+\\Sigma x_i w_i  \\quad y=\\frac {1} {1+e^{-z}} $$\n![nnml-sigmoid](http://p15i7i801.bkt.clouddn.com/86069ad24cfd3541f4b33070b67fd749.png)\n- Stochastic binary neurons\n$$z=b+\\Sigma x_i w_i  \\quad p(s=1)=\\frac {1} {1+e^{-z}}$$\n\n## 学习的类型\n- supervised learning\n  * Regression\n    * model class: $y=f(x;w)$\n    * 1/2这个系数在求导时被抵消\n![nnml-regression](http://p15i7i801.bkt.clouddn.com/93512d5be2aafca06ff3d058aff511b7.png)\n  * classification\n- reinforced learning\n\n  the output is an action or sequence of actions and the only supervisory is an occasional scalar reward._This is difficult_, The rewads are typically delayed so its had to know where we went wrong( or right)\n- unsupervised learning\n\n  ![nnml-unsupervised](http://p15i7i801.bkt.clouddn.com/ada9c5e7fe1e06957b0797be4022d724.png)\n## 神经网络的类型\n- Feed-Forward neural network\n\n  ![nnml-feedforward](http://p15i7i801.bkt.clouddn.com/3053d06760561e6169f20829df15a358.png)\n\n  if there is more than one hidden layer, we call them \"deep\" neural networks.\n\n- Recurrent network\n\n  It is hard to train but natural way for modeling sequence, recent algorithm can be able to do this, this also can be used to predict stock price.\n\n  ![nnml-recurrent](http://p15i7i801.bkt.clouddn.com/83abfe6f730b3ae61d9099ff0d5ff300.png)\n![nnml-rnn](http://p15i7i801.bkt.clouddn.com/3a688df70ed6bf1e46d62a03402e2a39.png)\n> 20171218 LSTM 现在貌似更流行\n\n- Symmetrically connected networks\n\n## 感知器\n![nnml-perceptron-arch](http://p15i7i801.bkt.clouddn.com/a1ba9669ccb220d05a8140bc8e0ca797.png)\n> how to learn biases using the same rule as we use for learning weights\n\n  ![nnml-handlebias](http://p15i7i801.bkt.clouddn.com/d73047067e05f31a249dc755851602aa.png)\n\n> 学习的过程：if the output unit is correct, leave its weights alone, if the output unit incorrectly output\n\n![nnml-per-train](http://p15i7i801.bkt.clouddn.com/6b72ab6f556908380fcd6d5561d2e650.png)\n  - __错误的输出了零，则Z需要新增以致其逐渐大于零而产生y=1（往正面），反之则需要减__ ？？此偏移向量为负如何解释，或者必须保证 该向量是指向正确的分类超平面的（perpendicular，正交）。\n  - $(w,b) \\leftarrow (w,b)-(x,1)$\n    $(w,b) \\leftarrow (w,b)+(x,1)$\n![nnml-adpateside](http://p15i7i801.bkt.clouddn.com/b109e1d1e4424d180575bb0f74e75906.png)\n  - *just like penalty function*\n\n> 为什么学习有效: _非正式的收敛证明_\n\n  1. Each time the perceptron makes a mistake, the current weight vector moves to decrease its squared distance from every weight vector in the \"generously feasible\" region.\n  2. <strong> The squared distance decreases by at least the squared length of the input vector. </strong>（向量的运算，几何解释）\n  3. So after a finite number of mistakes, the weight vector must lie in the feasible region if the region exist.\n\n## 感知器的缺陷\n> **right feature**\n\n  Once the hand-coded features have been determined, there are very strong limitations on what a perceptron can learn.\n  _The main question is : This type of table look-up won't generalize, it need too many feature_\n  _此篇习题未理解_\n\n> Group Invariance Theorem---Minsky and Papert\n\ncan't learn to do this if the transformations from a group{欧氏变换之内}，so it needs to use multiple feature unit to recognize transformations of informative sub-patterns.\nSo,more  important was all about how you learn **feature detectors, that's hidden units**. After 20 years, We know,\nwe need multiple layer of adaptive, non-linear hidden units.\n","slug":"Neural-Networks-for-Machine-Learning","published":1,"updated":"2017-12-19T09:22:41.763Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbowgdph0009j4n7wxqu5wz0","content":"<h1 id=\"机器学习：不必为特定任务编写程序\"><a href=\"#机器学习：不必为特定任务编写程序\" class=\"headerlink\" title=\"机器学习：不必为特定任务编写程序\"></a>机器学习：不必为特定任务编写程序</h1><h2 id=\"场景落地\"><a href=\"#场景落地\" class=\"headerlink\" title=\"场景落地\"></a>场景落地</h2><ul>\n<li>Recognizing pattern     <ul>\n<li>Objects    </li>\n<li>facial    </li>\n<li>Spoken word</li>\n</ul>\n</li>\n<li>Recognizing anomalies    <ul>\n<li>credit card transaction    </li>\n<li>Sensor reality</li>\n</ul>\n</li>\n<li>Prediction    <ul>\n<li>stock price,</li>\n<li>exchange rate   </li>\n<li>movie like</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"几种简单模型\"><a href=\"#几种简单模型\" class=\"headerlink\" title=\"几种简单模型\"></a>几种简单模型</h2><ul>\n<li>Linear neurons<br>$ y=b+\\sum x_iw_i $</li>\n<li>Binary threshold neurons<script type=\"math/tex; mode=display\">\nz=\\Sigma x_i w_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt 0\\\\\n0&otherwise\n\\end{cases} \\quad or \\qquad\n\\theta=-b \\quad\nz=b+\\Sigma x_i y_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt \\theta\\\\\n0&otherwise\n\\end{cases}</script></li>\n<li>Rectified Linear Neurons 2<script type=\"math/tex; mode=display\">z=\\Sigma x_i w_i, y=\n\\begin{cases}\nz,    &if\\;z \\gt 0 \\\\\n0,    &\\mbox{otherwise}\n\\end{cases}</script></li>\n<li>sigmoid neurons<script type=\"math/tex; mode=display\">z=b+\\Sigma x_i w_i  \\quad y=\\frac {1} {1+e^{-z}}</script><img src=\"http://p15i7i801.bkt.clouddn.com/86069ad24cfd3541f4b33070b67fd749.png\" alt=\"nnml-sigmoid\"></li>\n<li>Stochastic binary neurons<script type=\"math/tex; mode=display\">z=b+\\Sigma x_i w_i  \\quad p(s=1)=\\frac {1} {1+e^{-z}}</script></li>\n</ul>\n<h2 id=\"学习的类型\"><a href=\"#学习的类型\" class=\"headerlink\" title=\"学习的类型\"></a>学习的类型</h2><ul>\n<li>supervised learning<ul>\n<li>Regression<ul>\n<li>model class: $y=f(x;w)$</li>\n<li>1/2这个系数在求导时被抵消<br><img src=\"http://p15i7i801.bkt.clouddn.com/93512d5be2aafca06ff3d058aff511b7.png\" alt=\"nnml-regression\"></li>\n</ul>\n</li>\n<li>classification</li>\n</ul>\n</li>\n<li><p>reinforced learning</p>\n<p>the output is an action or sequence of actions and the only supervisory is an occasional scalar reward._This is difficult_, The rewads are typically delayed so its had to know where we went wrong( or right)</p>\n</li>\n<li><p>unsupervised learning</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/ada9c5e7fe1e06957b0797be4022d724.png\" alt=\"nnml-unsupervised\"></p>\n<h2 id=\"神经网络的类型\"><a href=\"#神经网络的类型\" class=\"headerlink\" title=\"神经网络的类型\"></a>神经网络的类型</h2></li>\n<li><p>Feed-Forward neural network</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/3053d06760561e6169f20829df15a358.png\" alt=\"nnml-feedforward\"></p>\n<p>if there is more than one hidden layer, we call them “deep” neural networks.</p>\n</li>\n<li><p>Recurrent network</p>\n<p>It is hard to train but natural way for modeling sequence, recent algorithm can be able to do this, this also can be used to predict stock price.</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/83abfe6f730b3ae61d9099ff0d5ff300.png\" alt=\"nnml-recurrent\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/3a688df70ed6bf1e46d62a03402e2a39.png\" alt=\"nnml-rnn\"></p>\n<blockquote>\n<p>20171218 LSTM 现在貌似更流行</p>\n</blockquote>\n</li>\n<li><p>Symmetrically connected networks</p>\n</li>\n</ul>\n<h2 id=\"感知器\"><a href=\"#感知器\" class=\"headerlink\" title=\"感知器\"></a>感知器</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/a1ba9669ccb220d05a8140bc8e0ca797.png\" alt=\"nnml-perceptron-arch\"></p>\n<blockquote>\n<p>how to learn biases using the same rule as we use for learning weights</p>\n</blockquote>\n<p>  <img src=\"http://p15i7i801.bkt.clouddn.com/d73047067e05f31a249dc755851602aa.png\" alt=\"nnml-handlebias\"></p>\n<blockquote>\n<p>学习的过程：if the output unit is correct, leave its weights alone, if the output unit incorrectly output</p>\n</blockquote>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/6b72ab6f556908380fcd6d5561d2e650.png\" alt=\"nnml-per-train\"></p>\n<ul>\n<li><strong>错误的输出了零，则Z需要新增以致其逐渐大于零而产生y=1（往正面），反之则需要减</strong> ？？此偏移向量为负如何解释，或者必须保证 该向量是指向正确的分类超平面的（perpendicular，正交）。</li>\n<li>$(w,b) \\leftarrow (w,b)-(x,1)$<br>$(w,b) \\leftarrow (w,b)+(x,1)$<br><img src=\"http://p15i7i801.bkt.clouddn.com/b109e1d1e4424d180575bb0f74e75906.png\" alt=\"nnml-adpateside\"></li>\n<li><em>just like penalty function</em></li>\n</ul>\n<blockquote>\n<p>为什么学习有效: _非正式的收敛证明_</p>\n</blockquote>\n<ol>\n<li>Each time the perceptron makes a mistake, the current weight vector moves to decrease its squared distance from every weight vector in the “generously feasible” region.</li>\n<li><strong> The squared distance decreases by at least the squared length of the input vector. </strong>（向量的运算，几何解释）</li>\n<li>So after a finite number of mistakes, the weight vector must lie in the feasible region if the region exist.</li>\n</ol>\n<h2 id=\"感知器的缺陷\"><a href=\"#感知器的缺陷\" class=\"headerlink\" title=\"感知器的缺陷\"></a>感知器的缺陷</h2><blockquote>\n<p><strong>right feature</strong></p>\n</blockquote>\n<p>  Once the hand-coded features have been determined, there are very strong limitations on what a perceptron can learn.<br>  _The main question is : This type of table look-up won’t generalize, it need too many feature_<br>  _此篇习题未理解_</p>\n<blockquote>\n<p>Group Invariance Theorem—-Minsky and Papert</p>\n</blockquote>\n<p>can’t learn to do this if the transformations from a group{欧氏变换之内}，so it needs to use multiple feature unit to recognize transformations of informative sub-patterns.<br>So,more  important was all about how you learn <strong>feature detectors, that’s hidden units</strong>. After 20 years, We know,<br>we need multiple layer of adaptive, non-linear hidden units.</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"机器学习：不必为特定任务编写程序\"><a href=\"#机器学习：不必为特定任务编写程序\" class=\"headerlink\" title=\"机器学习：不必为特定任务编写程序\"></a>机器学习：不必为特定任务编写程序</h1><h2 id=\"场景落地\"><a href=\"#场景落地\" class=\"headerlink\" title=\"场景落地\"></a>场景落地</h2><ul>\n<li>Recognizing pattern     <ul>\n<li>Objects    </li>\n<li>facial    </li>\n<li>Spoken word</li>\n</ul>\n</li>\n<li>Recognizing anomalies    <ul>\n<li>credit card transaction    </li>\n<li>Sensor reality</li>\n</ul>\n</li>\n<li>Prediction    <ul>\n<li>stock price,</li>\n<li>exchange rate   </li>\n<li>movie like</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"几种简单模型\"><a href=\"#几种简单模型\" class=\"headerlink\" title=\"几种简单模型\"></a>几种简单模型</h2><ul>\n<li>Linear neurons<br>$ y=b+\\sum x_iw_i $</li>\n<li>Binary threshold neurons<script type=\"math/tex; mode=display\">\nz=\\Sigma x_i w_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt 0\\\\\n0&otherwise\n\\end{cases} \\quad or \\qquad\n\\theta=-b \\quad\nz=b+\\Sigma x_i y_i \\quad\ny=\\begin{cases}\n1&if\\;z \\gt \\theta\\\\\n0&otherwise\n\\end{cases}</script></li>\n<li>Rectified Linear Neurons 2<script type=\"math/tex; mode=display\">z=\\Sigma x_i w_i, y=\n\\begin{cases}\nz,    &if\\;z \\gt 0 \\\\\n0,    &\\mbox{otherwise}\n\\end{cases}</script></li>\n<li>sigmoid neurons<script type=\"math/tex; mode=display\">z=b+\\Sigma x_i w_i  \\quad y=\\frac {1} {1+e^{-z}}</script><img src=\"http://p15i7i801.bkt.clouddn.com/86069ad24cfd3541f4b33070b67fd749.png\" alt=\"nnml-sigmoid\"></li>\n<li>Stochastic binary neurons<script type=\"math/tex; mode=display\">z=b+\\Sigma x_i w_i  \\quad p(s=1)=\\frac {1} {1+e^{-z}}</script></li>\n</ul>\n<h2 id=\"学习的类型\"><a href=\"#学习的类型\" class=\"headerlink\" title=\"学习的类型\"></a>学习的类型</h2><ul>\n<li>supervised learning<ul>\n<li>Regression<ul>\n<li>model class: $y=f(x;w)$</li>\n<li>1/2这个系数在求导时被抵消<br><img src=\"http://p15i7i801.bkt.clouddn.com/93512d5be2aafca06ff3d058aff511b7.png\" alt=\"nnml-regression\"></li>\n</ul>\n</li>\n<li>classification</li>\n</ul>\n</li>\n<li><p>reinforced learning</p>\n<p>the output is an action or sequence of actions and the only supervisory is an occasional scalar reward._This is difficult_, The rewads are typically delayed so its had to know where we went wrong( or right)</p>\n</li>\n<li><p>unsupervised learning</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/ada9c5e7fe1e06957b0797be4022d724.png\" alt=\"nnml-unsupervised\"></p>\n<h2 id=\"神经网络的类型\"><a href=\"#神经网络的类型\" class=\"headerlink\" title=\"神经网络的类型\"></a>神经网络的类型</h2></li>\n<li><p>Feed-Forward neural network</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/3053d06760561e6169f20829df15a358.png\" alt=\"nnml-feedforward\"></p>\n<p>if there is more than one hidden layer, we call them “deep” neural networks.</p>\n</li>\n<li><p>Recurrent network</p>\n<p>It is hard to train but natural way for modeling sequence, recent algorithm can be able to do this, this also can be used to predict stock price.</p>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/83abfe6f730b3ae61d9099ff0d5ff300.png\" alt=\"nnml-recurrent\"><br><img src=\"http://p15i7i801.bkt.clouddn.com/3a688df70ed6bf1e46d62a03402e2a39.png\" alt=\"nnml-rnn\"></p>\n<blockquote>\n<p>20171218 LSTM 现在貌似更流行</p>\n</blockquote>\n</li>\n<li><p>Symmetrically connected networks</p>\n</li>\n</ul>\n<h2 id=\"感知器\"><a href=\"#感知器\" class=\"headerlink\" title=\"感知器\"></a>感知器</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/a1ba9669ccb220d05a8140bc8e0ca797.png\" alt=\"nnml-perceptron-arch\"></p>\n<blockquote>\n<p>how to learn biases using the same rule as we use for learning weights</p>\n</blockquote>\n<p>  <img src=\"http://p15i7i801.bkt.clouddn.com/d73047067e05f31a249dc755851602aa.png\" alt=\"nnml-handlebias\"></p>\n<blockquote>\n<p>学习的过程：if the output unit is correct, leave its weights alone, if the output unit incorrectly output</p>\n</blockquote>\n<p><img src=\"http://p15i7i801.bkt.clouddn.com/6b72ab6f556908380fcd6d5561d2e650.png\" alt=\"nnml-per-train\"></p>\n<ul>\n<li><strong>错误的输出了零，则Z需要新增以致其逐渐大于零而产生y=1（往正面），反之则需要减</strong> ？？此偏移向量为负如何解释，或者必须保证 该向量是指向正确的分类超平面的（perpendicular，正交）。</li>\n<li>$(w,b) \\leftarrow (w,b)-(x,1)$<br>$(w,b) \\leftarrow (w,b)+(x,1)$<br><img src=\"http://p15i7i801.bkt.clouddn.com/b109e1d1e4424d180575bb0f74e75906.png\" alt=\"nnml-adpateside\"></li>\n<li><em>just like penalty function</em></li>\n</ul>\n<blockquote>\n<p>为什么学习有效: _非正式的收敛证明_</p>\n</blockquote>\n<ol>\n<li>Each time the perceptron makes a mistake, the current weight vector moves to decrease its squared distance from every weight vector in the “generously feasible” region.</li>\n<li><strong> The squared distance decreases by at least the squared length of the input vector. </strong>（向量的运算，几何解释）</li>\n<li>So after a finite number of mistakes, the weight vector must lie in the feasible region if the region exist.</li>\n</ol>\n<h2 id=\"感知器的缺陷\"><a href=\"#感知器的缺陷\" class=\"headerlink\" title=\"感知器的缺陷\"></a>感知器的缺陷</h2><blockquote>\n<p><strong>right feature</strong></p>\n</blockquote>\n<p>  Once the hand-coded features have been determined, there are very strong limitations on what a perceptron can learn.<br>  _The main question is : This type of table look-up won’t generalize, it need too many feature_<br>  _此篇习题未理解_</p>\n<blockquote>\n<p>Group Invariance Theorem—-Minsky and Papert</p>\n</blockquote>\n<p>can’t learn to do this if the transformations from a group{欧氏变换之内}，so it needs to use multiple feature unit to recognize transformations of informative sub-patterns.<br>So,more  important was all about how you learn <strong>feature detectors, that’s hidden units</strong>. After 20 years, We know,<br>we need multiple layer of adaptive, non-linear hidden units.</p>\n"},{"title":"改善深层神经网络：超参数调试、正则化以及优化","date":"2017-09-15T09:45:29.000Z","mathjax":true,"_content":"\n\n#  第一周 深度学习的实用层面\n## 训练、验证、测试集\n* 在小数时代，比例一般为6：2：2，大数据时代可适当调整，验证及测试集为10000左右即可。\n* train set 最好和后两者有相当的分布，比如猫的问题，在网上爬的猫有许多精良的照片，而用户实际上传的可能比较糙。\n* test set是为了拿到测试的无偏估计\n* ML是一个高度迭代的过程，即使最牛的专家也是如此。\n\n## 偏差与方差\n* 偏差代表训练集的错误率有多高，方差代表dev set 和train set的错误率的差别\n\n## 正则化\nregularizer，规则化，通过先验的知识来最大化贝叶斯最大后验估计，本质上是贝叶斯最大后验估计的**似然函数**\n* L1\n* L2：Frobenius Norm，直观上Lambda越大，权重矩阵被设为零\n![nn_reg_12181821](http://p15i7i801.bkt.clouddn.com/2d81fcb8cfe1cb048929564aa91f9c4f.png)\n\n* dropout\n* 其它减少过拟合的方法\n    * Data augmentation\n    * early stop: 此种方法相对L2来说减少lambda的尝试计算量\n\n## 正则化输入（normalize)\n使用零均值，主要是用来提升速度，范围尺度类似即可，也不是一定要0-1。如下图，若不同特征相差尺度很大，则会形成如左不规则超球体，这样w的迭代次数会大大增加，对应的迭代步长也应设置偏小。\n\n## 梯度消失及爆炸\n由于层数太多，梯度指数级变化，这容易导致步长会非常小，训练时间会变得很长。传统激活函数都存在这一缺点，正向时爆炸，反向时消失，为防止上述问题，权重的初始值的选择有：\n* 若用RELU作为激活函数（[种类及特点](http://blog.csdn.net/mzpmzk/article/details/77418030)), $w$ 初始值可以使用 $np.sqrt(\\frac{2}{n^{[l-1]}+n^{[l]}})$\n![ng_vanishing_exploding_grad](http://p15i7i801.bkt.clouddn.com/1f3a7fe0374fc7a514f6c69809520867.png)\n# 第二周 优化算法\n* Min-batch梯度下降：避免需要处理所有数据才能进行下一步\n* 指数加权平均\n* 动量（Momentum）梯度下降:类似移动平均线减缓摆动\n$$ v_{dw} = \\beta v_{dw} + (1-\\beta)dw $$\n$$ v_{db}=\\beta v_{db} + (1-\\beta)db $$\n$$ w=w-\\alpha v_{dw} $$\n$$b=b-\\alpha v_{db}$$\n* RMSprop：让幅度大的参数变缓，让幅度小的参数变大\n\n$$ S_{dw} = \\beta S_{dw} + (1-\\beta){dw}^2 $$\n$$ S_{db}=\\beta S_{db} + (1-\\beta){db}^2$$\n$$ w=w-\\alpha \\frac{dw}{\\sqrt{S_{dw}}}$$\n$$ b=b-\\alpha \\frac{db}{\\sqrt{S_{db}}}$$\n* Adam(Adaptive Moment Estimation): 各算法适应于不同的神经网络结构，该算法结合Momentum and RMSprop，非常常用。\n  * 公式\n  $$ v_{dw} = \\beta v_{dw} + (1-\\beta_1)dw $$\n  $$ v_{db}=\\beta v_{db} + (1-\\beta_1)db$$\n  $$ S_{dw} = \\beta S_{dw} + (1-\\beta_2){dw}^2 $$\n  $$ S_{db}=\\beta S_{db} + (1-\\beta_2){db}^2$$\n  $$ w=w-\\alpha \\frac{v_{dw}}{\\sqrt{S_{dw}}}$$\n  $$ b=b-\\alpha \\frac{v_{db}}{\\sqrt{S_{db}}}$$\n  * 超参数一般值：\n      * $\\alpha$ : needs to be tune\n      * $\\beta_1$ : (first momentum):0.9\n      * $\\beta_2$ : (second momnetum:RMSprop):0.99\n* 学习率衰减：多个方法，如倒数、指数等等\n* 关于局部最优：在大量参数情况下可能会出现saddle问题，这样问题容易到达plateau状态，较难走出。这种可能性较小，通常使用Adam之类的优化方法解决\n\n# 第三周 超参数调度、Batch正则与程序框架\n\n## 超参数调整\n![ng_hp](http://p15i7i801.bkt.clouddn.com/149d38d31a5fa4e0576ff617c0408f08.png)\n* 首先是红色的 $\\alpha$，再者是黄色的学习率，第一、二动量及min-batch，最后再考虑调整层数及学习率延时 $\\varepsilon$。\n* 使用随机数、从粗到细\n* 使用合适的分布尺寸，而不是使用均匀分布，比如对于 $\\alpha$可以使用a log scala，举例而言：\n  ```python\n  r = -4*np.random.rand()\n  alpha = 10^r\n  ```\n\n## 实践经验：两个流派\n* 使用各领域的常见参数（**这意味需要对各个智能领域有的了解**）\n* 直觉很有效，**定期更新参数，比如几个月**\n* babysitting（pandan）: 长期照看，评估调整——（**不断的持久化模型吗？，一直迭代下去，还是每次都重新训练，模型可以以直训练下去吗？**）\n* train in paraller: Caviar(鱼子酱)\n\n## 网络内激活函数的normalizaion\n![ng_batchnorm](http://p15i7i801.bkt.clouddn.com/eaa10f3214ad4cbd03b074755a13a8e0.png)\n请注意以下三点：\n* 通过 $\\alpha、\\beta$两个超参数，可以控制 $Z^i$ 的范围，如右上图所示\n* 单层网络中我们通过normalizaion来加快梯度下降的速度，那么也可以通过控制隐藏层的输入值来控制\n* andrew的推荐: Normalize $z$ rather than $a$\n\n## 将Batch Norm拟合进神经网络\n![ng_addBatchNorm](http://p15i7i801.bkt.clouddn.com/b398a029678304574bdf4e67d9ecc406.png)\n* 偏执项b从此被标准化抵消，即式子可简化为 $z^l=w^l a^{l-1}$\n\n## Batch Norm奏效的原因\n* 加速梯度下降\n* It makes weights, later or deeper than your network, more robost to changes to weights in earlier layers of neural netowork. 举例而言，对于不带颜色的猫可能训练得很好，当带上颜色后，对于同样判断是否为猫的模型而言，输入数据的分布变化了，导致模型需要重新训练。这也称为**covariate shift**，为解决这个问题，可以使用Batch Norm。\n* 顺便的作用，如dropout，引入了噪声，相当于加入了regularization.\n\n## 测试模型时BatchNorm的计算\n* 需要估算平均值及标准差\n* 简单靠谱的处理方式是用指数加权平均去**估算**\n* 跑全量数据batchnorm也可以\n\n## softmax分类:LR的一般形式\n![ng_softmax](http://p15i7i801.bkt.clouddn.com/6d6b110e436fb245d51cbf1179d231d2.png)\n","source":"_posts/改善深层神经网络：超参数调试、正则化以及优化.md","raw":"---\ntitle: 改善深层神经网络：超参数调试、正则化以及优化\ndate: 2017-09-15 17:45:29\ntags:\n      - 深度学习\n      - Andrew NG\n      - 公开课\ncategories: AI梦\nmathjax: true\n---\n\n\n#  第一周 深度学习的实用层面\n## 训练、验证、测试集\n* 在小数时代，比例一般为6：2：2，大数据时代可适当调整，验证及测试集为10000左右即可。\n* train set 最好和后两者有相当的分布，比如猫的问题，在网上爬的猫有许多精良的照片，而用户实际上传的可能比较糙。\n* test set是为了拿到测试的无偏估计\n* ML是一个高度迭代的过程，即使最牛的专家也是如此。\n\n## 偏差与方差\n* 偏差代表训练集的错误率有多高，方差代表dev set 和train set的错误率的差别\n\n## 正则化\nregularizer，规则化，通过先验的知识来最大化贝叶斯最大后验估计，本质上是贝叶斯最大后验估计的**似然函数**\n* L1\n* L2：Frobenius Norm，直观上Lambda越大，权重矩阵被设为零\n![nn_reg_12181821](http://p15i7i801.bkt.clouddn.com/2d81fcb8cfe1cb048929564aa91f9c4f.png)\n\n* dropout\n* 其它减少过拟合的方法\n    * Data augmentation\n    * early stop: 此种方法相对L2来说减少lambda的尝试计算量\n\n## 正则化输入（normalize)\n使用零均值，主要是用来提升速度，范围尺度类似即可，也不是一定要0-1。如下图，若不同特征相差尺度很大，则会形成如左不规则超球体，这样w的迭代次数会大大增加，对应的迭代步长也应设置偏小。\n\n## 梯度消失及爆炸\n由于层数太多，梯度指数级变化，这容易导致步长会非常小，训练时间会变得很长。传统激活函数都存在这一缺点，正向时爆炸，反向时消失，为防止上述问题，权重的初始值的选择有：\n* 若用RELU作为激活函数（[种类及特点](http://blog.csdn.net/mzpmzk/article/details/77418030)), $w$ 初始值可以使用 $np.sqrt(\\frac{2}{n^{[l-1]}+n^{[l]}})$\n![ng_vanishing_exploding_grad](http://p15i7i801.bkt.clouddn.com/1f3a7fe0374fc7a514f6c69809520867.png)\n# 第二周 优化算法\n* Min-batch梯度下降：避免需要处理所有数据才能进行下一步\n* 指数加权平均\n* 动量（Momentum）梯度下降:类似移动平均线减缓摆动\n$$ v_{dw} = \\beta v_{dw} + (1-\\beta)dw $$\n$$ v_{db}=\\beta v_{db} + (1-\\beta)db $$\n$$ w=w-\\alpha v_{dw} $$\n$$b=b-\\alpha v_{db}$$\n* RMSprop：让幅度大的参数变缓，让幅度小的参数变大\n\n$$ S_{dw} = \\beta S_{dw} + (1-\\beta){dw}^2 $$\n$$ S_{db}=\\beta S_{db} + (1-\\beta){db}^2$$\n$$ w=w-\\alpha \\frac{dw}{\\sqrt{S_{dw}}}$$\n$$ b=b-\\alpha \\frac{db}{\\sqrt{S_{db}}}$$\n* Adam(Adaptive Moment Estimation): 各算法适应于不同的神经网络结构，该算法结合Momentum and RMSprop，非常常用。\n  * 公式\n  $$ v_{dw} = \\beta v_{dw} + (1-\\beta_1)dw $$\n  $$ v_{db}=\\beta v_{db} + (1-\\beta_1)db$$\n  $$ S_{dw} = \\beta S_{dw} + (1-\\beta_2){dw}^2 $$\n  $$ S_{db}=\\beta S_{db} + (1-\\beta_2){db}^2$$\n  $$ w=w-\\alpha \\frac{v_{dw}}{\\sqrt{S_{dw}}}$$\n  $$ b=b-\\alpha \\frac{v_{db}}{\\sqrt{S_{db}}}$$\n  * 超参数一般值：\n      * $\\alpha$ : needs to be tune\n      * $\\beta_1$ : (first momentum):0.9\n      * $\\beta_2$ : (second momnetum:RMSprop):0.99\n* 学习率衰减：多个方法，如倒数、指数等等\n* 关于局部最优：在大量参数情况下可能会出现saddle问题，这样问题容易到达plateau状态，较难走出。这种可能性较小，通常使用Adam之类的优化方法解决\n\n# 第三周 超参数调度、Batch正则与程序框架\n\n## 超参数调整\n![ng_hp](http://p15i7i801.bkt.clouddn.com/149d38d31a5fa4e0576ff617c0408f08.png)\n* 首先是红色的 $\\alpha$，再者是黄色的学习率，第一、二动量及min-batch，最后再考虑调整层数及学习率延时 $\\varepsilon$。\n* 使用随机数、从粗到细\n* 使用合适的分布尺寸，而不是使用均匀分布，比如对于 $\\alpha$可以使用a log scala，举例而言：\n  ```python\n  r = -4*np.random.rand()\n  alpha = 10^r\n  ```\n\n## 实践经验：两个流派\n* 使用各领域的常见参数（**这意味需要对各个智能领域有的了解**）\n* 直觉很有效，**定期更新参数，比如几个月**\n* babysitting（pandan）: 长期照看，评估调整——（**不断的持久化模型吗？，一直迭代下去，还是每次都重新训练，模型可以以直训练下去吗？**）\n* train in paraller: Caviar(鱼子酱)\n\n## 网络内激活函数的normalizaion\n![ng_batchnorm](http://p15i7i801.bkt.clouddn.com/eaa10f3214ad4cbd03b074755a13a8e0.png)\n请注意以下三点：\n* 通过 $\\alpha、\\beta$两个超参数，可以控制 $Z^i$ 的范围，如右上图所示\n* 单层网络中我们通过normalizaion来加快梯度下降的速度，那么也可以通过控制隐藏层的输入值来控制\n* andrew的推荐: Normalize $z$ rather than $a$\n\n## 将Batch Norm拟合进神经网络\n![ng_addBatchNorm](http://p15i7i801.bkt.clouddn.com/b398a029678304574bdf4e67d9ecc406.png)\n* 偏执项b从此被标准化抵消，即式子可简化为 $z^l=w^l a^{l-1}$\n\n## Batch Norm奏效的原因\n* 加速梯度下降\n* It makes weights, later or deeper than your network, more robost to changes to weights in earlier layers of neural netowork. 举例而言，对于不带颜色的猫可能训练得很好，当带上颜色后，对于同样判断是否为猫的模型而言，输入数据的分布变化了，导致模型需要重新训练。这也称为**covariate shift**，为解决这个问题，可以使用Batch Norm。\n* 顺便的作用，如dropout，引入了噪声，相当于加入了regularization.\n\n## 测试模型时BatchNorm的计算\n* 需要估算平均值及标准差\n* 简单靠谱的处理方式是用指数加权平均去**估算**\n* 跑全量数据batchnorm也可以\n\n## softmax分类:LR的一般形式\n![ng_softmax](http://p15i7i801.bkt.clouddn.com/6d6b110e436fb245d51cbf1179d231d2.png)\n","slug":"改善深层神经网络：超参数调试、正则化以及优化","published":1,"updated":"2017-12-21T04:16:23.274Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbowgdpj000cj4n75hv7x0oe","content":"<h1 id=\"第一周-深度学习的实用层面\"><a href=\"#第一周-深度学习的实用层面\" class=\"headerlink\" title=\"第一周 深度学习的实用层面\"></a>第一周 深度学习的实用层面</h1><h2 id=\"训练、验证、测试集\"><a href=\"#训练、验证、测试集\" class=\"headerlink\" title=\"训练、验证、测试集\"></a>训练、验证、测试集</h2><ul>\n<li>在小数时代，比例一般为6：2：2，大数据时代可适当调整，验证及测试集为10000左右即可。</li>\n<li>train set 最好和后两者有相当的分布，比如猫的问题，在网上爬的猫有许多精良的照片，而用户实际上传的可能比较糙。</li>\n<li>test set是为了拿到测试的无偏估计</li>\n<li>ML是一个高度迭代的过程，即使最牛的专家也是如此。</li>\n</ul>\n<h2 id=\"偏差与方差\"><a href=\"#偏差与方差\" class=\"headerlink\" title=\"偏差与方差\"></a>偏差与方差</h2><ul>\n<li>偏差代表训练集的错误率有多高，方差代表dev set 和train set的错误率的差别</li>\n</ul>\n<h2 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h2><p>regularizer，规则化，通过先验的知识来最大化贝叶斯最大后验估计，本质上是贝叶斯最大后验估计的<strong>似然函数</strong></p>\n<ul>\n<li>L1</li>\n<li><p>L2：Frobenius Norm，直观上Lambda越大，权重矩阵被设为零<br><img src=\"http://p15i7i801.bkt.clouddn.com/2d81fcb8cfe1cb048929564aa91f9c4f.png\" alt=\"nn_reg_12181821\"></p>\n</li>\n<li><p>dropout</p>\n</li>\n<li>其它减少过拟合的方法<ul>\n<li>Data augmentation</li>\n<li>early stop: 此种方法相对L2来说减少lambda的尝试计算量</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"正则化输入（normalize\"><a href=\"#正则化输入（normalize\" class=\"headerlink\" title=\"正则化输入（normalize)\"></a>正则化输入（normalize)</h2><p>使用零均值，主要是用来提升速度，范围尺度类似即可，也不是一定要0-1。如下图，若不同特征相差尺度很大，则会形成如左不规则超球体，这样w的迭代次数会大大增加，对应的迭代步长也应设置偏小。</p>\n<h2 id=\"梯度消失及爆炸\"><a href=\"#梯度消失及爆炸\" class=\"headerlink\" title=\"梯度消失及爆炸\"></a>梯度消失及爆炸</h2><p>由于层数太多，梯度指数级变化，这容易导致步长会非常小，训练时间会变得很长。传统激活函数都存在这一缺点，正向时爆炸，反向时消失，为防止上述问题，权重的初始值的选择有：</p>\n<ul>\n<li>若用RELU作为激活函数（<a href=\"http://blog.csdn.net/mzpmzk/article/details/77418030\" target=\"_blank\" rel=\"noopener\">种类及特点</a>), $w$ 初始值可以使用 $np.sqrt(\\frac{2}{n^{[l-1]}+n^{[l]}})$<br><img src=\"http://p15i7i801.bkt.clouddn.com/1f3a7fe0374fc7a514f6c69809520867.png\" alt=\"ng_vanishing_exploding_grad\"><h1 id=\"第二周-优化算法\"><a href=\"#第二周-优化算法\" class=\"headerlink\" title=\"第二周 优化算法\"></a>第二周 优化算法</h1></li>\n<li>Min-batch梯度下降：避免需要处理所有数据才能进行下一步</li>\n<li>指数加权平均</li>\n<li>动量（Momentum）梯度下降:类似移动平均线减缓摆动<script type=\"math/tex; mode=display\">v_{dw} = \\beta v_{dw} + (1-\\beta)dw</script><script type=\"math/tex; mode=display\">v_{db}=\\beta v_{db} + (1-\\beta)db</script><script type=\"math/tex; mode=display\">w=w-\\alpha v_{dw}</script><script type=\"math/tex; mode=display\">b=b-\\alpha v_{db}</script></li>\n<li>RMSprop：让幅度大的参数变缓，让幅度小的参数变大</li>\n</ul>\n<script type=\"math/tex; mode=display\">S_{dw} = \\beta S_{dw} + (1-\\beta){dw}^2</script><script type=\"math/tex; mode=display\">S_{db}=\\beta S_{db} + (1-\\beta){db}^2</script><script type=\"math/tex; mode=display\">w=w-\\alpha \\frac{dw}{\\sqrt{S_{dw}}}</script><script type=\"math/tex; mode=display\">b=b-\\alpha \\frac{db}{\\sqrt{S_{db}}}</script><ul>\n<li>Adam(Adaptive Moment Estimation): 各算法适应于不同的神经网络结构，该算法结合Momentum and RMSprop，非常常用。<ul>\n<li>公式<script type=\"math/tex; mode=display\">v_{dw} = \\beta v_{dw} + (1-\\beta_1)dw</script><script type=\"math/tex; mode=display\">v_{db}=\\beta v_{db} + (1-\\beta_1)db</script><script type=\"math/tex; mode=display\">S_{dw} = \\beta S_{dw} + (1-\\beta_2){dw}^2</script><script type=\"math/tex; mode=display\">S_{db}=\\beta S_{db} + (1-\\beta_2){db}^2</script><script type=\"math/tex; mode=display\">w=w-\\alpha \\frac{v_{dw}}{\\sqrt{S_{dw}}}</script><script type=\"math/tex; mode=display\">b=b-\\alpha \\frac{v_{db}}{\\sqrt{S_{db}}}</script></li>\n<li>超参数一般值：<ul>\n<li>$\\alpha$ : needs to be tune</li>\n<li>$\\beta_1$ : (first momentum):0.9</li>\n<li>$\\beta_2$ : (second momnetum:RMSprop):0.99</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>学习率衰减：多个方法，如倒数、指数等等</li>\n<li>关于局部最优：在大量参数情况下可能会出现saddle问题，这样问题容易到达plateau状态，较难走出。这种可能性较小，通常使用Adam之类的优化方法解决</li>\n</ul>\n<h1 id=\"第三周-超参数调度、Batch正则与程序框架\"><a href=\"#第三周-超参数调度、Batch正则与程序框架\" class=\"headerlink\" title=\"第三周 超参数调度、Batch正则与程序框架\"></a>第三周 超参数调度、Batch正则与程序框架</h1><h2 id=\"超参数调整\"><a href=\"#超参数调整\" class=\"headerlink\" title=\"超参数调整\"></a>超参数调整</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/149d38d31a5fa4e0576ff617c0408f08.png\" alt=\"ng_hp\"></p>\n<ul>\n<li>首先是红色的 $\\alpha$，再者是黄色的学习率，第一、二动量及min-batch，最后再考虑调整层数及学习率延时 $\\varepsilon$。</li>\n<li>使用随机数、从粗到细</li>\n<li>使用合适的分布尺寸，而不是使用均匀分布，比如对于 $\\alpha$可以使用a log scala，举例而言：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">r = <span class=\"number\">-4</span>*np.random.rand()</span><br><span class=\"line\">alpha = <span class=\"number\">10</span>^r</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"实践经验：两个流派\"><a href=\"#实践经验：两个流派\" class=\"headerlink\" title=\"实践经验：两个流派\"></a>实践经验：两个流派</h2><ul>\n<li>使用各领域的常见参数（<strong>这意味需要对各个智能领域有的了解</strong>）</li>\n<li>直觉很有效，<strong>定期更新参数，比如几个月</strong></li>\n<li>babysitting（pandan）: 长期照看，评估调整——（<strong>不断的持久化模型吗？，一直迭代下去，还是每次都重新训练，模型可以以直训练下去吗？</strong>）</li>\n<li>train in paraller: Caviar(鱼子酱)</li>\n</ul>\n<h2 id=\"网络内激活函数的normalizaion\"><a href=\"#网络内激活函数的normalizaion\" class=\"headerlink\" title=\"网络内激活函数的normalizaion\"></a>网络内激活函数的normalizaion</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/eaa10f3214ad4cbd03b074755a13a8e0.png\" alt=\"ng_batchnorm\"><br>请注意以下三点：</p>\n<ul>\n<li>通过 $\\alpha、\\beta$两个超参数，可以控制 $Z^i$ 的范围，如右上图所示</li>\n<li>单层网络中我们通过normalizaion来加快梯度下降的速度，那么也可以通过控制隐藏层的输入值来控制</li>\n<li>andrew的推荐: Normalize $z$ rather than $a$</li>\n</ul>\n<h2 id=\"将Batch-Norm拟合进神经网络\"><a href=\"#将Batch-Norm拟合进神经网络\" class=\"headerlink\" title=\"将Batch Norm拟合进神经网络\"></a>将Batch Norm拟合进神经网络</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/b398a029678304574bdf4e67d9ecc406.png\" alt=\"ng_addBatchNorm\"></p>\n<ul>\n<li>偏执项b从此被标准化抵消，即式子可简化为 $z^l=w^l a^{l-1}$</li>\n</ul>\n<h2 id=\"Batch-Norm奏效的原因\"><a href=\"#Batch-Norm奏效的原因\" class=\"headerlink\" title=\"Batch Norm奏效的原因\"></a>Batch Norm奏效的原因</h2><ul>\n<li>加速梯度下降</li>\n<li>It makes weights, later or deeper than your network, more robost to changes to weights in earlier layers of neural netowork. 举例而言，对于不带颜色的猫可能训练得很好，当带上颜色后，对于同样判断是否为猫的模型而言，输入数据的分布变化了，导致模型需要重新训练。这也称为<strong>covariate shift</strong>，为解决这个问题，可以使用Batch Norm。</li>\n<li>顺便的作用，如dropout，引入了噪声，相当于加入了regularization.</li>\n</ul>\n<h2 id=\"测试模型时BatchNorm的计算\"><a href=\"#测试模型时BatchNorm的计算\" class=\"headerlink\" title=\"测试模型时BatchNorm的计算\"></a>测试模型时BatchNorm的计算</h2><ul>\n<li>需要估算平均值及标准差</li>\n<li>简单靠谱的处理方式是用指数加权平均去<strong>估算</strong></li>\n<li>跑全量数据batchnorm也可以</li>\n</ul>\n<h2 id=\"softmax分类-LR的一般形式\"><a href=\"#softmax分类-LR的一般形式\" class=\"headerlink\" title=\"softmax分类:LR的一般形式\"></a>softmax分类:LR的一般形式</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/6d6b110e436fb245d51cbf1179d231d2.png\" alt=\"ng_softmax\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"第一周-深度学习的实用层面\"><a href=\"#第一周-深度学习的实用层面\" class=\"headerlink\" title=\"第一周 深度学习的实用层面\"></a>第一周 深度学习的实用层面</h1><h2 id=\"训练、验证、测试集\"><a href=\"#训练、验证、测试集\" class=\"headerlink\" title=\"训练、验证、测试集\"></a>训练、验证、测试集</h2><ul>\n<li>在小数时代，比例一般为6：2：2，大数据时代可适当调整，验证及测试集为10000左右即可。</li>\n<li>train set 最好和后两者有相当的分布，比如猫的问题，在网上爬的猫有许多精良的照片，而用户实际上传的可能比较糙。</li>\n<li>test set是为了拿到测试的无偏估计</li>\n<li>ML是一个高度迭代的过程，即使最牛的专家也是如此。</li>\n</ul>\n<h2 id=\"偏差与方差\"><a href=\"#偏差与方差\" class=\"headerlink\" title=\"偏差与方差\"></a>偏差与方差</h2><ul>\n<li>偏差代表训练集的错误率有多高，方差代表dev set 和train set的错误率的差别</li>\n</ul>\n<h2 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h2><p>regularizer，规则化，通过先验的知识来最大化贝叶斯最大后验估计，本质上是贝叶斯最大后验估计的<strong>似然函数</strong></p>\n<ul>\n<li>L1</li>\n<li><p>L2：Frobenius Norm，直观上Lambda越大，权重矩阵被设为零<br><img src=\"http://p15i7i801.bkt.clouddn.com/2d81fcb8cfe1cb048929564aa91f9c4f.png\" alt=\"nn_reg_12181821\"></p>\n</li>\n<li><p>dropout</p>\n</li>\n<li>其它减少过拟合的方法<ul>\n<li>Data augmentation</li>\n<li>early stop: 此种方法相对L2来说减少lambda的尝试计算量</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"正则化输入（normalize\"><a href=\"#正则化输入（normalize\" class=\"headerlink\" title=\"正则化输入（normalize)\"></a>正则化输入（normalize)</h2><p>使用零均值，主要是用来提升速度，范围尺度类似即可，也不是一定要0-1。如下图，若不同特征相差尺度很大，则会形成如左不规则超球体，这样w的迭代次数会大大增加，对应的迭代步长也应设置偏小。</p>\n<h2 id=\"梯度消失及爆炸\"><a href=\"#梯度消失及爆炸\" class=\"headerlink\" title=\"梯度消失及爆炸\"></a>梯度消失及爆炸</h2><p>由于层数太多，梯度指数级变化，这容易导致步长会非常小，训练时间会变得很长。传统激活函数都存在这一缺点，正向时爆炸，反向时消失，为防止上述问题，权重的初始值的选择有：</p>\n<ul>\n<li>若用RELU作为激活函数（<a href=\"http://blog.csdn.net/mzpmzk/article/details/77418030\" target=\"_blank\" rel=\"noopener\">种类及特点</a>), $w$ 初始值可以使用 $np.sqrt(\\frac{2}{n^{[l-1]}+n^{[l]}})$<br><img src=\"http://p15i7i801.bkt.clouddn.com/1f3a7fe0374fc7a514f6c69809520867.png\" alt=\"ng_vanishing_exploding_grad\"><h1 id=\"第二周-优化算法\"><a href=\"#第二周-优化算法\" class=\"headerlink\" title=\"第二周 优化算法\"></a>第二周 优化算法</h1></li>\n<li>Min-batch梯度下降：避免需要处理所有数据才能进行下一步</li>\n<li>指数加权平均</li>\n<li>动量（Momentum）梯度下降:类似移动平均线减缓摆动<script type=\"math/tex; mode=display\">v_{dw} = \\beta v_{dw} + (1-\\beta)dw</script><script type=\"math/tex; mode=display\">v_{db}=\\beta v_{db} + (1-\\beta)db</script><script type=\"math/tex; mode=display\">w=w-\\alpha v_{dw}</script><script type=\"math/tex; mode=display\">b=b-\\alpha v_{db}</script></li>\n<li>RMSprop：让幅度大的参数变缓，让幅度小的参数变大</li>\n</ul>\n<script type=\"math/tex; mode=display\">S_{dw} = \\beta S_{dw} + (1-\\beta){dw}^2</script><script type=\"math/tex; mode=display\">S_{db}=\\beta S_{db} + (1-\\beta){db}^2</script><script type=\"math/tex; mode=display\">w=w-\\alpha \\frac{dw}{\\sqrt{S_{dw}}}</script><script type=\"math/tex; mode=display\">b=b-\\alpha \\frac{db}{\\sqrt{S_{db}}}</script><ul>\n<li>Adam(Adaptive Moment Estimation): 各算法适应于不同的神经网络结构，该算法结合Momentum and RMSprop，非常常用。<ul>\n<li>公式<script type=\"math/tex; mode=display\">v_{dw} = \\beta v_{dw} + (1-\\beta_1)dw</script><script type=\"math/tex; mode=display\">v_{db}=\\beta v_{db} + (1-\\beta_1)db</script><script type=\"math/tex; mode=display\">S_{dw} = \\beta S_{dw} + (1-\\beta_2){dw}^2</script><script type=\"math/tex; mode=display\">S_{db}=\\beta S_{db} + (1-\\beta_2){db}^2</script><script type=\"math/tex; mode=display\">w=w-\\alpha \\frac{v_{dw}}{\\sqrt{S_{dw}}}</script><script type=\"math/tex; mode=display\">b=b-\\alpha \\frac{v_{db}}{\\sqrt{S_{db}}}</script></li>\n<li>超参数一般值：<ul>\n<li>$\\alpha$ : needs to be tune</li>\n<li>$\\beta_1$ : (first momentum):0.9</li>\n<li>$\\beta_2$ : (second momnetum:RMSprop):0.99</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>学习率衰减：多个方法，如倒数、指数等等</li>\n<li>关于局部最优：在大量参数情况下可能会出现saddle问题，这样问题容易到达plateau状态，较难走出。这种可能性较小，通常使用Adam之类的优化方法解决</li>\n</ul>\n<h1 id=\"第三周-超参数调度、Batch正则与程序框架\"><a href=\"#第三周-超参数调度、Batch正则与程序框架\" class=\"headerlink\" title=\"第三周 超参数调度、Batch正则与程序框架\"></a>第三周 超参数调度、Batch正则与程序框架</h1><h2 id=\"超参数调整\"><a href=\"#超参数调整\" class=\"headerlink\" title=\"超参数调整\"></a>超参数调整</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/149d38d31a5fa4e0576ff617c0408f08.png\" alt=\"ng_hp\"></p>\n<ul>\n<li>首先是红色的 $\\alpha$，再者是黄色的学习率，第一、二动量及min-batch，最后再考虑调整层数及学习率延时 $\\varepsilon$。</li>\n<li>使用随机数、从粗到细</li>\n<li>使用合适的分布尺寸，而不是使用均匀分布，比如对于 $\\alpha$可以使用a log scala，举例而言：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">r = <span class=\"number\">-4</span>*np.random.rand()</span><br><span class=\"line\">alpha = <span class=\"number\">10</span>^r</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"实践经验：两个流派\"><a href=\"#实践经验：两个流派\" class=\"headerlink\" title=\"实践经验：两个流派\"></a>实践经验：两个流派</h2><ul>\n<li>使用各领域的常见参数（<strong>这意味需要对各个智能领域有的了解</strong>）</li>\n<li>直觉很有效，<strong>定期更新参数，比如几个月</strong></li>\n<li>babysitting（pandan）: 长期照看，评估调整——（<strong>不断的持久化模型吗？，一直迭代下去，还是每次都重新训练，模型可以以直训练下去吗？</strong>）</li>\n<li>train in paraller: Caviar(鱼子酱)</li>\n</ul>\n<h2 id=\"网络内激活函数的normalizaion\"><a href=\"#网络内激活函数的normalizaion\" class=\"headerlink\" title=\"网络内激活函数的normalizaion\"></a>网络内激活函数的normalizaion</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/eaa10f3214ad4cbd03b074755a13a8e0.png\" alt=\"ng_batchnorm\"><br>请注意以下三点：</p>\n<ul>\n<li>通过 $\\alpha、\\beta$两个超参数，可以控制 $Z^i$ 的范围，如右上图所示</li>\n<li>单层网络中我们通过normalizaion来加快梯度下降的速度，那么也可以通过控制隐藏层的输入值来控制</li>\n<li>andrew的推荐: Normalize $z$ rather than $a$</li>\n</ul>\n<h2 id=\"将Batch-Norm拟合进神经网络\"><a href=\"#将Batch-Norm拟合进神经网络\" class=\"headerlink\" title=\"将Batch Norm拟合进神经网络\"></a>将Batch Norm拟合进神经网络</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/b398a029678304574bdf4e67d9ecc406.png\" alt=\"ng_addBatchNorm\"></p>\n<ul>\n<li>偏执项b从此被标准化抵消，即式子可简化为 $z^l=w^l a^{l-1}$</li>\n</ul>\n<h2 id=\"Batch-Norm奏效的原因\"><a href=\"#Batch-Norm奏效的原因\" class=\"headerlink\" title=\"Batch Norm奏效的原因\"></a>Batch Norm奏效的原因</h2><ul>\n<li>加速梯度下降</li>\n<li>It makes weights, later or deeper than your network, more robost to changes to weights in earlier layers of neural netowork. 举例而言，对于不带颜色的猫可能训练得很好，当带上颜色后，对于同样判断是否为猫的模型而言，输入数据的分布变化了，导致模型需要重新训练。这也称为<strong>covariate shift</strong>，为解决这个问题，可以使用Batch Norm。</li>\n<li>顺便的作用，如dropout，引入了噪声，相当于加入了regularization.</li>\n</ul>\n<h2 id=\"测试模型时BatchNorm的计算\"><a href=\"#测试模型时BatchNorm的计算\" class=\"headerlink\" title=\"测试模型时BatchNorm的计算\"></a>测试模型时BatchNorm的计算</h2><ul>\n<li>需要估算平均值及标准差</li>\n<li>简单靠谱的处理方式是用指数加权平均去<strong>估算</strong></li>\n<li>跑全量数据batchnorm也可以</li>\n</ul>\n<h2 id=\"softmax分类-LR的一般形式\"><a href=\"#softmax分类-LR的一般形式\" class=\"headerlink\" title=\"softmax分类:LR的一般形式\"></a>softmax分类:LR的一般形式</h2><p><img src=\"http://p15i7i801.bkt.clouddn.com/6d6b110e436fb245d51cbf1179d231d2.png\" alt=\"ng_softmax\"></p>\n"},{"title":"draft","date":"2017-12-27T07:10:09.000Z","mathjax":true,"_content":"\n# 增强学习\n* 四个要素 $(A,S,R,P)$\n\n* 策略 $\\pi$ 决定选择哪个行动 $a$, 即：\n  $$\n  \\begin{cases}\n  \\pi(s) \\implies a \\\\\n  \\pi(a|s)\n  \\end{cases}\n  $$\n* 根据要素抽象行为成序列，通常要满足[马尔可夫条件](http://www.cnblogs.com/jinxulin/p/3517377.html),这些序列构成训练模型样本\n  $$ \\{s_1,a_1,r_1,s_2,a_2,r_2,...,s_t,a_t,r_t\\} $$\n* 定义目标 $G_t$，作为长期回报，其值用收益折现来表达，有 $G_t=\\sum_{k=0}$, 因此可推导目标期望：\n$$\nV_\\pi(s)=E_\\pi(G_t|S_t=s)  \\\\\nQ_\\pi(s,a)=E_\\pi(G_t|S_t=s,A_t=a)\n$$\n* [Bellman等式](https://en.wikipedia.org/wiki/Bellman_equation)，也称动态规划等式，在RL里非常重要，其在此场景下用来表示长期回报——值函数：\n$$\n\\mbox{ 在某策略下：}V_\\pi(s)=\\sum\\pi(a|s)E[R_{t+1}+\\gamma V(s_{t+1})|S_t=s]  \\\\\n V_*(s)=E[R_{t+1}+\\gamma max_\\pi V(s_{t+1})|S_t=s]  \\\\\n Q_*(s,a)=E[R_{t+1}+\\gamma max_{a^\\prime} Q(s_{t+1},a^\\prime)|S_t=s,A_t=a]\n\n$$\n* 有了值函数 $V_\\pi$的概念后,则问题抽象为在马尔可夫决策过程中，任意初始条件 $s$下，求能够最大化值函数的策略 $\\pi$，其可表达为 $\\pi^*=arg_\\pi \\{maxV_\\pi(s)\\}$，具体求解过程可参看[计算实例](http://www.cnblogs.com/jinxulin/p/3517377.html)\n","source":"_posts/draft.md","raw":"---\ntitle: draft\ndate: 2017-12-27 15:10:09\ntags:\n  - draft\nmathjax: true\n---\n\n# 增强学习\n* 四个要素 $(A,S,R,P)$\n\n* 策略 $\\pi$ 决定选择哪个行动 $a$, 即：\n  $$\n  \\begin{cases}\n  \\pi(s) \\implies a \\\\\n  \\pi(a|s)\n  \\end{cases}\n  $$\n* 根据要素抽象行为成序列，通常要满足[马尔可夫条件](http://www.cnblogs.com/jinxulin/p/3517377.html),这些序列构成训练模型样本\n  $$ \\{s_1,a_1,r_1,s_2,a_2,r_2,...,s_t,a_t,r_t\\} $$\n* 定义目标 $G_t$，作为长期回报，其值用收益折现来表达，有 $G_t=\\sum_{k=0}$, 因此可推导目标期望：\n$$\nV_\\pi(s)=E_\\pi(G_t|S_t=s)  \\\\\nQ_\\pi(s,a)=E_\\pi(G_t|S_t=s,A_t=a)\n$$\n* [Bellman等式](https://en.wikipedia.org/wiki/Bellman_equation)，也称动态规划等式，在RL里非常重要，其在此场景下用来表示长期回报——值函数：\n$$\n\\mbox{ 在某策略下：}V_\\pi(s)=\\sum\\pi(a|s)E[R_{t+1}+\\gamma V(s_{t+1})|S_t=s]  \\\\\n V_*(s)=E[R_{t+1}+\\gamma max_\\pi V(s_{t+1})|S_t=s]  \\\\\n Q_*(s,a)=E[R_{t+1}+\\gamma max_{a^\\prime} Q(s_{t+1},a^\\prime)|S_t=s,A_t=a]\n\n$$\n* 有了值函数 $V_\\pi$的概念后,则问题抽象为在马尔可夫决策过程中，任意初始条件 $s$下，求能够最大化值函数的策略 $\\pi$，其可表达为 $\\pi^*=arg_\\pi \\{maxV_\\pi(s)\\}$，具体求解过程可参看[计算实例](http://www.cnblogs.com/jinxulin/p/3517377.html)\n","slug":"draft","published":1,"updated":"2017-12-27T10:03:56.224Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbowgdpk000dj4n7hhverpvm","content":"<h1 id=\"增强学习\"><a href=\"#增强学习\" class=\"headerlink\" title=\"增强学习\"></a>增强学习</h1><ul>\n<li><p>四个要素 $(A,S,R,P)$</p>\n</li>\n<li><p>策略 $\\pi$ 决定选择哪个行动 $a$, 即：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\n\\pi(s) \\implies a \\\\\n\\pi(a|s)\n\\end{cases}</script></li>\n<li>根据要素抽象行为成序列，通常要满足<a href=\"http://www.cnblogs.com/jinxulin/p/3517377.html\" target=\"_blank\" rel=\"noopener\">马尔可夫条件</a>,这些序列构成训练模型样本<script type=\"math/tex; mode=display\">\\{s_1,a_1,r_1,s_2,a_2,r_2,...,s_t,a_t,r_t\\}</script></li>\n<li>定义目标 $G_t$，作为长期回报，其值用收益折现来表达，有 $G_t=\\sum_{k=0}$, 因此可推导目标期望：<script type=\"math/tex; mode=display\">\nV_\\pi(s)=E_\\pi(G_t|S_t=s)  \\\\\nQ_\\pi(s,a)=E_\\pi(G_t|S_t=s,A_t=a)</script></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Bellman_equation\" target=\"_blank\" rel=\"noopener\">Bellman等式</a>，也称动态规划等式，在RL里非常重要，其在此场景下用来表示长期回报——值函数：<br>$$<br>\\mbox{ 在某策略下：}V_\\pi(s)=\\sum\\pi(a|s)E[R_{t+1}+\\gamma V(s_{t+1})|S_t=s]  \\\\<br>V_<em>(s)=E[R_{t+1}+\\gamma max_\\pi V(s_{t+1})|S_t=s]  \\\\<br>Q_</em>(s,a)=E[R_{t+1}+\\gamma max_{a^\\prime} Q(s_{t+1},a^\\prime)|S_t=s,A_t=a]</li>\n</ul>\n<p>$$</p>\n<ul>\n<li>有了值函数 $V_\\pi$的概念后,则问题抽象为在马尔可夫决策过程中，任意初始条件 $s$下，求能够最大化值函数的策略 $\\pi$，其可表达为 $\\pi^*=arg_\\pi \\{maxV_\\pi(s)\\}$，具体求解过程可参看<a href=\"http://www.cnblogs.com/jinxulin/p/3517377.html\" target=\"_blank\" rel=\"noopener\">计算实例</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"增强学习\"><a href=\"#增强学习\" class=\"headerlink\" title=\"增强学习\"></a>增强学习</h1><ul>\n<li><p>四个要素 $(A,S,R,P)$</p>\n</li>\n<li><p>策略 $\\pi$ 决定选择哪个行动 $a$, 即：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\n\\pi(s) \\implies a \\\\\n\\pi(a|s)\n\\end{cases}</script></li>\n<li>根据要素抽象行为成序列，通常要满足<a href=\"http://www.cnblogs.com/jinxulin/p/3517377.html\" target=\"_blank\" rel=\"noopener\">马尔可夫条件</a>,这些序列构成训练模型样本<script type=\"math/tex; mode=display\">\\{s_1,a_1,r_1,s_2,a_2,r_2,...,s_t,a_t,r_t\\}</script></li>\n<li>定义目标 $G_t$，作为长期回报，其值用收益折现来表达，有 $G_t=\\sum_{k=0}$, 因此可推导目标期望：<script type=\"math/tex; mode=display\">\nV_\\pi(s)=E_\\pi(G_t|S_t=s)  \\\\\nQ_\\pi(s,a)=E_\\pi(G_t|S_t=s,A_t=a)</script></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Bellman_equation\" target=\"_blank\" rel=\"noopener\">Bellman等式</a>，也称动态规划等式，在RL里非常重要，其在此场景下用来表示长期回报——值函数：<br>$$<br>\\mbox{ 在某策略下：}V_\\pi(s)=\\sum\\pi(a|s)E[R_{t+1}+\\gamma V(s_{t+1})|S_t=s]  \\\\<br>V_<em>(s)=E[R_{t+1}+\\gamma max_\\pi V(s_{t+1})|S_t=s]  \\\\<br>Q_</em>(s,a)=E[R_{t+1}+\\gamma max_{a^\\prime} Q(s_{t+1},a^\\prime)|S_t=s,A_t=a]</li>\n</ul>\n<p>$$</p>\n<ul>\n<li>有了值函数 $V_\\pi$的概念后,则问题抽象为在马尔可夫决策过程中，任意初始条件 $s$下，求能够最大化值函数的策略 $\\pi$，其可表达为 $\\pi^*=arg_\\pi \\{maxV_\\pi(s)\\}$，具体求解过程可参看<a href=\"http://www.cnblogs.com/jinxulin/p/3517377.html\" target=\"_blank\" rel=\"noopener\">计算实例</a></li>\n</ul>\n"},{"title":"神经网络与深度学习","date":"2017-08-30T09:20:41.000Z","mathjax":true,"_content":"\n\n### 第一、二周：概论与基础\n> sigmoid在逻辑回归里为什么没用被ReLU替代\n\n  * sigmoid在感知器结果输出里可以将任意输入连接的映射到 $[0,1]$ 之间，且结果保持连续\n  * ReLu作为深度学习引入的激活函数，特性在于稀疏的激活性、单侧抑制与相对宽的兴奋边界\n\n> LR中损失函数不用平方误差，为什么它是非凸的。\n\n* 因为加入了Sigmoid函数，$a=\\sigma (z)$ 它的特点：\n$$ da/dz = z(1-z), dL/dz= a - y \\quad  if \\quad L=-( y log a + (1-y)log(1-a) ) $$\n\n> 矩阵运算Numpy的高效\n\n* 避免矩阵运算使用For循环，100m时差300倍效率，因为 numpy使用到了CPU\\GPU的SIMD指令，运行速度更快。同时，对于numpy有内置的函数也一样，尽量使用，如log,max  etc..\n* 在python中，z= np.dot(w.T, x) + b , b会自动扩展成一个n维向量，这在python称为broadcast\n![ng_broadcast](http://p15i7i801.bkt.clouddn.com/849e2ed8c823392209bd02a6e7397727.png)\n* 在使用的过程中，reshape是o(1)操作，多多益善，尽量不使用rank为1的数组。\n\n### 第三周 浅层神经网络\n* 二级神经网络，即只有一个隐藏层的神经网络。\n*  输出层通常使用sigmoid函数，而隐藏层不用，一般至少使用tanh来代替，因为它的输出介于-1与1之间，这样均值为0，便于下一层的计算。考虑到梯度下降，sigmoid与tanh在最大小值处导数变化小，reLU及leaky reLU更常用。当然这一切根据实际情况调整。\n*  隐藏层不应使用线性激活函数，若用则相当于没有隐藏层，输出层若要出回归值，则可使用。\n*  随机w,并乘以较小的系数，一般让随机参数比较小。\n\n### 第四周 深层神经网络\n####  hyper parameter——控制参数的参数\n  * 层数\n  * 迭代次数\n  * 下降速率\n  * 激活函数的选择\n  * batch size\n####  parameter\n  * 权重 $w$\n  * 偏移量 $b$\n","source":"_posts/神经网络与深度学习.md","raw":"---\ntitle: 神经网络与深度学习\ndate: 2017-08-30 17:20:41\ntags:\n      - 深度学习\n      - Andrew NG\n      - 公开课\ncategories: AI梦\nmathjax: true\n---\n\n\n### 第一、二周：概论与基础\n> sigmoid在逻辑回归里为什么没用被ReLU替代\n\n  * sigmoid在感知器结果输出里可以将任意输入连接的映射到 $[0,1]$ 之间，且结果保持连续\n  * ReLu作为深度学习引入的激活函数，特性在于稀疏的激活性、单侧抑制与相对宽的兴奋边界\n\n> LR中损失函数不用平方误差，为什么它是非凸的。\n\n* 因为加入了Sigmoid函数，$a=\\sigma (z)$ 它的特点：\n$$ da/dz = z(1-z), dL/dz= a - y \\quad  if \\quad L=-( y log a + (1-y)log(1-a) ) $$\n\n> 矩阵运算Numpy的高效\n\n* 避免矩阵运算使用For循环，100m时差300倍效率，因为 numpy使用到了CPU\\GPU的SIMD指令，运行速度更快。同时，对于numpy有内置的函数也一样，尽量使用，如log,max  etc..\n* 在python中，z= np.dot(w.T, x) + b , b会自动扩展成一个n维向量，这在python称为broadcast\n![ng_broadcast](http://p15i7i801.bkt.clouddn.com/849e2ed8c823392209bd02a6e7397727.png)\n* 在使用的过程中，reshape是o(1)操作，多多益善，尽量不使用rank为1的数组。\n\n### 第三周 浅层神经网络\n* 二级神经网络，即只有一个隐藏层的神经网络。\n*  输出层通常使用sigmoid函数，而隐藏层不用，一般至少使用tanh来代替，因为它的输出介于-1与1之间，这样均值为0，便于下一层的计算。考虑到梯度下降，sigmoid与tanh在最大小值处导数变化小，reLU及leaky reLU更常用。当然这一切根据实际情况调整。\n*  隐藏层不应使用线性激活函数，若用则相当于没有隐藏层，输出层若要出回归值，则可使用。\n*  随机w,并乘以较小的系数，一般让随机参数比较小。\n\n### 第四周 深层神经网络\n####  hyper parameter——控制参数的参数\n  * 层数\n  * 迭代次数\n  * 下降速率\n  * 激活函数的选择\n  * batch size\n####  parameter\n  * 权重 $w$\n  * 偏移量 $b$\n","slug":"神经网络与深度学习","published":1,"updated":"2017-12-21T04:16:18.786Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbowgdpn000hj4n7hcxpr14n","content":"<h3 id=\"第一、二周：概论与基础\"><a href=\"#第一、二周：概论与基础\" class=\"headerlink\" title=\"第一、二周：概论与基础\"></a>第一、二周：概论与基础</h3><blockquote>\n<p>sigmoid在逻辑回归里为什么没用被ReLU替代</p>\n</blockquote>\n<ul>\n<li>sigmoid在感知器结果输出里可以将任意输入连接的映射到 $[0,1]$ 之间，且结果保持连续</li>\n<li>ReLu作为深度学习引入的激活函数，特性在于稀疏的激活性、单侧抑制与相对宽的兴奋边界</li>\n</ul>\n<blockquote>\n<p>LR中损失函数不用平方误差，为什么它是非凸的。</p>\n</blockquote>\n<ul>\n<li>因为加入了Sigmoid函数，$a=\\sigma (z)$ 它的特点：<script type=\"math/tex; mode=display\">da/dz = z(1-z), dL/dz= a - y \\quad  if \\quad L=-( y log a + (1-y)log(1-a) )</script></li>\n</ul>\n<blockquote>\n<p>矩阵运算Numpy的高效</p>\n</blockquote>\n<ul>\n<li>避免矩阵运算使用For循环，100m时差300倍效率，因为 numpy使用到了CPU\\GPU的SIMD指令，运行速度更快。同时，对于numpy有内置的函数也一样，尽量使用，如log,max  etc..</li>\n<li>在python中，z= np.dot(w.T, x) + b , b会自动扩展成一个n维向量，这在python称为broadcast<br><img src=\"http://p15i7i801.bkt.clouddn.com/849e2ed8c823392209bd02a6e7397727.png\" alt=\"ng_broadcast\"></li>\n<li>在使用的过程中，reshape是o(1)操作，多多益善，尽量不使用rank为1的数组。</li>\n</ul>\n<h3 id=\"第三周-浅层神经网络\"><a href=\"#第三周-浅层神经网络\" class=\"headerlink\" title=\"第三周 浅层神经网络\"></a>第三周 浅层神经网络</h3><ul>\n<li>二级神经网络，即只有一个隐藏层的神经网络。</li>\n<li>输出层通常使用sigmoid函数，而隐藏层不用，一般至少使用tanh来代替，因为它的输出介于-1与1之间，这样均值为0，便于下一层的计算。考虑到梯度下降，sigmoid与tanh在最大小值处导数变化小，reLU及leaky reLU更常用。当然这一切根据实际情况调整。</li>\n<li>隐藏层不应使用线性激活函数，若用则相当于没有隐藏层，输出层若要出回归值，则可使用。</li>\n<li>随机w,并乘以较小的系数，一般让随机参数比较小。</li>\n</ul>\n<h3 id=\"第四周-深层神经网络\"><a href=\"#第四周-深层神经网络\" class=\"headerlink\" title=\"第四周 深层神经网络\"></a>第四周 深层神经网络</h3><h4 id=\"hyper-parameter——控制参数的参数\"><a href=\"#hyper-parameter——控制参数的参数\" class=\"headerlink\" title=\"hyper parameter——控制参数的参数\"></a>hyper parameter——控制参数的参数</h4><ul>\n<li>层数</li>\n<li>迭代次数</li>\n<li>下降速率</li>\n<li>激活函数的选择</li>\n<li>batch size<h4 id=\"parameter\"><a href=\"#parameter\" class=\"headerlink\" title=\"parameter\"></a>parameter</h4></li>\n<li>权重 $w$</li>\n<li>偏移量 $b$</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"第一、二周：概论与基础\"><a href=\"#第一、二周：概论与基础\" class=\"headerlink\" title=\"第一、二周：概论与基础\"></a>第一、二周：概论与基础</h3><blockquote>\n<p>sigmoid在逻辑回归里为什么没用被ReLU替代</p>\n</blockquote>\n<ul>\n<li>sigmoid在感知器结果输出里可以将任意输入连接的映射到 $[0,1]$ 之间，且结果保持连续</li>\n<li>ReLu作为深度学习引入的激活函数，特性在于稀疏的激活性、单侧抑制与相对宽的兴奋边界</li>\n</ul>\n<blockquote>\n<p>LR中损失函数不用平方误差，为什么它是非凸的。</p>\n</blockquote>\n<ul>\n<li>因为加入了Sigmoid函数，$a=\\sigma (z)$ 它的特点：<script type=\"math/tex; mode=display\">da/dz = z(1-z), dL/dz= a - y \\quad  if \\quad L=-( y log a + (1-y)log(1-a) )</script></li>\n</ul>\n<blockquote>\n<p>矩阵运算Numpy的高效</p>\n</blockquote>\n<ul>\n<li>避免矩阵运算使用For循环，100m时差300倍效率，因为 numpy使用到了CPU\\GPU的SIMD指令，运行速度更快。同时，对于numpy有内置的函数也一样，尽量使用，如log,max  etc..</li>\n<li>在python中，z= np.dot(w.T, x) + b , b会自动扩展成一个n维向量，这在python称为broadcast<br><img src=\"http://p15i7i801.bkt.clouddn.com/849e2ed8c823392209bd02a6e7397727.png\" alt=\"ng_broadcast\"></li>\n<li>在使用的过程中，reshape是o(1)操作，多多益善，尽量不使用rank为1的数组。</li>\n</ul>\n<h3 id=\"第三周-浅层神经网络\"><a href=\"#第三周-浅层神经网络\" class=\"headerlink\" title=\"第三周 浅层神经网络\"></a>第三周 浅层神经网络</h3><ul>\n<li>二级神经网络，即只有一个隐藏层的神经网络。</li>\n<li>输出层通常使用sigmoid函数，而隐藏层不用，一般至少使用tanh来代替，因为它的输出介于-1与1之间，这样均值为0，便于下一层的计算。考虑到梯度下降，sigmoid与tanh在最大小值处导数变化小，reLU及leaky reLU更常用。当然这一切根据实际情况调整。</li>\n<li>隐藏层不应使用线性激活函数，若用则相当于没有隐藏层，输出层若要出回归值，则可使用。</li>\n<li>随机w,并乘以较小的系数，一般让随机参数比较小。</li>\n</ul>\n<h3 id=\"第四周-深层神经网络\"><a href=\"#第四周-深层神经网络\" class=\"headerlink\" title=\"第四周 深层神经网络\"></a>第四周 深层神经网络</h3><h4 id=\"hyper-parameter——控制参数的参数\"><a href=\"#hyper-parameter——控制参数的参数\" class=\"headerlink\" title=\"hyper parameter——控制参数的参数\"></a>hyper parameter——控制参数的参数</h4><ul>\n<li>层数</li>\n<li>迭代次数</li>\n<li>下降速率</li>\n<li>激活函数的选择</li>\n<li>batch size<h4 id=\"parameter\"><a href=\"#parameter\" class=\"headerlink\" title=\"parameter\"></a>parameter</h4></li>\n<li>权重 $w$</li>\n<li>偏移量 $b$</li>\n</ul>\n"},{"title":"集体智慧编程","date":"2017-07-17T06:27:46.000Z","mathjax":true,"_content":"\n####推荐\n* 对于每人都持有的项，使用权重稀释\n* itemCF，通常我们需要在用户基数和评分数量不是很大的时候去计算，随着用户的增长，物品之间的相似度评价会越来越稳定\n* 文本聚类，注意过滤太常用的the     ，可以定一个上下界【10%，50%】\n####聚类\n* 分层聚类，优点在于直观，不用指定聚类数，缺点计算量惊人\n####搜索\n* 流程：\n爬虫——设计SCHEMA——索引——查询——评分排名（单词频度 、文档位置、单词距离）——\n* 评分\n * 普通指标：单词频度 、文档位置、单词距离\n * 利用外部回指-简单计数\n *　pagerank\n","source":"_posts/集体智慧编程.md","raw":"---\ntitle: 集体智慧编程\ndate: 2017-07-17 14:27:46\ntags:\n      - 机器学习\n      - 经典著作\ncategories: AI梦\nmathjax: true\n---\n\n####推荐\n* 对于每人都持有的项，使用权重稀释\n* itemCF，通常我们需要在用户基数和评分数量不是很大的时候去计算，随着用户的增长，物品之间的相似度评价会越来越稳定\n* 文本聚类，注意过滤太常用的the     ，可以定一个上下界【10%，50%】\n####聚类\n* 分层聚类，优点在于直观，不用指定聚类数，缺点计算量惊人\n####搜索\n* 流程：\n爬虫——设计SCHEMA——索引——查询——评分排名（单词频度 、文档位置、单词距离）——\n* 评分\n * 普通指标：单词频度 、文档位置、单词距离\n * 利用外部回指-简单计数\n *　pagerank\n","slug":"集体智慧编程","published":1,"updated":"2017-12-27T06:30:14.171Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjbowgdpp000kj4n7qr900a1a","content":"<h4 id=\"推荐\"><a href=\"#推荐\" class=\"headerlink\" title=\"推荐\"></a>推荐</h4><ul>\n<li>对于每人都持有的项，使用权重稀释</li>\n<li>itemCF，通常我们需要在用户基数和评分数量不是很大的时候去计算，随着用户的增长，物品之间的相似度评价会越来越稳定</li>\n<li>文本聚类，注意过滤太常用的the     ，可以定一个上下界【10%，50%】<h4 id=\"聚类\"><a href=\"#聚类\" class=\"headerlink\" title=\"聚类\"></a>聚类</h4></li>\n<li>分层聚类，优点在于直观，不用指定聚类数，缺点计算量惊人<h4 id=\"搜索\"><a href=\"#搜索\" class=\"headerlink\" title=\"搜索\"></a>搜索</h4></li>\n<li>流程：<br>爬虫——设计SCHEMA——索引——查询——评分排名（单词频度 、文档位置、单词距离）——</li>\n<li>评分<ul>\n<li>普通指标：单词频度 、文档位置、单词距离</li>\n<li>利用外部回指-简单计数<br>*　pagerank</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"推荐\"><a href=\"#推荐\" class=\"headerlink\" title=\"推荐\"></a>推荐</h4><ul>\n<li>对于每人都持有的项，使用权重稀释</li>\n<li>itemCF，通常我们需要在用户基数和评分数量不是很大的时候去计算，随着用户的增长，物品之间的相似度评价会越来越稳定</li>\n<li>文本聚类，注意过滤太常用的the     ，可以定一个上下界【10%，50%】<h4 id=\"聚类\"><a href=\"#聚类\" class=\"headerlink\" title=\"聚类\"></a>聚类</h4></li>\n<li>分层聚类，优点在于直观，不用指定聚类数，缺点计算量惊人<h4 id=\"搜索\"><a href=\"#搜索\" class=\"headerlink\" title=\"搜索\"></a>搜索</h4></li>\n<li>流程：<br>爬虫——设计SCHEMA——索引——查询——评分排名（单词频度 、文档位置、单词距离）——</li>\n<li>评分<ul>\n<li>普通指标：单词频度 、文档位置、单词距离</li>\n<li>利用外部回指-简单计数<br>*　pagerank</li>\n</ul>\n</li>\n</ul>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjbowgdpf0008j4n7f5a42brx","category_id":"cjbowgdpb0004j4n7vfgupkd5","_id":"cjbowgdpm000ej4n7br1uhjnk"},{"post_id":"cjbowgdp30000j4n74qhuc5nj","category_id":"cjbowgdpb0004j4n7vfgupkd5","_id":"cjbowgdpo000ij4n7rc417scu"},{"post_id":"cjbowgdp80002j4n7m3eegig5","category_id":"cjbowgdpb0004j4n7vfgupkd5","_id":"cjbowgdpq000lj4n7n9buold1"},{"post_id":"cjbowgdpn000hj4n7hcxpr14n","category_id":"cjbowgdpm000fj4n7a5hxbi4h","_id":"cjbowgdps000pj4n76kqgqeac"},{"post_id":"cjbowgdpd0006j4n7llxzttvz","category_id":"cjbowgdpm000fj4n7a5hxbi4h","_id":"cjbowgdpt000sj4n76drq5dnt"},{"post_id":"cjbowgdpp000kj4n7qr900a1a","category_id":"cjbowgdpm000fj4n7a5hxbi4h","_id":"cjbowgdpu000uj4n7e1ksv31t"},{"post_id":"cjbowgdph0009j4n7wxqu5wz0","category_id":"cjbowgdpm000fj4n7a5hxbi4h","_id":"cjbowgdpv000xj4n7wm8y1ewa"},{"post_id":"cjbowgdpj000cj4n75hv7x0oe","category_id":"cjbowgdpm000fj4n7a5hxbi4h","_id":"cjbowgdpv000yj4n76d5ehkn7"}],"PostTag":[{"post_id":"cjbowgdp30000j4n74qhuc5nj","tag_id":"cjbowgdpc0005j4n71mr2u70g","_id":"cjbowgdpo000jj4n79sriwmzm"},{"post_id":"cjbowgdp30000j4n74qhuc5nj","tag_id":"cjbowgdpi000bj4n7n56lfqvx","_id":"cjbowgdpq000mj4n7bh9bwoas"},{"post_id":"cjbowgdp80002j4n7m3eegig5","tag_id":"cjbowgdpm000gj4n7odzzxgo3","_id":"cjbowgdpu000tj4n7a88qayxt"},{"post_id":"cjbowgdp80002j4n7m3eegig5","tag_id":"cjbowgdpi000bj4n7n56lfqvx","_id":"cjbowgdpu000vj4n7dee58s6h"},{"post_id":"cjbowgdpd0006j4n7llxzttvz","tag_id":"cjbowgdpt000rj4n7qaaix7s3","_id":"cjbowgdpw0010j4n7hsv08ak2"},{"post_id":"cjbowgdpd0006j4n7llxzttvz","tag_id":"cjbowgdpi000bj4n7n56lfqvx","_id":"cjbowgdpw0011j4n7t6t7jbgd"},{"post_id":"cjbowgdpf0008j4n7f5a42brx","tag_id":"cjbowgdpv000zj4n7elt220er","_id":"cjbowgdpy0014j4n7n3offoer"},{"post_id":"cjbowgdpf0008j4n7f5a42brx","tag_id":"cjbowgdpi000bj4n7n56lfqvx","_id":"cjbowgdpy0015j4n719hyh72u"},{"post_id":"cjbowgdph0009j4n7wxqu5wz0","tag_id":"cjbowgdpx0013j4n7ye15tnux","_id":"cjbowgdq10019j4n739rzkvem"},{"post_id":"cjbowgdph0009j4n7wxqu5wz0","tag_id":"cjbowgdpy0016j4n70r01zosi","_id":"cjbowgdq1001aj4n7cp2kwqsg"},{"post_id":"cjbowgdph0009j4n7wxqu5wz0","tag_id":"cjbowgdq00017j4n75s2yuq8b","_id":"cjbowgdq2001cj4n74e4hhlex"},{"post_id":"cjbowgdpj000cj4n75hv7x0oe","tag_id":"cjbowgdq10018j4n7bx95q1yd","_id":"cjbowgdq4001fj4n7974xw99w"},{"post_id":"cjbowgdpj000cj4n75hv7x0oe","tag_id":"cjbowgdq2001bj4n7rvkspeu2","_id":"cjbowgdq4001gj4n7dpakdpsn"},{"post_id":"cjbowgdpj000cj4n75hv7x0oe","tag_id":"cjbowgdq00017j4n75s2yuq8b","_id":"cjbowgdq5001ij4n79qcy18nf"},{"post_id":"cjbowgdpk000dj4n7hhverpvm","tag_id":"cjbowgdq3001ej4n7lvl7f413","_id":"cjbowgdq5001jj4n7r2dkk1gf"},{"post_id":"cjbowgdpn000hj4n7hcxpr14n","tag_id":"cjbowgdq10018j4n7bx95q1yd","_id":"cjbowgdq7001nj4n7lon4thv6"},{"post_id":"cjbowgdpn000hj4n7hcxpr14n","tag_id":"cjbowgdq2001bj4n7rvkspeu2","_id":"cjbowgdq7001oj4n7um2b3nkp"},{"post_id":"cjbowgdpn000hj4n7hcxpr14n","tag_id":"cjbowgdq00017j4n75s2yuq8b","_id":"cjbowgdq8001pj4n7xo2i8oy5"},{"post_id":"cjbowgdpp000kj4n7qr900a1a","tag_id":"cjbowgdpt000rj4n7qaaix7s3","_id":"cjbowgdq8001qj4n7n7x5r2im"},{"post_id":"cjbowgdpp000kj4n7qr900a1a","tag_id":"cjbowgdpi000bj4n7n56lfqvx","_id":"cjbowgdq8001rj4n769b7t9hm"}],"Tag":[{"name":"系统结构","_id":"cjbowgdpc0005j4n71mr2u70g"},{"name":"经典著作","_id":"cjbowgdpi000bj4n7n56lfqvx"},{"name":"计算机语言","_id":"cjbowgdpm000gj4n7odzzxgo3"},{"name":"机器学习","_id":"cjbowgdpt000rj4n7qaaix7s3"},{"name":"数据结构","_id":"cjbowgdpv000zj4n7elt220er"},{"name":"神经网络","_id":"cjbowgdpx0013j4n7ye15tnux"},{"name":"Geoffrey hinton","_id":"cjbowgdpy0016j4n70r01zosi"},{"name":"公开课","_id":"cjbowgdq00017j4n75s2yuq8b"},{"name":"深度学习","_id":"cjbowgdq10018j4n7bx95q1yd"},{"name":"Andrew NG","_id":"cjbowgdq2001bj4n7rvkspeu2"},{"name":"draft","_id":"cjbowgdq3001ej4n7lvl7f413"}]}}